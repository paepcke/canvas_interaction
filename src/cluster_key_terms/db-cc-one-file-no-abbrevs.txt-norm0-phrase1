 Welcome to the course Introduction to Databases I'm Jennifer Widom from Stanford University In this course we'll be learning_about databases and the use of database_management systems primarily from the viewpoint of the designer user and developer of database applications I'm going to start_by describing in one very long sentence what a database_management system provides for applications It provides a means of handling large amounts of data primarily but let's looks at a little_more detail What it provides in a long sentence is efficient reliable convenient and safe multi user storage of and access to massive amounts of persistent data So I'm going to go into each one of those adjectives in a little_binary_digit more_detail in a moment But I did want to mention that database_systems are extremely prevalent in the world today They sit behind many websites that will run your banking systems your telecommunications deployments of sensors scientific experiments and much much more Highly prevalent So let's talk a little_binary_digit about why database_systems are so popular so and prevalent by looking_at these seven adjectives The first aspect of database_systems is that they handle data at a massive scale So if you think_about the amount of data that is being produced today database_systems are handling terabytes of data sometimes even terabytes of data every day And one of the critical aspects is that the data that's handled by database_management systems systems is much larger than can fit in the memory of a typical computing system So memories are indeed growing very very fast but the amount of data in the world and data to be handled by database_systems is growing much faster So database_systems are designed to handle data that to residing outside of memory Secondly the data that's handled by database_management systems is typically persistent And what I_mean by that is that the data in the database outlives the programs that execute on that data So if you run a typical computer program the program will start the variables we created There will be data that's operated on the program the program will finish and the data will go away It's sort of the other way with databases The data is what sits there and then program will start up it will operate_on the data the program will stop and the data will still be there Very often actually multiple programs will be operating_on the same data Next safety So database_systems since they run critical applications such as telecommunications and banking systems have to have guarantees that the data managed by the system will stay in a consistent state it won't be lost or overwritten when there are failures and there can be hardware failures There can be software failures Even simple power outages You don't want your bank balance to change because the power went out at your bank branch And of course there are the problem of malicious users that may try to corrupt data So database_systems have a number of built in mechanisms that ensure that the data remains consistent regardless of what_happens Next multi user So I_mentioned that multiple programs may operate_on the same database And even with one program operating_on a database that program may allow many different users or applications to access the data concurrently So when you have multiple applications working on the same data the system has to have some mechanisms again to ensure that the data stays consistent That you don't have for example half of a data item overwritten by one person and the other half overwritten by another So there's mechanisms in database_systems called concurrency control And the idea there is that we control the way multiple users access the database Now we don't control it by only having one user have exclusive access to the database or the performance would slow down considerably So the control actually occurs at the level of the data items in the database So many users might be operating_on the same database but be operating_on different individual data items It's a little_binary_digit similar to say file system concurrency or even variable concurrency in programs except it's more centered around the data itself The next adjective is convenience and convenience is actually one of the critical features of database_systems They really are designed to make it easy to work with large amounts of data and to do very_powerful and interesting processing on that data So there's a couple levels at which that happens There's a notion in databases called Physical Data Independence It's kind of a mouthful but what that's saying is that the way that data is actually stored and laid out on disk is independent of the way that programs think_about the structure of the data So you could have a program that operates on a database and underneath there could be a complete change in the way the data is stored yet the program itself would not have to be changed So the operations on the data are independent from the way the data is laid out And somewhat related to that is the notion of high_level query languages So the databases are usually queried by languages that are relatively compact to describe really at a very high_level what information you want from the database Specifically they obey a notion that's called declarative and what declarative is saying is that in the query you describe what you want out of the database but you don't need to describe the algorithm to get the data out and that's a really nice feature It allows you to write queries in a very_simple way and then the system itself will find the algorithm to get that data out efficiently And speaking of efficiency that's number six but certainly not sixth importance There's in real estate as a little aside here a old saying that when you have a piece of property the most_important three aspects of the property are the location of the property the location and the location And people say the same thing about databases a similar parallel joke which is that the three most_important things in a database system is first performance second performance and again performance So database_systems have to do really thousands of queries or updates per second These are not simple queries necessarily These may be very complex operations So constructing a database system that can execute queries complex queries at that rate over gigantic amounts of data terabytes of data is no simple task and that is one of the major features also provided by a database_management system And lastly but again not last in importance is reliability Again looking back at say your banking system or your telecommunications system it's critically important that those are up all the time So up time is the type of guarantee that database_management systems are making for their applications So that gives_us an idea of all the terrific things that a database system provides I_hope you're all ready convinced that if you have a application you want to build that involves data it would be great to have all of these features provided for you in a database system Now let_me_mention a few of the aspects surrounding database_systems and scope a little_binary_digit what we're going to be covering in this course When people build database applications sometimes they program them with what's_known_as a framework Currently at the time of this video some of the popular frameworks are Django or Ruby on Rails and these are environments that help you develop your programs and help you generate say the calls to the database system We're not in this set of videos going to be talking_about the frameworks but rather we're going to be talking_about the data base system itself and how it is used and what it provides Second of all database_systems are often used in conjunction with what's_known_as middle ware Again at the time of this video typical middle ware might be application servers web servers so this middle ware helps applications interact with database_systems in certain types of ways Again that's sort of outside the scope of the course We won't be talking_about middleware in the course Finally it's not the case that every application that involves data necessarily uses the database system so historically a lot of data has been stored in files I think that's a little_binary_digit less so these days Still there's a lot of data out there that's simply sitting in files Excel spreadsheets is another domain where there's a lot of data sitting out there and it's useful in certain ways and the processing of data is not always done through query languages associated_with database_systems For_example Hadoop is a processing framework for running operations on data that's stored in files Again in this set of videos we're going to focus_on the database_management system itself and on storing and operating_on data through a database_management system So there are four key concepts that we're going to cover for now The first one is the data_model The data_model is a description of in general how the data is structured One of the most common data models is the relational dot data_model we'll spend quite a binary_digit of time on that In the relational data_model the data and the database is thought of as a set of records Now another popular way to store data is for example in eXtensible_Markup_Language documents so an eXtensible_Markup_Language document captures data instead of a set of records as a hierarchical structure of labeled values Another possible data_model would be a graph data_model or all data in the database is in the form of nodes and edges So again a data_model is telling you the general form of data that's going to be stored in the database Next is the concept of schema versus data One can think of this kind of like types and variables in a programming language The schema sets up the structure of the database Maybe I'm going to have information_about students with IDs and GPAs or about colleges and it's just going to tell me the structure of the database where the data is the actual data stored within the schema Again in a program you set_up types and then you have variables of those types we'll set_up a schema and then we will have a whole_bunch of data that adheres to that schema Typically the schema is set_up at the beginning and doesn't change very much where the data changes rapidly Now to set_up the schema one normally uses what's_known_as a data definition language Sometimes people use higher_level design tools that help them think_about the design and then from there go to the data definition language But it's used in general to set_up a scheme or structure for a particular database Once the schema has been set_up and data has been loaded then it's possible to start querying and modifying the data and that's typically done with what's_known_as the data manipulation language so for querying and modifying the database Okay so those are some key concepts certainly we're going to get in to much more_detail in later_videos about each of these concepts Now let's talk_about the people that are involved in a database system So the first person we'll mention is the person_who implements the database system itself the database implementer That's the person_who builds the system that's not going to be the focus of this course We're going to be focusing more on the types of things that are done by the other three people that I'm going to describe The next one is the database designer So the database designer is the person_who establishes the schema for a database So let's_suppose we have an application We know there's going to be a lot of data involved in the application and we want to figure_out how we are gonna structure that data before we build the application That's the job of the database designer It's a surprisingly difficult job when you have a very complex data involved in an application Once you've established the structure of the database then it's time to build the applications or programs that are going to run on the database often interfacing between the eventual user and the data itself and that's the job of the application developer so those are the programs that operate_on the database And again I've mentioned already that you can have a database with many different programs that operate_on it be very common You_might for example have a sales database where some applications are actually inserting the sales as they happen while others are analyzing the sales So it's not necessary to have a one to one coupling between programs and databases And the last person is the database administrator So the database administrator is the person_who loads the data sort of gets the whole thing running and keeps it running smoothly So this actually turns_out to be a very_important job for large database applications For better or worse database_systems do tend to have a number of tuning parameters associated_with them and getting those tuning parameters right can make a significant difference in the all important performance of the database system So database administrators are actually highly valued very_important highly paid as a matter of fact and are for large deployments an important person in the entire process So those are the people that are involved again in this class we'll be focusing mostly on designing and developing applications a little_binary_digit on administration but in general thinking about databases and the use of database_management systems from the perspective of the application builder and user To conclude we're going to be learning_about databases and whether you know it or not not you're already using a database every day In fact more likely than not you're using a database every hour In this video we'll learn_about the Relational Model The Relational Model is more_than years old and it's really the foundation of database_management systems It's spawned a many billion dollar industry The relational_model underlies all commercial database_systems at this point in time It's actually an extremely simple model and that's one of its benefits Furthermore it can be queried By that I_mean we can ask questions of databases in the model using High Level Languages High Level Languages are simple yet extremely expressive for asking questions over the database And finally very importantly there are extremely efficient implementations of the relational_model and of the query languages on that model So let's move_ahead and understand the basic constructs in the relational_model So the primary construct is in fact the relation A database consists of a set of relations or sometimes referred to as tables each of which has a name So we're gonna use two relations in our example Our example is gonna be a fictitious database about students_applying to colleges For now we're just gonna look_at the students and colleges themselves So we're gonna have two tables and let's call those tables the Student table and the College table Now as an aside there's a healthy debate in the database world about whether tables relations ought to be named using the singular or the plural I personally don't have a stake in that debate I'm going to use the singular Next we have the concept of attributes So every relation and relational database has a predefined set of columns or attributes each of which has a name So for our student table let's say that each student is gonna have an ID a name a GPA and a photo And for our college table let's say that every college is going to have a name a state and an enrollment We'll just abbreviate that ENR So those are the labeled columns Now the actual data itself is stored in what are called the tuples or the rows in the tables So let's put a couple of the data tables data tuples in our tables So let's start with the students and lets say that our first student has ID Name is Amy GPA and she's happy with that So she has a smiley photo And our second student is Bob his GPA is He's not quite as happy And typically of course a table will have thousands maybe millions even sometimes billions of rows each_row containing a value for each attribute In our college table let's_suppose we have well of course we're going to start with Stanford in the state of California and Stanford's enrollment is We'll include our cross bay rival Berkeley again in the state of California Berkeley's enrollment is a whopping And last of all we are going to not be West Coast biased We'll include Massachusetts_Institute of Technology in the state of Massachusetts with an enrollment of Now of course there's gonna be again many more tuples in the college table and many more tuples in the student table Okay next let_me_mention that in a relational database typically each attribute or column has a type sometimes referred to as a domain For_example the ID might be an integer the name might be a string GPA might be a float photo might be a jpeg file We do also in most relational_databases have a concept of enumerated domain So for example the state might be an enumerated domain for the abbreviations for states Now it's typical for relational_databases to have just atomic types in their attributes as we have here but many database_systems do also support structured types inside attributes Okay a little_binary_digit more terminology The schema of a database is the structure of the relation So the schema includes the name of the relation and the attributes of the relation and the types of those attributes Where the instance is the actual contents of the table at a given point in time So typically you set_up a schema in advance then the instances of the data will change over time Now I_mentioned that most columns have types But there's also a special value that's in any type of any column and that's a special value known_as null and nulls are actually quite important in relational_databases Null values are used to denote that a particular value is maybe unknown or undefined And so let's_suppose let's add another tuple to our database Let's say another student named Craig and for whatever_reason Craig doesn't have a GPA Maybe Craig is home schooled maybe Craig doesn't want to reveal his GPA So then the database would contain a null value for Craig and we'll just put a neutral face there Or for example maybe Bob doesn't want to have his photo in the database so then Bob would have a null value for his photo again nulls can go anywhere Now null_values are useful but one has to be very_careful in a database system when you run queries over relations that have null_values In a later video we'll go into this in more_detail but I just wanted to give a just sort of example of what can happen So let's_suppose we're asking a query over our student table of all students_whose GPA is greater_than So when we run that query on our database obviously we'll get Amy out obviously we won't get Bob out but should we get Craig The answer is No We don't know for a fact that Craig's GPA is greater_than so we'll only get one student out from that query Now let's_suppose we had another query where we were gonna ask for the GPA less_than or equal to So similarly where we would not have Amy in result and we would certainly have Bob in the result and similarly would not have Craig in the result because we don't know that his GPA is less_than or equal to So far so good but it gets a little weird is when we add an or here in our query we say I want everyone who's GPA is greater_than or who's GPA is less_than or equal to And even_though it looks_like every tuple should satisfy this condition that it's always true that's not the case when we have null_values So that's why one has to be careful when one uses null_values in relational_databases Let_me erase this now and let's move on to our next concept which is the concept of Key Key is again another important concept in relational_databases And a key is an attribute in of a relation where every value for that attribute is unique So if we look_at the student relation we can feel pretty confident that the ID is going to be a key In_other_words every tuple is going to have a unique for ID Thinking about the college relation it's a little less clear We might be tempted to say that the name of the college is an ID that actually college names probably are not unique across the country There's probably a lot of or several colleges named Washington college for example You know what we're allowed to have sets of attributes that are unique and that makes_sense in the college relation Most likely the combination of the name and state of a college is unique and that's what we would identify as the key for the college relation Now you_might_wonder why it's even important to have attributes that are identified as keys There's actually several uses for them One of them is just to identify specific tuples So if you want to run a query to get a specific tuple out of the database you would do that by asking for that tuple by its key And related to that database_systems for efficiency tend to build special index structures or store the database in a particular way So it's very fast to find a tuple based_on its key And lastly if one relation in a relational database wants to refer to tuples of another there 's no concept of pointer in relational_databases Therefore the first relation will typically refer to a tuple in the second relation by its unique key As our videos develop we'll see the importance of keys Okay just to wrap_up I'll mention how one creates relations or tables in the Structured_Query_Language language It's very_simple you just say create table give the name of the relation and a list of the attributes And if you want to give types for the attributes It's similar except you follow each attribute name with its type So to wrap_up the relational_model has been around a long time Has started a huge industry It's used by all database_systems As you've seen it's a very_simple model and will shortly see that it can be queried with very nice languages And finally it's been implemented very efficiently In this video we're going to learn_about querying relational_databases We're not going to focus_on a specific query language we'll do that later We're just going to talk_about querying relational_databases in general Let's start_by talking_about the basic steps in creating and using a relational database So by the way I should mention that database people have this habit of drawing databases and database_systems as gigantic disks So I'll be using that same habit So the first step is to design the schema of the database and then create the schema using a data definition language So as we discussed in previous_videos in a relational database the schema consists of the structure of the relations and the attributes of those relations So we set those up inside our big disk Once that's ready the next step is to load up the database with the initial data So it's fairly common for the database to be initially loaded from data that comes from an outside source Maybe the data is just stored in files of some type and then that data could be loaded into the database Once the data is loaded then we have a bunch of tuples in our relation Now we're ready for the fun part which is to query and modify the data And so that happens continuously over time as long as the database is in existence So let's just say for now that we're going to have human users that are directly querying the database In reality that typically happens through say an application or a website So a user will come along and we'll ask a question of the database and we will get an answer He might come along and ask another question Q and he'd get another answer back The same human or maybe a different human might ask to modify the database So they might want to insert new data or update some of the data and the database will come_back and say Okay I made that change for you So that's the basic paradigm of querying and updating relational_databases Relational databases support ad hoc queries and high_level languages By ad hoc I_mean that you can pose queries that you didn't think of in advance So it's not necessary to write long programs for specific queries Rather the language can be used to pose a query as you think_about what you want to ask And as mentioned in previous_videos the languages supported by relational systems are high_level meaning you can write in a fairly compact fashion rather complicated queries and you don't have to write the algorithms that get the data out of the database So let's look_at an example of a few queries Let's go to again to our imaginary database of students_who are applying to colleges And here's just three examples of the types of things that you might ask of a relational database You_might want to get all students_whose GPA is greater_than who are applying to Stanford and Massachusetts_Institute of Technology only You_might want to get all engineering departments in California with fewer_than applicants or you might ask for the college with the highest average accept rate over the last five years Now these might seem like a fairly_complicated queries but all of these can be written in a few lines in say the Structured_Query_Language language or a pretty_simple expression in relational_algebra So some queries are easier to pose than others that's certainly true Though the queries you see here are as I said pretty easy to pose Now some queries are easier for the database system to execute efficiently than others And interestingly it's not necessarily These two things aren't necessarily correlated There are some queries that are easy to post but hard to execute efficiently and some that are vice versa Now just a binary_digit about terminology Frequently people talk_about the query language of the database system That's usually used sort of synonymously with the Definitive Media Library or Data Manipulation Language which usually includes not only querying but also data modifications In all relational query languages when you ask a query over a set of relations you get a relation as a result So let's run a query cue say over these three relations shown here and what we'll get back is another relation When you get back the same type of object that you query that's known_as closure of the language And it really is a nice feature For_example when I want to run another query say Q that query could be posed over the answer of my first query and could even combine that answer with some of the existing relations in the database That's known_as compositionality the ability to run a query over the result of our previous query Now let_me talk briefly about two query languages We'll be learning these languages in detail later but I'm just going to give the basic flavor of the languages here Relational algebra is a formal language Well it's an algebra as you can tell by its name So it's very theoretically well grounded Structured_Query_Language by contrast is what I'll call an actual language or an implemented language That 's the one you're going to run on an actual deployed database application But the Structured_Query_Language language does have as its foundation relational_algebra That's how the semantics of the Structured_Query_Language language are defined Now let_me just give you a flavor of these two languages and I'm going to write one query in each of the two languages So let_me get_rid of this little line here Let's start in relational_algebra So we're looking for the ID's of students_whose GPA is greater_than and they've_applied to Stanford In relational_algebra the basic operators language are Greek symbols Again we'll learn the details later but this particular expression will be written by a Phi followed_by a Sigma The Phi says we're going to get the ID the Sigma says we want students_whose GPA is greater_than and the college that the students have applied to is Stanford And then that will operate_on what's_called the natural_join of the student relation with the apply relation Again we'll learn the details of that in a later video Now here's the same query in Structured_Query_Language And this is something that you would actually run on a deployed database system and the Structured_Query_Language query is in fact directly equivalent to the relational_algebra query Now pedagogically I would highly recommend that you learn the relational_algebra by watching the relational_algebra videos before you move on to the Structured_Query_Language videos but I'm not going to absolutely require that So if you're in a big hurry to learn Structured_Query_Language right away you may move_ahead to the Structured_Query_Language videos If you're interested in the formal foundations and a deeper understanding I recommend moving next to the relational_algebra video This video introduces the basics of eXtensible_Markup_Language eXtensible_Markup_Language can be thought of as a data_model an alternative to the relational_model for structuring data In addition to introducing eXtensible_Markup_Language we will compare it to the relational_model although it is not critical to have watched the relational_model videos in order to get something out of this one The full name of eXtensible_Markup_Language is the extensible markup language eXtensible_Markup_Language is a standard for data representation and exchange and it was designed initially for exchanging information on the Internet Now don't worry if you can't read the little snippet in the corner of the video here You're not expected to at this point eXtensible_Markup_Language can be thought of as a document format similar to Hypertext_Markup_Language if you're familiar with Hypertext_Markup_Language Most people are The big difference is that the tags in an Hypertext_Markup_Language document describe the content of the data rather_than how to format the data which is what the tags in Hypertext_Markup_Language tend to represent eXtensible_Markup_Language also has a streaming format or a streaming standard and that's typically for the use of eXtensible_Markup_Language in programs for admitting eXtensible_Markup_Language and consuming eXtensible_Markup_Language So now let's take a look_at the eXtensible_Markup_Language data itself You see on the left_side of the video a portion of an eXtensible_Markup_Language document The entire document is available from the website for the course eXtensible_Markup_Language has three basic components Again fairly similar to Hypertext_Markup_Language The first is tagged element So for example let's take a look_at this element here This is an element saying that the data here is a first name So we have a opening_tag and we have a matching closing_tag We also have nesting development So for example here we have an element that's authored We have the opening_tag here the closing_tag here and we have a nesting of the first name and last_name elements Even larger we have a book element here with opening and closing_tags with a nesting of numerous elements inside and the entire document actually is one element whose opening_tag is bookstore and the closing_tag isn't visible on the video here So that's what elements consist of an opening_tag text or other sub_elements and a closing_tag In addition we have have attributes so each element may have within its opening_tag and let's take a look_at the book element here A set of attributes and an attribute consists of an attribute name the equal sign and then an attribute value So our book element right here has three attributes One called ISPN one called Price and one called Edition And any element can have any number of attributes as long as the attribute names are unique And finally the third component of eXtensible_Markup_Language is the text itself which is depicted here in black So within elements we can have strengths We have a strength all right here we have a title here here we have a remark And so that's generally sort of think of eXtensible_Markup_Language as a tree the strings form or the text form the leaf element of the tree So again those are the three major components of xml Look's a lot like Hypertext_Markup_Language except the tags are describing the content of the data and not how to format it Now let's spend some time comparing the relational_model against eXtensible_Markup_Language Again it's not critical that you learn_about the relational_model and you can skip this material if you're not interested but in many cases when designing an application that's dealing with data you might have to make a decision whether you want to use a relational database or whether you want to store the data in eXtensible_Markup_Language So let's look_at a few different aspects of the data and how it's used and how it compares between relational and eXtensible_Markup_Language Let's start with the structure of the data itself So as we learn the structure in a relational_model is basically a set of tables So we define the set of columns and we have a set of rows eXtensible_Markup_Language is generally again it's usually in a document or a string format but if you think_about the structure itself the structure is hierarchical The nested elements induce a hierarchy or a tree There are constructs that actually allow_us to have links within documents and so you can also have eXtensible_Markup_Language representing a graph though in general it's mostly thought of as a tree structure Next let's talk_about schemas In the relational_model the schema is very_important You fix your schema in advance when you design your database and them you add the data to conform to the schema Now in eXtensible_Markup_Language you have a lot more flexibility So the schema is flexible In fact a lot of people refer to eXtensible_Markup_Language as self describing In_other_words the schema and the data kind of mixed together The tags on elements are telling you the kind of data you'll have and you can have a lot of irregularity Now I will say that their are many mechanisms for introducing schemas into eXtensible_Markup_Language but they're not required In the relational_model schemas are absolutely required In eXtensible_Markup_Language they're more optional In particular let's go_back and take a look_at our example and we'll see that we have sort of some structure in our example but not everything is perfectly structured as it would be in the model So coming back here and taking a look first of all we have the situation where in this first book we have an attribute called edition the third edition Whereas in the second book we only have two attributes so there's_no addition in this book Now in the relational_model we would have to have a column for addition and we have one for every book Although of course we could have null editions for some books In eXtensible_Markup_Language it's perfectly acceptable to have some attributes for some elements and those attributes don't appear in other elements Here's another example where we have a component in one book that's not in another and it's this remark component So here we have a book where we happen to have a remark and incidentally you can see that this book suggests this remark suggests that we buy the complete book together with the first course The first course is a subset so it's not a very good suggestion although Amazon actually did make that one Anyway enough of the asides We do see that we have remark for the first book and we have no remark for the second book and that's not a problem whatsoever in eXtensible_Markup_Language In the relational_model we would again have to use null_values for that case And the third example I just wanted to give is the number of authors So this first book has two authors The second book you can't see them all but it has three authors Not a problem in eXtensible_Markup_Language Having different numbers of things is perfectly standard So the main point being that there's a lot of flexibility in eXtensible_Markup_Language in terms of the schema You can create your database with certain types of elements later add more elements remove elements introduce inconsistencies in the structure and it's not a problem And again I'll mention one more time that there are mechanisms for adding schema like elements to eXtensible_Markup_Language or schema like specifications to eXtensible_Markup_Language We will be covering those in the next two videos actually Next let's talk_about how this data is queried So for the relational_model we have relational_algebra We have Structured_Query_Language These are pretty_simple nice languages I would say It's a little_binary_digit of a matter of opinion but I'm going to give them a smiley face eXtensible_Markup_Language querying is a little trickier Now one of the factors here is that eXtensible_Markup_Language is a lot newer than the relational_model and querying eXtensible_Markup_Language is still settling down to some extent But I'm just gonna say it's a little less so I'm gonna give it a neutral face here in terms of how simple and nice the languages are for querying eXtensible_Markup_Language and we'll be spending some time in later_videos learning some of those languages Next in our chart is the aspect of ordering So the relational_model is fundamentally an unordered model and that can actually be considered a bad thing to some extent Sometimes in data applications it's nice to have ordering We learned the order by clause in Structured_Query_Language and that's a way to get order in query results But fundamentally the data in our table in our relationship database is a set of data without an ordering within that set Now in eXtensible_Markup_Language we do have I would say an implied ordering So eXtensible_Markup_Language as I said can be thought of as either a document model or a stream model And either case just the nature of the eXtensible_Markup_Language being laid out in a document as we have here or being in a stream induces an order Very_specifically let's take a look_at the authors here So here we have two authors and these authors are in an order in the document If we put those authors in a relational database there would be no order They could come out in either order unless we did a order by clause in our query whereas in eXtensible_Markup_Language implied by the document structure is an order And there's an order between these two books as_well Sometimes that order is meaningful sometimes it's not But it is available to be used in an application Lastly let's talk_about implementation As I_mentioned in earlier videos the relational_model has been around for as least years and the systems that implement it have been around almost as long They're very mature systems They implement the relational_model as the native model of the systems and they're widely_used Things with eXtensible_Markup_Language are a little_binary_digit different partly again because eXtensible_Markup_Language hasn't been around as long But what's happening right now in terms of eXtensible_Markup_Language and conventional database_systems is eXtensible_Markup_Language is typically an add on So in most systems eXtensible_Markup_Language will be a layer over the relational database system You can enter data in eXtensible_Markup_Language you can query data in eXtensible_Markup_Language It will be translated to a relational implementation That's not necessarily a bad thing And it does allow you to combine relational data and eXtensible_Markup_Language in a single system sometimes even in a single query but it's not the native model of the system itself Now you might have noticed that the name of this video is Well formed XML So well_formed eXtensible_Markup_Language is actually the most flexible eXtensible_Markup_Language An eXtensible_Markup_Language document or an eXtensible_Markup_Language stream is considered well_formed if it adheres to the basic_structural_requirements of eXtensible_Markup_Language And there aren't many Just that we have a single root_element as we discussed before a single bookstore in this case that all of our tags are matching we don't have open tags without closed tags and our tags are properly nested so we don't have interweaving of elements And finally within each element if we have attribute names they're unique And that's about it That's all we require for a eXtensible_Markup_Language document or a set of eXtensible_Markup_Language data to be considered well_formed And for many applications that's all we're concerned about In order to test whether a document is well_formed and specifically to access the components of the document in a program we have what's_called an eXtensible_Markup_Language parser So we'll take an eXtensible_Markup_Language document here and we'll feed it to an eXtensible_Markup_Language parser and the parser will check the basic structure of the document just to make_sure that everything is okay If the document doesn't appear to these three requirements up here the parser will just send an_error saying it's not well_formed If the document does adhere to the structure then what comes out is parsed eXtensible_Markup_Language And there's various standards for how we show parsed eXtensible_Markup_Language One is called the document object model or Document Object Model it's a programmatic interface for sort of traversing the tree that's implied by eXtensible_Markup_Language Another popular one is Simple API for XML That's a more of a stream model for eXtensible_Markup_Language So these are the ways in which a program would access the parsed eXtensible_Markup_Language when it comes out of the parser So one issue that comes up because the eXtensible_Markup_Language data is used frequently on the internet is how we display eXtensible_Markup_Language So one way to display eXtensible_Markup_Language is just as we see it here but very often we want to format the data that's in an eXtensible_Markup_Language document or an eXtensible_Markup_Language string in a more intuitive way And actually there's a nice setup for doing that What we can do is use a rule based language to take the eXtensible_Markup_Language and translate it automatically to Hypertext_Markup_Language which we can then render in a browser A couple of popular languages are cascading style sheets known_as Cross Site Scripting or the extensible style sheet language known_as eXtensible_Stylesheet Language We're going to look a little_binary_digit with eXtensible_Stylesheet Language on a later video in the context of query in eXtensible_Markup_Language We won't be covering Cross Site Scripting in this course But let's just understand how these languages are used what the basic structure is So the idea is that we have an eXtensible_Markup_Language document and then we send it to an interpreter of Cross Site Scripting or eXtensible_Stylesheet Language but we also have to have the rules that we're going to use on that particular document And the rules are going to do things like match patterns or add extra commands and once we send an eXtensible_Markup_Language document thorugh the interpreter we'll get an Hypertext_Markup_Language document out and then we can render that document in the browser Now one thing I should mention is that we'll also check with the parser to make_sure that the document is well_formed as_well before we translate it to Hypertext_Markup_Language To conclude eXtensible_Markup_Language is a standard for data representation and exchange It can also be thought of as a data_model Sort of a competitor to the relational_model for structuring the data in one's application It generally has a lot more flexibility than the relational_model which can be a plus and a minus actually In this video we covered the well_formed eXtensible_Markup_Language so eXtensible_Markup_Language that adheres to basic_structural_requirements in the next_video we will cover valid eXtensible_Markup_Language where we actually do introduce a kind of schema for eXtensible_Markup_Language The last thing I want to mention is that the formal specification for eXtensible_Markup_Language is quite enormous There are a lot of bells and whistles We're going to cover in these videos the most_important components for understanding anything eXtensible_Markup_Language In the previous_video we learned the basics of eXtensible_Markup_Language In this video we're going to learn_about Document_Type Descriptors also known_as DTDs and also ID and ID ref attributes We learned that well_formed eXtensible_Markup_Language is eXtensible_Markup_Language that adheres to basic_structural_requirements a single root_element matched tags with proper nesting and unique attributes within each element Now we're going to learn_about what's_known_as valid eXtensible_Markup_Language Valid eXtensible_Markup_Language has to adhere to the same basic_structural_requirements as well_formed eXtensible_Markup_Language but it also adheres to content specific specifications And we're going to learn two languages for those specifications One of them is Document_Type Descriptors or DTDs and the other a more powerful language is eXtensible_Markup_Language schema Specifications in eXtensible_Markup_Language schema are known_as XSDs for eXtensible_Markup_Language Schema Descriptions So as a reminder here's how things worked with well_formed eXtensible_Markup_Language documents We sent the document to a parser and the parser would either return that the document was not well_formed or it would return parsed eXtensible_Markup_Language Now let's consider what_happens with valid eXtensible_Markup_Language Now we use a validating eXtensible_Markup_Language parser and we have an additional input to the process which is a specification either a Document_Type_Definition or an XML_Schema_Definition So that's also fed to the parser along with the document The parser can again say the document is not well_formed if it doesn't meet the basic_structural_requirements It could also say that the document is not valid meaning the structure of the document doesn't match the content specific specification If everything is good then once again parsed eXtensible_Markup_Language is returned Now let's talk_about the document_type_descriptors or DTDs We see a Document_Type_Definition in the lower left corner of the video but we won't look_at it in any detail because we'll be doing demos of DTDs a little later on A Document_Type_Definition is a language that's kind of like a grammar and what you can specify in that language is for a particular document what elements you want that document to contain the tags of the elements what attributes can be in the elements how the different types of elements can be nested Sometimes the ordering of the elements might want to be specified and sometimes the number of occurrences of different elements DTDs also allow the introduction of special types of attributes called id and idrefs And effectively what these allow you to do is specify pointers within a document although these pointers are untyped Before moving to the demo let's talk a little_binary_digit about the positives and negatives about choosing to use a Document_Type_Definition or and XML_Schema_Definition for one's eXtensible_Markup_Language data After all if you're building an application that encodes its data in eXtensible_Markup_Language you'll have to decide whether you want the eXtensible_Markup_Language to just be well_formed or whether you want to have specifications and require the eXtensible_Markup_Language to be valid to satisfy those specifications So let's put a few positives of choosing a later of requiring a Document_Type_Definition or an XML_Schema_Definition First of all one of them is that when you write your program you can assume that the data adheres to a specific structure So programs can assume a structure and so the programs themselves are simpler because they don't have to be doing a lot of error checking on the data They'll know that before the data reaches the program it's been run through a validator and it does satisfy a particular structure Second of all we talked at some time ago about the cascading style sheet language and the extensible style sheet languages These are languages that take eXtensible_Markup_Language and they run rules on it to process it into a different form often Hypertext_Markup_Language When you write those rules if you note that the data has a certain structure then those rules can be simpler so like the programs they also can assume particular structure and it makes them simpler Now another use for DTDs or XSDs is as a specification language for conveying what eXtensible_Markup_Language might need to look like So as an example if you're performing data exchange using eXtensible_Markup_Language maybe a company is going to receive purchase orders in eXtensible_Markup_Language the company can actually use the Document_Type_Definition as a specification for what the eXtensible_Markup_Language needs to look like when it arrives at the program it's going to operate_on it Also documentation it can be useful to use one of the specifications to just document what the data itself looks_like In general really what we have here is the benefits of typing We're talking_about strongly typed data versus loosely typed data if you want to think of it that way Now let's look_at when we might prefer not to use a Document_Type_Definition So what I'm going describe down here is the benefits of not using a Document_Type_Definition So the biggest benefit is flexibility So a Document_Type_Definition makes your eXtensible_Markup_Language data have to conform to a specification If you want more flexibility or you want ease of change in the way that the data is formatted without running into a lot of errors then if that's what you want then the Document_Type_Definition can be constraining Another fact is that DTDs can be fairly messy and this is not going to be obvious to you yet until we get into the demo but if the data is irregular very irregular then specifying its structure can be hard especially for irregular documents Actually when we see the schema language we'll discover that XSDs can be I would say really messy so they can actually get very_large It's possible to have a document where the specification of the structure of the document is much much larger than the document itself which seems not entirely intuitive but when we get to learn_about XSDs I think you'll see how that can happen So overall this is the benefits of nil typing It' s really quite similar to the analogy in programming_languages The remainder of this video will teach about the DTDs themselves through a set of examples We'll have a separate video for learning_about eXtensible_Markup_Language schema and XSDs So here we are with our first document that we're going to look_at with a document type descriptor We have on the left the document itself We have on the right the document type descriptor and then we have in the lower right a command line shell that we're going to use to validate the document So this is similar data to what we saw on the last video but let's go through it just to see what we have We have an outermost element called bookstore and we have two books in our bookstore The first book has an ISBN_number price and editions As attributes and then it has a sub_element called title another sub_element called authors with two authors underneath first names and last names The second book element is similar except it doesn't have a edition It also has as we see a remark Now let's take a look_at the Document_Type_Definition and I'm just going to walk through Document_Type_Definition not too slowly not too fast and explain exactly what it's doing So the start of the Document_Type_Definition says this a Document_Type_Definition named bookstore and the root_element is called bookstore and now we have the first grammar like construct So these constructs in fact are a little_binary_digit like regular expressions if you know them What this says is that a bookstore element has as its sub_element any number of elements that are called book or magazine We have book or magazine We don't have any magazines yet but we'll add one And then this star says zero or more instances It's the Kleene for those of you familiar with regular expression Now let's talk_about what the book element has so that's our next specification The book element has a title followed_by authors followed_by an optional remark So now we don't have an or we have a comma and that says that these are going to be in that order title authors and remark and the question mark says that the remark is optional Next we have the attributes of our book elements So this bang attribute list says we're going to describe the attributes and we're going to have three of them the ISBN the price and the edition C data is the type of the attribute It's just a string And then required says that the attribute must be present whereas implied says it doesn't have to be present As you may remember we have one book that doesn't have an edition Our magazines are simply going to have titles and they're going to have attributes that are month and year Again we don't have any magazines yet A title is going to consist of string data So here we see our title of first course and database system You can think of that as the leaf data in the eXtensible_Markup_Language tree And when you have a leaf that consists of text data this is what you put in the Document_Type_Definition just take my word for it hash Personal Computer data in parentheses Now our authors are an element that still has structure Our authors have a sub_element author sub_elements or elements and we're going to specify here that the author's element must have one or more author subelements So that's what the plus is saying here again taken from regular expressions Plus means one or more instances We have the remark which is just going to be pc data or string data We have our authors which consist of a first name sub_element and a last_name sub_element and in that order And then finally our first names and last names are also strengths So this is the entire Document_Type_Definition and it describes in detail the structure of our document Now we have a command we're using something called xmllint that will check to see if the document meets the structure We'll just run that command here with a couple of options and it doesn't give_us any output which actually means that our document is correct Well be making some edits and seeing when our document is not correct what_happens when we run the command So let's make our first edit let's say that we decide that we want the additional attribute of our books to be required rather_than applied So we'll change the Document_Type_Definition We'll save the file and now when we run our command So as expected we got an_error and the error said that one of our book elements does not have attribute addition Now that addition is required every book element ought to have it So let's add an addition to our second book Let 's say that it's the second edition save the file we'll validate our document again and now everything is good Let's do an edit to the document this time to see what_happens when we change the order of the first name and the last_name So we've swapped Jeffrey_Ullman to be Ullman Jeffery We validate our document and now we see we got an_error because the elements are not in the correct order In this case let's undo that change rather_than change our Document_Type_Definition Let's try another edit to our document Let's add a remark to our first book But what we'll do is we'll leave the remark empty so we'll add a opening and then directly a closing_tag and let's see if that validates So it did validate And in fact when we have Personal Computer data as the type of an element it's perfectly acceptable to have a empty element As a final change let's add a magazine to our database You'll have to bear with me as I type I'm always a little_binary_digit slow So we see over here that when we have a magazine there are two required attributes the month and the year So let's say the month is January and the year let's make that and then we have a title for our magazine Here We'll go down here Our title let's make it National_Geographic We'll close the tag title tag And then sorry again about my typing Let's go_ahead and validate the document we saw premature end of something or other We forgot our closing_tag for magazine let's put that in My terrible typing and here we go Let's validate and we're done Now we're gonna learn_about and id rep attributes The document on the left_side contains the same data as our previous document but completely restructured Instead of having authors as subelements of book elements we're going to have our authors listed separately and then effectively point from the books to the authors of the book We'll take a look_at the data first and then we'll look_at the Document_Type_Definition that describes the data Let's actually start with the author so our bookstore element here has two subelements that are books and three that are authors So looking_at the authors we have the first name and last_name as sub_elements as usual but we've added what we call the ident attribute That's not a keyword we've just called the attribute ident and then for each of the three authors we've given a string value to that attribute that we're going to use effectively for the pointers in the book So we have our three authors now let's take a look_at the books Our book has the ISBN_number and price I've taken the addition out for now special attribute called authors Authors is an ID reps attribute and it's value can refer to one or more strings that are ID attributes attributes in another element So that's what we're doing here We're referring to the two author elements here And in our second book we're referring to the three author elements We still have the title subelement and we still have the remarks subelement And furthermore we have one other cute thing here which is instead of referring to the book by name within the remark when we're talking_about the other book we have another type of pointer So we'll specify that the ISBN is an ID for books and then this is an id reps attribute that's referring to the id of the other book The Document_Type_Definition on the right that describes the structure of this document This time our bookstore is going to contain zero or more books followed_by zero or more authors Our books contain a title and an optional remark is subelements and now they contain three attributes the IDBN which is now a special type of attribute called and ID the price which is the string value as usual and the authors which is the special type called id reps Let's keep going our title is just string Value as usual A remark here this is a actually interesting construct A remark consist of the Personal Computer data which is string or a book reference and then zero more instances of those This is the type of construct that can be used to mix strings and sub_elements within an element So anytime you want an element that might have some strings and then another element and then more string value That's how it's done Personal Computer data or the element type zero or more Then we have our book reference which is actually an empty element it's only interesting because is has an attribute so let's go_back here we see our book wrap here it actually doesn't have any data or sub_elements but it has an attribute called book and that is an ID ref That_means it refers to an ID attribute of another another element Now we have our authors the first name and the last_name and our author attributes have again an ID and we're calling it the ident And finally the first name and last_name are string values This may seem overwhelming but the key points in this Document_Type_Definition are the ID the attributes So the ID attributes the ISBN attributes in the book and the ident wherever it went ident attribute in the author are special attributes and by the way they do need to be unique values for those attributes and they're special in that ID refs attributes can refer to them and that will be checked as_well Now I did want to point out that the book reference here says ID ref singular When you have a singular ID ref then the string has to be exactly one ID value When you have the plural ID refs Then the string of the attribute is one or more ID ref value I'm_sorry one or more ID values separated by spaces So it's a little_binary_digit clunky but it does seem to work Now let's go to our command line and let's validate the document So the document is in fact valid That's what it means when we get nothing back and let's make some changes as we did before to explore what structure is imposed and what's checked with this Document_Type_Definition in the presence IDs and ID refs As a first change let's change this ID this identifier HG to JU That should actually cause a couple of problems when we do that let's validate the document and see what_happens And we do in fact get two different errors The first error says that we have two_instances of JU As you can see here we now have JU twice where ID values do have to be unique They have to be globally unique throughout the document The second error that occurred when we changed HG to JU is we effectively have a dangling pointer We refer to HG here in this ID refs attribute but there's no_longer an element whose value is HG So that's an_error as_well So let's change it back to HG just so our document is valid again Now let's make another change let's take our book reference We can see that our book reference is referring to the other book We're in the complete book here and the comment the remark is referring to the first course through the ISBN_number but let's change this string instead to refer to HG So now we're actually referring to an author rather_than another book Let's check if the document validates In fact it does And that shows that the pointers when you have a Document_Type_Definition are untyped So it does check to make_sure that this is an id of another element but we weren't able to specify that it should be a book element in our Document_Type_Definition and since we're not able to specify it of course it's not possible to check it We will see that in eXtensible_Markup_Language schema we can have typed pointers but it's not possible to have them in DTDs The last change I'm going to show is to add a second book reference within our remark So as I pointed out over here when we write Personal Computer data or in an element type Kleene the zero or more star that means we can freely mix text and sub_elements So just right in the middle here let's put a book reference and we can put let's say book equals JU and that will be the end of our reference there and now we see that we have text followed_by a subelement followed_by more text then so on That should validate fine and in fact it does That completes our demonstration of eXtensible_Markup_Language documents with DTDs In this video we'll be learning_about eXtensible_Markup_Language schema Like document_type_descriptors eXtensible_Markup_Language schema allows_us a way to give content specific specifications for our eXtensible_Markup_Language data As you may remember we send to a validating eXtensible_Markup_Language parser or eXtensible_Markup_Language document as_well as a description We talked_about DTDs in the last video We'll talk_about XSDs in this one The validating eXtensible_Markup_Language parser will check that the document is well_formed and it will also check that it matches it's specification If it does eXtensible_Markup_Language comes out If it doesn't we get an_error that the document is not valid eXtensible_Markup_Language schema is an extensive language very_powerful Like document_type_descriptors we can specify the elements we want in our eXtensible_Markup_Language data the attributes the nesting of the elements how elements need to be ordered and and number of occurrences of elements In addition we can specify data types we can specify keys the pointers that we can specify are now typed like in DTDs and much much more Now one difference between eXtensible_Markup_Language schema and DTDs is that the specification locations in eXtensible_Markup_Language schemas called XSD's are actually written in the xml language itself That can be useful for example if we have a browser that nicely renders the eXtensible_Markup_Language The languages I said is vast In this video we're going to show one sort of quote easy example But that example will give very much the flavor of eXtensible_Markup_Language schema And we'll try to highlight the differences between eXtensible_Markup_Language schema and using document_type_descriptors Ok here were are with our eXtensible_Markup_Language document on the left On the right we have our eXtensible_Markup_Language schema descriptor or XML_Schema_Definition and we have a little command line that we're gonna use for our validation command Now let_me just say up_front that we're not going to be going through the XML_Schema_Definition line by line in detail the way we did with DTDs As you can see it's rather long and that would take us far too long and be rather boring So what I highly suggest is that you download the file for the XML_Schema_Definition so you can look_at it yourself and look_at the entire file as_well as the eXtensible_Markup_Language and give it a try with validating What I'm gonna do in this demo primarily is focus_on those aspects of the XML_Schema_Definition that are different are more powerful than we had in document_type_descriptors First let's take a look_at the data itself So we have our bookstore data as usual with two books and three authors Its slightly restructured from any of the versions we've used before It looks closest to the last one we used because the books and authors are separate and the authors are actually exactly the same The have an identifier and a first name last_name sub_element But the primary difference is in the books instead of using ID refs attributes to refer from books to authors we still we now back our back having an author's sub_element with the two authors underneath and then those authors themselves have what are effectively the pointers to the identifiers for the authors And we'll see how that's going to mesh with the eXtensible_Markup_Language schema descriptor that we're using for this file So the other thing I want to mention is that right now we have the eXtensible_Markup_Language schema descriptor in one file and the eXtensible_Markup_Language in another You_might remember for the Document_Type_Definition we simply placed the DTDs specification at the top of the file with the eXtensible_Markup_Language For DTDs you can do it either way in the same file or in a separate file For XSDs we always put those in a separate file Also notice that the XML_Schema_Definition itself is in eXtensible_Markup_Language It is using special tags These are tags that are part of the XML_Schema_Definition language but we are still expressing it in eXtensible_Markup_Language So we have two eXtensible_Markup_Language files the data file and the schema file To validate the data file against the schema file we can use again the eXtensible_Markup_Language link feature We specify the schema file the data file and when we execute the command we can see that the file validates correctly So I'm now going to highlight four features of eXtensible_Markup_Language schema that aren't present in DTD's One of them is typed values One of them is key declarations Similar to IDs but a little_binary_digit more powerful One is references which are again similar to pointers But a little_more powerful and finally a currents constraints So let's start with tights In our data we see that the price attribute is denoted with a string and when we had DTDs all attribute values were in fact stringed In excess fees we can say that we want to check that the values which are still look like strings actually confirm to specific types For_example we can say that the price must be in integer Again I'm not going to be labor the syntactic details but rather I'm just going to highlight the places in the XML_Schema_Definition where we're declaring things of interest So specifically here's where we declare the attribute price and we say that the type of price must be an integer So our document validated correctly what if we change this one hundred to be foo instead Of_course with a Document_Type_Definition this would be fine because all attributes are treated as strings But if we try to validate now we see an_error specifically foo is not a value of the correct type So let's change that foo back to a hundred so that we validate correctly Next let's talk_about keys In DTD's we were able to specify ID's ID's were globally unique values that could be used to identify specific elements For_example when we wanted to point to those elements using ID refs Keys are a little_binary_digit more powerful or more specific I should say If you think_about the relational_model a key in the relational_model is an attribute or set of attributes that must be unique for each tuple in a table So we don't have tables or tuples right now but we do have elements and we often have repeated elements So similarly we can specify that a particular attribute or component must be unique within every element of the same type And we have two keys in our specification one key which we can see here for books and one for authors Specifically we say for books that the ISBN attribute must be a key And we say for authors that the ident attribute must be a key So let's go over to our data and let's start_by looking_at the authors So if we change for example U to HG then we should get a key violation because we'll have two authors that have the same ident attribute Let's try to validate In fact we do correctly get a key validation we also get a couple of other errors and those have to do with the fact that we are using these items as the destination of what are affect doubly pointers or references So let's change that back to JU make_sure everything now validates fine and it does Now lets make another change So we have the ident key here and we have the ISBN_number being the number for books what if changed the ISBN_number to one of the values we used as a key for the author say HG When we did something similar with DTDs we got an_error because in DTDs IDs have be globally unique Here we should not get an_error HG should be a perfectly reasonable key for books because we don't have another value that's the same And in fact it does validate Now let's undo that change Next let's talk_about references So references allow_us to have what are Possibly typed pointers using the dtd So they are called key refs and here we have an example let_me just change this to the middle of the document So one of the reference types that we've defined in our Document_Type_Definition is a pointer to authors that we're using in our books Specifically we want to specify that this attribute here the auth ident has a value that is a key for the author elements And we want to make_sure it's author elements that its pointing to and not other types of elements Now the syntax for doing this in eXtensible_Markup_Language schema is rather detailed Its alright here and just to give you a flavor this middle selector here is actually using the XPath language which we'll be using which we'll be learning later but what it says is that when we navigate in the document down to one of these auth elements Within that auth element the auth ident attribute is a reference to what we have already defined as author keys We've done something similar with books We have our book remark bookref that brings us down to this element here And there we specified that the book attribute must be a reference to a book key and the book key was earlier defined to be the ISBN_number Again I know this is all complicated and the syntax is very clunky so I urge you to download the specification and spend time looking_at it on your_own Now let's make a couple of changes to our document to demonstrate how the checking of these typed pointers works For_example lets change our first reference here to food Let's validate the document and we should get an_error and indeed we do the author key rep is incorrect Now lets change that FU to JW so originally it was JU But now we're going to have two authors both of whom refer to JW Now this should not be a problem It's simply two pointers to the same author and we did not prohibit that in our XMLs schema specification and indeed our document validates We'll change that one back And the last as a last change we'll change our book reference here to refer to JW This should not validate because this time unlike with DTDs we're we've actually specified typed pointers In_other_words we've specified that this pointer or this reference must be to a book element and not to an author element So we'll validate and indeed it fails I've undone that change and now let's move to the last feature that we're gonna look_at in this demonstration which is a currents constraint So in let_me just bring up the first instance of it in eXtensible_Markup_Language schema we can specify how_many times an element type is allowed to occur Specifically we can specify the minimum number of occurrences and the maximum number of occurrences As a default if we don't specify for an element the minOccurs or maxiOccurs the default for both of them is one So here for books we've said that we can have zero Books and we can have any number So this is the maximum flexibility any number of elements For authors we've also said we can have any number of authors that's in the actual database itself Remember that our book store consists of a set of books and a set of authors But we are going to specify something a little different for how_many authors we have within a specific book So let's continue to look_at other cases where we've specified occurrence constraints Here is the case where we're specifying how_many authors we have within a book and again few boy this is a lot of eXtensible_Markup_Language here so take your time when looking_at it or for now just take my word for it What we're specifying here is how_many sub_elements how_many auth sub_elements we have within each author's element And here we have no minOccurs specification only a maxOccurs That_means by default minOccurs is one So what this is saying specifically is that every book has in it's authors sub_element atleast one off but we can have any number of them that's the string unbounded Looking at the remaining occurrence constraints for remarks we have the minimum number of occurrences is zero In_other_words we don't have to have a remark And we haven't specified max occurs so the default max occurs is one So what we're saying here is that every book may have either no remark or exactly one remark but it may not have more_than that And there's a few more occurrence constraints that you can take a look_at again as you browse the eXtensible_Markup_Language schema description on your_own Now let's make some changes in the document to test these occurrence constraints So first let's remove the authors from our first book We won't remove the whole author sub_element but just the two off sub_elements of authors We attempt to validate and we see that it doesn't validate We're missing some child elements specifically the off child elements because we expected there to be at_least one of them Incidentally if we took the entire author sub_element out we'll also get an_error since we've specified the books must have author sub_element So now we're missing the entire author structure in that book and again we don't validate Let's put authors back and now let's look_at the remark occurrence constraint so we said that every book can have zero or one remarks so let's just add another remark to this book Oh hi actually remarks are allowed to be empty In any case we have added a small remark We validate and we see that we have too many remarks again because we specified that every book can have at most one remark So that concludes our demonstration of eXtensible_Markup_Language schema again it's been rather cursory we've only covered a few of the constructs but I did focus_on the constructs that we have in eXtensible_Markup_Language schema that are not specifiable in DTDs Finally one more time I urge you download the access fee and the document and play around with it yourself This video introduces JavaScript_Object_Notation Let's start_by talking_about its pronunciation Some people call it Jason and some call it J sahn I'll do a little_binary_digit of investigation and discovered that the original developer of JavaScript_Object_Notation calls it JavaScript_Object_Notation so I'll do that too Like eXtensible_Markup_Language JavaScript_Object_Notation can be thought of as a data_model An alternative to the relational data_model that is more appropriate for semi_structured data In this video I'll introduce the basics of JavaScript_Object_Notation and I'll actually compare JavaScript_Object_Notation to the relational data_model and I'll compare it to eXtensible_Markup_Language But it's not crucial to have watched those videos to get something out of this one Now among the three models the relational_model eXtensible_Markup_Language and JavaScript_Object_Notation JavaScript_Object_Notation is by a large margin the newest and it does show there aren't as many tools for JavaScript_Object_Notation as we have for eXtensible_Markup_Language and certainly not as we have for relational JavaScript_Object_Notation stands for Javascript object notation Although it's evolved to become pretty_much independent of Javascript at this point The little snippet of JavaScript_Object_Notation in the corner right now mostly for decoration We'll talk_about the details in just a minute Now JavaScript_Object_Notation was designed originally for what's_called serializing data objects That is taking the objects that are in a program and sort of writing them down in a serial fashion typically in files one thing about JavaScript_Object_Notation is that it is human readable similar to the way xml is human readable and is often use for data interchange So for writing out say the objects program so that they can be exchanged with another program and read into that one Also just more generally because JavaScript_Object_Notation is not as rigid as the relational_model it's generally useful for representing and for storing data that doesn't have rigid structure that we've been calling semi_structured data As I_mentioned JavaScript_Object_Notation is no_longer closely tied to Many different programming_languages do have parsers for reading JavaScript_Object_Notation data into the program and for writing out JavaScript_Object_Notation data as_well Now let's talk_about the basic constructs in JavaScript_Object_Notation and as we will see this constructs are recursively_defined We'll use the example JavaScript_Object_Notation data shown on the screen and that data is also available in a file for download from the website The basic atomic values in JavaScript_Object_Notation are fairly typical We have numbers we have strings We also have Boolean Values although there are none of those in this example that's true and false and no values There are two types of composite values in JavaScript_Object_Notation objects and arrays Objects are enclosed in curly_braces and they consist of sets of label_value_pairs For_example we have an object here that has a first name and a last_name We have a more bigger let's say object here that has ISBN price edition and so on When we do our JavaScript_Object_Notation demo we'll go into these constructs in more_detail At this point we're just introducing them the second type of composite value in JavaScript_Object_Notation is arrays and arrays are enclosed in square_brackets with commas between the array elements Actually we have commas in the objects as and arrays are list of values For_example we can see here that authors is a list of author objects Now I_mentioned that the constructs are recursive specifically the values inside arrays can be anything they can be other arrays or objects space values and the values are making up the label_value_pairs and objects can also be any composite value or a base value And I did want to mention by the way that sometime this word label here for label_value_pairs is called a property So just like eXtensible_Markup_Language JavaScript_Object_Notation has some basic_structural_requirements in its format but it doesn't have a lot of requirements in terms of uniformity We have a couple of examples of heterogeneity in here for example this book has an edition and the other one doesn't this book has a remark and the other one doesn't But we'll see many more examples of heterogeneity when we do the demo and look into JavaScript_Object_Notation data in more_detail Now let's compare JavaScript_Object_Notation and the relational_model We will see that many of the comparisons are fairly similar to when we compared eXtensible_Markup_Language to the relational_model Let's start with the basic structures underling the data_model So the relational_model is based_on tables We set_up structure of table a set of columns and then the data becomes rows in those tables JavaScript_Object_Notation is based instead on sets the sets of label pairs and arrays and as we saw they can be nested One of the big differences between the two models of course is the scheme So the Relational model has a Schema fixed in advance you set it up before you have any data loaded and then all data needs to confirm to that Schema JavaScript_Object_Notation on the other other_hand typically does not require a schema in advance In fact the schema and the data are kinda mix together just like an xml and this is often referred to as self describing data where the schema elements are within the data itself And this is of course typically more flexible than the to a model But there are advantages to having schema sp as_well definitely As far as queries go one of the nice features of the relational_model is that there are simple expressive languages for clearing the database In terms of JavaScript_Object_Notation although a few New things have been proposed at this point there's nothing widely_used for querying JavaScript_Object_Notation data Typically JavaScript_Object_Notation data is read into a program and it's manipulated programatically Now let_me interject that this video is being made in February So it is possible that some JavaScript_Object_Notation query languages will emerge and become widely_used there is just nothing used at this point There are some proposals There's a JavaScript_Object_Notation path language JavaScript_Object_Notation Query a language called jaql It may be that just like eXtensible_Markup_Language the query language are gonna follow the prevalent use of the data format or the data_model But that does not happened yet as of February How about ordering One aspect of the relational_model is that it's an unordered model It's based_on sets and if we want to see relational data in sorted order then we put that inside a query In JavaScript_Object_Notation we have arrays as one of the basic data structures and arrays are ordered Of_course there's also the fact like eXtensible_Markup_Language that JavaScript_Object_Notation data is often is usually written files and files themselves are naturally ordered but the ordering of the data in files usually isn't relevant sometimes it is but typically not finally in terms of implementation for the relational_model there are systems that implement the relational_model natively They're very generally quite efficient and powerful systems For JavaScript_Object_Notation we haven't yet seen stand alone database_systems that use JavaScript_Object_Notation their data_model instead JavaScript_Object_Notation is more typically coupled with programming_languages One thing I should add however JavaScript_Object_Notation is used in NoSQL_systems We do have videos about NoSQL_systems you may or may not have have watched those yet There's a couple of different_ways that JavaScript_Object_Notation is used used in those systems One of them is just as a format for reading data into the systems and writing data out from the systems The other way that it is used is that some of the note systems are what are called Document Management Systems where the documents themselves may contain JavaScript_Object_Notation data and then the systems will have special features for manipulating the JavaScript_Object_Notation in the document is better stored by the system Now let's compared JavaScript_Object_Notation and eXtensible_Markup_Language This is actually a hotly debated comparison right now There are signification overlap in the usage of JavaScript_Object_Notation and eXtensible_Markup_Language Both of them are very good for putting semi_structured data into a file format and using it for data interchange And so because there's so much overlap in what they're used for it's not surprising that there's significant debate I'm not gonna take sides I'm just going to try to give you a comparison Let's start_by looking_at the verbosity of expressing data in the two languages So it is the case that eXtensible_Markup_Language is in general a little_more verbose than JavaScript_Object_Notation So the same data expressed in the formats will tend to have more characters in eXtensible_Markup_Language than JavaScript_Object_Notation and you can see that in our examples because our big Json example was actually pretty_much the same data that we used when we showed eXtensible_Markup_Language And the reason for eXtensible_Markup_Language being a binary_digit more verbose largely has to do actually with closing_tags and some other features But I'll let you judge for yourself whether the somewhat longer expression of eXtensible_Markup_Language is a problem Second is complexity and here too most people would say that eXtensible_Markup_Language is a binary_digit more complex than JavaScript_Object_Notation I'm not sure I entirely agree with that comparison If you look_at the subset of eXtensible_Markup_Language that people really use you've got attributes sub_elements and text and that's more or less it If you look_at Json you got your basic values and you've got your objects and your arrays I think the issue is that eXtensible_Markup_Language has a lot of extra stuff that goes along with it So if you read the entire eXtensible_Markup_Language specification It will take you a long time JavaScript_Object_Notation you can grasp the entire specification a little_binary_digit more quickly Now let's turn to validity And by validity I_mean the ability to specify constraints or restriction or schema on the structure of data in one of these models and have it enforced by tools or by a system Specifically in eXtensible_Markup_Language we have the notion of document_type_descriptors or DTDs we also have eXtensible_Markup_Language Schema which gives_us XSD's eXtensible_Markup_Language Schema Descriptors And these are schema like things that we can specify and we can have our data checked to make_sure it conforms to the schema and these are I would say fairly widely_used at this point for eXtensible_Markup_Language For JavaScript_Object_Notation there's something called JavaScript_Object_Notation Schema And you know similar to eXtensible_Markup_Language Schema it's a way to specify the structure and then we can check that JavaScript_Object_Notation conforms that and we will see some of that in our demo The current status February is that this is not widely_used this point But again it could really just be evolution If we look back at eXtensible_Markup_Language as it was originally proposed probably we didn't see a whole of lot of use of DTDs and in fact not as XSDs for sure until later on So we'll just have to see whether JavaScript_Object_Notation evolves in a similar way Now the programming interface is where JavaScript_Object_Notation really shines The programming interface for eXtensible_Markup_Language can be fairly clunky The eXtensible_Markup_Language model the attributes and sub_elements and so on don't typically match the model of data inside a programming language In fact that's something called the impedance mismatch The impedance miss match has been discussed in database_systems actually for decades because one of the original criticisms of relational database_systems is that the data structures used in the database specifically tables didn't match directly with the data structures and programming_languages So there had to be some manipulation at the interface between programming_languages and the database system and that's the mismatch So that same impedance mismatch is pretty_much present in eXtensible_Markup_Language wherein JavaScript_Object_Notation is really a more direct mapping between many programming_languages and the structures of JavaScript_Object_Notation Finally let's talk_about querying I've already touched on this a binary_digit but JavaScript_Object_Notation does not have any mature widely_used query languages at this point for eXtensible_Markup_Language we do have XPath we have XQuery we have eXtensible_Stylesheet_Language_Transformations Maybe not all of them are widely_used but there's_no question that XPath at_least and eXtensible_Stylesheet Language are used quiet a binary_digit As far as Json goes there is a proposal called Json path It looks actually quiet a lot like XPath maybe he'll catch on There's something called JavaScript_Object_Notation Query Doesn't look so much like eXtensible_Markup_Language Query I_mean XQuery and finally there has been a proposal called JAQL for the JavaScript_Object_Notation query language but again as of February all of these are still very early so we just don't know what's going to catch on So now let's talk_about the validity of JavaScript_Object_Notation data So do JavaScript_Object_Notation data that's syntactically valid simply needs to adhere to the basic_structural_requirements As a reminder that would be that we have sets of label_value_pairs we have arrays of values and our values are from predefined types And again these values here are defined recursively So we start with a JavaScript_Object_Notation file and we send it to a the parser may determine that the file has syntactic errors or if the file is syntactically correct then it can parsed into objects in a programming language Now if we're interested in semantically valid JavaScript_Object_Notation that is JavaScript_Object_Notation that conforms to some constraints or a schema then in addition to checking the basics structural_requirements we check_whether JavaScript_Object_Notation conforms to the specified schema If we use a language like JavaScript_Object_Notation schema for example we put a specification in as a separate file and in fact JavaScript_Object_Notation schema is expressed in JavaScript_Object_Notation itself as we'll see in our demo we send it to a validator and that validator might find that there are some syntactic errors or it may find that there are some symantic errors so the data could to be correct syntactically but not conform to the schema If it's both syntactically and semantically correct then it can move on to the parser where will be parsed again into objects in a programming language So to summarize JavaScript_Object_Notation stands for Java Script Object_Notation It's a standard for taking data objects and serializing them into a format that's human readable It's also very useful for exchanging data between programs and for representing and storing semi_structured data in a flexible fashion In the next_video we'll go live with a demonstration of JavaScript_Object_Notation We'll use a couple of JavaScript_Object_Notation editors we'll take a look_at the structure of JavaScript_Object_Notation data when it's syntactically correct We'll demonstrate how it's very flexible when our data might irregular and we'll also demonstrate schema checking using an example of JSON's_schema In this video we'll see a demonstration of JavaScript_Object_Notation data As a reminder JavaScript_Object_Notation stands for Java Script Object_Notation and it's a standard for writing data objects into human readable format typically in a file It's useful for exchanging data between programs and generally because it's quite flexible it's useful for representing and for storing data that's semi_structured A reminder of the basic constructs in JavaScript_Object_Notation we have the atomic value such as integers and strings and so on And then we have two types of composite things we have objects that are sets of label_value_pairs and then we have arrays that are lists of values In the demonstration we'll go through in more_detail the basic constructs of JavaScript_Object_Notation and we'll look_at some tactic correctness we'll demonstrate the flexibility of the data_model and then we'll look briefly at JSON's_schema not widely_used yet but still fairly interesting to look_at and we'll look_at some validation of JavaScript_Object_Notation data against a particular schema So here's the JavaScript_Object_Notation data that we're gonna be working with during this demo It's the same data that appeared in the slides in the introduction to JavaScript_Object_Notation but now we're going to look into the components of the data It's also by the way the same example pretty_much that we used for eXtensible_Markup_Language it's reformatted of course to meet the JavaScript_Object_Notation data_model but you can compare the two directly Lastly we do have the file for the data on the website and I do suggest that you download the file so that you can take a look_at it closely on your_own computer All right So let's see what we have right now we're in an editor for JavaScript_Object_Notation data It happens to be the Eclipse editor and we're going to make make some edits to the file after we look through the constructs of the file So this is JavaScript_Object_Notation data representing books and magazines and we have a little_more information_about our books and our magazines So at the outermost the curly brace indicates that this is a JavaScript_Object_Notation object And as a reminder an object is a set of label_value_pairs separated by commas So our first value is the label books And then our first element in the object is the label books and this big value and the second so there's only two label_value_pairs here is the label magazines and this big value here And let's take a look first at magazines So magazines again is the label and the value we can see with the square_brackets here is an array An array is a list of values and here we have two values in our array They're still composite values So we have two values each of which is an object a set of label_value_pairs Let_me mention sometimes people call these labels 'properties' by the way Okay So now we are inside our objects that are the elements in the array that's the value of magazines And each one of those has labels and values And now we're finally down to the base values So we have the title being National_Geographic a string the month being January a string and the year where is an integer And again we have another object here that's a different magazine with a different name month and happens to be the same year Now these two have exactly the same structure but they don't have to and we will see that as we start editing the file But before we edit the file let's go and look_at our books here The value of our other label_value pair inside the outermost object books is also an array and the array in this case also has just two elements so we've represented two books here It's a little_more complicated than the magazines but those elements are still objects that are label_value_pairs So we have now the ISBN the price the addition the title all either integers or strings and then we have one nested composite object which is the authors and that's an array again So the array again is indicated by the square_brackets And inside this array we have two authors and each of the authors has a first name and a last_name but again that uniformity is not required by the model itself as we'll see So as I_mentioned this is actually an editor for JavaScript_Object_Notation data and we're going to come_back to this editor in a moment But what I wanted to do is show the same data in a browser because browsers actually offer some nice features for navigating in JavaScript_Object_Notation So here we are in the Chrome browser which has nice features for navigating JavaScript_Object_Notation and other browsers do as_well We can see here again that we have an object in our JavaScript_Object_Notation data that consists of two label_value_pairs books and magazines which are currently closed and and then this plus allows_us to open them up and see the structure For_example we open magazines and we see that magazines is an array containing two objects We can open one of those objects and see that the three label_value_pairs Now we're at the lowest levels and similarly for the other object We can see here that Books is also an array and we go_ahead and open it up It's an array of two objects We open one of those objects and we see again the set of label_value_pairs where one of the values is a further nesting It's an array and we open that array and we see two objects and we open them and finally see the data at the lowest levels So again the browser here gives_us a nice way to navigate the JavaScript_Object_Notation data and see its structure So now we're back to our JavaScript_Object_Notation editor By the way this editor Eclipse does also have some features for opening and closing the structure of the data but it's not quite as nice as the browser that we use So we decided to use the browser instead What we are going to use the editor for is to make some changes to the JavaScript_Object_Notation data and see which changes are legal and which aren't So let's take a look_at the first change a very_simple one What if we forgot a comma Well when we try to save that file we get a little notice that we have an_error we expected an N value so that's a pretty straightforward mistake let's put that comma back Let's say insert an extra brace somewhere here for whatever_reason We accidentally put in an extra brace Again we see that that's marked as an_error So an_error that can be fairly common to make is to forget to put quotes around strings So for example this ISBN_number here if we don't quote it we're gonna get an_error As we'll see the only things that can be unquoted are numbers and the values null true and false So let's put our quotes back there Now actually even more common is to forget to put quotes around the labels in label_value_pairs But if we forget to quote that that's going to be an_error as_well You_might have noticed by the way when we use the browser that the browser didn't even show us the quotes in the labels But you do when you make the raw JavaScript_Object_Notation data you do need to include those quotes Speaking of quotes what if we quoted our price here Well that's actually not an_error because now we've simply turned price into a string and string values are perfectly well allowed anywhere Now we'll see when we use JSON's_schema that we can make restrictions that don't allow strings in certain places but just for syntactic correctness of JavaScript_Object_Notation data any of our values can be strings Now as I_mentioned there are a few values that are sort of reserved words in JavaScript_Object_Notation For_example true is a reserved word for a bullion value That_means we don't need to quote it because it's actually its_own special type of value And so is false And the third one is null so there's a built in concept of null Now if we wanted to use nil for whatever_reason instead of null well now we're going to get an_error because nil is not a reserved word and if we really wanted nil then we would need to actually make it a quoted string Now let's take a look inside our author list And I'm going to show you that arrays do not have to have the same type of value for every element in the array So here we have a homogeneous list of authors Both of them are objects with a first name and a last_name as separate label_value_pairs but if I change that first one the entire value to be instead of a composite one simply the string Jefferey Ullman Oops sorry about my typing there and that is not an_error it is allowed to have a string and then a composite object And we could even have an array and anything we want In an array when you have a list of values all you need is for each one to be syntactically a correct value in JavaScript_Object_Notation Now let's go visit our magazines for a moment here and let_me show that empty objects are okay So a list of label_value_pairs comprising an object can be the empty list And so now I've turned this magazine into having no information_about it but that is legal in JavaScript_Object_Notation And similarly arrays are allowed to be of zero length So I can take these authors here and I can just take out all of the authors and make that an empty list but that's still valid JavaScript_Object_Notation Now what if I took this array out altogether In that case now we have an_error because this is an object where we have label_value_pairs and every label_value pair has to have both a label and a value So let's put our array back and we can have anything in there so let's just make it fu and that corrects the error What if we didn't want an array here instead and we tried to make it say an object Well we're going to see an_error there because an object as a reminder and this is an easy mistake to make Objects are always label_value_pairs So if you want just a value that should be an array if you want an object then we're talking_about a label_value pair so we can just add fu as our value and then we're all set So what we've_seen so far is syntactic correctness Again there's_no required uniformity across values in arrays or in the label_value_pairs in objects we just need to ensure that all of our values our basic values are of the right types and things like our commas and curly_braces are all in place What we're gonna do next is look_at JSON's_schema where we have a mechanism for enforcing certain constraints beyond simple syntactic correctness If you've been very observant you might even have noticed that we have a second tab up here in our editor for a second JavaScript_Object_Notation file and this file is going to be the schema for our bookstore data We're using JavaScript_Object_Notation schema and JavaScript_Object_Notation schema like eXtensible_Markup_Language schema is expressed in the data_model itself So our schema description for this JavaScript_Object_Notation data is itself JavaScript_Object_Notation data and here it is And it's going to take a binary_digit of time to explain Now the first thing that you might notice is wow the schema looks more_complicated and in fact longer than the data itself Well that is true but that's mostly because our data file is tiny So if we had thousands you know tens of thousands of books and magazines our schema file wouldn't change but our data file would be much longer and that's the typical case in reality Now this video is not a complete tutorial about JSON's_schema There's many constructs in JSON's_schema that weren't needed to describe the bookstore data for example And even this file here I'm not gonna go through every detail of it right here You can download the file and take a look read a little_more about JavaScript_Object_Notation schema I'm just going to give the flavor of the schema specification and then we're going to work with validating the data itself to see how the schema and data work together But to give you the flavor here let's go through at_least some portions of the schema So in some sense the structure of the schema file reflects the structure of the data file that it's describing So the outermost constructs in the schema file are the outermost in the data file and as we nest it parallels the nesting Let_me just show a little_binary_digit here we'll probably look_at most of it in the context of validation So we see here that our outermost construct in our data file is an object And that's told to us because we have type as one of our built in labels for the schema So we we have an object with two properties as we can see here the book's property and the magazine's property And I use the word labels frequently for label_value_pairs that's synonymous with property value pairs Then inside the books property for example we see that the type of that is array so we've_got a label_value pair where the value is an array And then we follow the nesting and see that it's an array of objects And we go further down and we see the different label_value_pairs of the object that make up the books and nesting further into the authors and so on We see similarly for magazines that the value of the a label_value pair for magazines is an array and that array consists of objects with further nesting So what we're looking_at here is an online JavaScript_Object_Notation schema validator We have two windows On the left we have our schema and on the right we have our data and this is exactly the same data file and schema file that we were looking_at earlier If we hit the validate button hopefully everything should work and it does This tells_us that the JavaScript_Object_Notation data is valid with respect to the schema Now this system will of course find basic syntactic errors so I can take away a comma just like I did before and when I validate I'll get a parsing error that really has nothing to do with the schema What I'm going to focus_on now is actually validating semantic correctness of the JavaScript_Object_Notation with respect back to the constructs that we've specified in this schema Let_me first put that comma back so we start with a valid file So the first thing I'll show is the ability to constrain basic types and then the ability to constrain the range of values of those basic types And let's focus_on price So here we're talking_about the price property inside books and we specify in our schema that the type of the price must be an integer So for example if our price were instead a string and we went ahead and try to validate that we would get an_error Let's make it back into an integer but let's make it into the integer now instead of And why am I doing that Because the JavaScript_Object_Notation schema also lets me constrain the range of values that are allowed if we have a numeric value So not only in price did I say that it's an integer but I also said that it has a minimum and maximum value the integer of prices must be between and So if I try to make the price of and I validate I'm again getting an_error Now it's not a type error but it's an_error that my integer was outside of the allowed range I've put the price back to a hundred and now let's look_at constraints on string values JavaScript_Object_Notation schema actually has a little pattern matching language that can be used to constrain the allowable strings for a specific type of value We'll look_at ISBN_number here as an example of that We've said that ISBN is of type string and then we've further constrained in the schema that the string values for ISBN must satisfy a certain pattern I'm not gonna go into the details of this pattern matching language I'm just gonna give an example And in fact this entire demo is really just an example lots of things in JSON's_schema that we're not seeing What this pattern here says is that the string value for ISBN must start with the four characters ISBN and then can be followed_by anything else So if we go over to our data and we look_at the ISBN_number here and say we have a typo we forgot the I and we try to validate Then we'll see that our data no_longer matches our schema specification Now let's look_at some other constraints we can specify in JSON's_schema We can constrain the number of elements in an array We can give a minimum or maximum or both And I've done that here in the context of the authors array Remember the authors are an array that's a list of objects and here I've said that we have a minimum number of items of and a maximum number items of In_other_words every book has to have between one and ten authors So let's try for example taking out all of our authors here in our first book We actually looked at this before in terms of syntactic validity and it was perfectly valid to have an empty array But when we try to validate now we do get an_error and the reason is that we said that we needed between one and ten array elements in the case of authors Now let's fix that not by putting our authors back but let's say we actually decide we would like to be able to have books that have no authors So we can simply fix that by changing that minimum item to zero and that makes our data valid again and in fact we could actually take that minimum constraint out all together and if we do that our data is still going to be valid Now let's see what_happens when we add something to our data that isn't mentioned in the schema If you look carefully you'll see that everything that we have in the data so far has been specified in the schema Let's say we come along and decide were gonna also have ratings for our books So let's add here a rating label property with the value We go_ahead and validate you probaly think it's not going to validate properly but actually it did The definition of JavaScript_Object_Notation schema that it can constrain things by describing them but you can also have components in the data that aren't present in this schema If we want to insist that every property that is present in the data is also described in this schema then we can actually add a constraint to the schema that tells_us that Specifically under the object here we can put in a special flag which itself is specified as a label called additional properties And this flag if we set it to false and remember false can is actually a keyword in JSON's_schema tells_us that in our data we're not allowed to have any properties beyond those that are specified in the schema So now we validate and we get an_error because the property rating hasn't been defined in the schema If additional properties is missing or have the default value of true then the validation goes through Now lets take a look_at our authors that are still here Let's_suppose that we don't have a first name for our middle author here If we take that away and we try to validate we do get an_error because we specified in our schema and it's right down here that author objects must have both a first name and a last_name It turns_out that we can specify for every property that the property is optional So we can add to the description of the first name not only that the type is a string but that that property is optional so we say optional true Now let's validate and now we're in good shape Now let's take a look_at what_happens when we have object that has more_than one instance of the same label or same property So let's_suppose for example in our magazine the magazine has two different years and This is syntactically valid JavaScript_Object_Notation it meets the structure of having a list of label_value_pairs When we validate it we see that we can't add a second property year So this validator doesn't permit two_copies of the same property and it's actually kind of a parsing thing and not so much related to JSON's_schema Many parsers actually do enforce that labels or properties need to be unique within objects even_though technically syntactically correct JavaScript_Object_Notation does allow multiple copies So that's just something to remember the typical use of objects is to have unique labels sometimes are even called keys of which evokes a concept of them unique So typically they are unique They don't have to be for syntactic validity Usually when you wanna have repeated values it actually makes more sense to create an array I've taken away the second year in order to make the JavaScript_Object_Notation valid again Now let's take a look_at months I've used months to illustrate the enumeration constraint so we saw that we could constrain the values of integers and we saw that we can constrain strings using a pattern but we can also constrain any type by enumerating the values that are allowed So for the month we've set it a string type which it is but we've further constrained it by saying that string must be either January or February So if we try to say put in the string March we validate and we get the obvious error here We can fix that by changing the month back but maybe it makes more sense that March would be part of our enumeration type so we'll add March to the possible values for months and now we're good As a next example let's take a look_at something that we saw was syntactically correct but isn't going to be semantically correct which is when we have the author list be a mixture of objects and strings So let's put Jeffrey_Ullman here just as a string We saw that that was still valid JavaScript_Object_Notation but when we try to validate now we're gonna get an_error because we expected to see an object we have specified that the authors are objects and instead we got a string Now JavaScript_Object_Notation schema does allow_us to specify that we can have different types of data in the same context and I'm going to show that with a little_binary_digit of a simpler example here So let's first take away our author there so that we're back with a valid file And what I_am going to look_at is simply the year values So let suppose for whatever_reason that in our magazines one of the years was a string and the other year was an integer So that's not gonna work out right now because we have specified clearly that the year must be an integer In JavaScript_Object_Notation schema specifications when we want to allow multiple types for values that are used in the same context we actually make the type be an array So instead of just saying integer if we put an array here that has both integer and string that's telling us that our year value can be either an integer or a string and now when we validate we get a correct JavaScript_Object_Notation file That_concludes our demo of JavaScript_Object_Notation schema validation Again we've just seen one example with a number of the constructs that are available in JavaScript_Object_Notation schema but it's not nearly exhaustive there are many others and I encourage_you to read a binary_digit more about it You can download this data and this schema as a starting point and start adding things playing around and I think you'll get a good feel for how JavaScript_Object_Notation schema can be used to constrain the allowable data in a JavaScript_Object_Notation file This is the first of two videos where we learn_about relational_algebra Relational Algebra is a formal language It's an algebra that forms the underpinnings of implemented languages like Structured_Query_Language In this video we're going to learn the basics of the Relational Algebra Query_Language and a few of the most popular operators In the second video we'll learn some additional operators and some alternate notations for relational_algebra Now let's just review first from our previous_video on relational querying that queries over relational_databases operate_on relations and they also produce relations as a result So if we write a query that operates say on the three relations depicted here the result of that query is going to be a new relation And in fact we can post queries on that new relation or combine that new relation with our previous relations So let's start out with Relational Algebra For the examples in this video we're going to be using a simple college admission relations database with three relations The first relation the college relation contains information_about the college name state and enrollment of the college The second relation the student relation contains an ID for each student the student's name GPA and the size of the high_school they attended And finally the third relation contains information_about students_applying to colleges Specifically the student's ID the college name where they're_applying the major they're_applying for and the decision of that application I've underlined the keys for these three relations As a reminder a key is an attribute or a set of attributes whose value is guaranteed to be unique So for example we're going to assume the college names are unique student IDs are unique and that students will only apply to each college for a particular major one time So we're going to have a picture of these three relations at the bottom of the slides throughout the video The simplest query in relational_algebra is a query that is simply the name of a relation So for example we can write a query student and that's a valid expression in relational_algebra If we run that query on our database we'll get as a result a copy of the student relation Pretty straightforward Now what_happens next is that we're going to use operators of the relational_algebra to filter relations slice relations and combine relations So let's through those operators The first operator is the select operator So the select operator is used to pick certain rows out of a relation The select operator is denoted by a Sigma with a subscript that's the condition that's used to filter the rows that we extract from the relations So we're just going through three examples here The first example says that we want to find the students_whose GPA is greater_than So to write that expression in relational_algebra we write the sigma which is the selection operator as a subscript the condition that we're filtering for GPA greater_than and the relation over which we're finding that selection predicate So this expression will return a subset of the student table containing those rows where the GPA is greater If we want to filter for two conditions we just do an and of the conditions in the subscript of the sigma So if we want say students_whose GPA is greater_than and whose high_school_size is less_than a thousand we'll write select GPA greater_than We used a logical and operator a caret high_school_size is less_than a thousand and again we'll apply that to the student relation And once again the result of that will be a subset of the student relation containing the rows that satisfy the condition If we want to find the applications to Stanford for a Computer_Science major then we'll be applying a selection condition to the apply relation Again we write the sigma and now the subscript is going to say that the college name is Stanford and the major is Computer_Science Again the and operator and that will be applied to the apply relation and it will return as a result a subset of the apply relation So the general case of the select operator is that we have the sigma We have a condition as a subscript and then we have a relation name And we return as a result the subset of the relation Our_next operator is the Project Operator So the select operator picks certain rows and the project_operator picks certain columns So let's say we're interested in the applications but all we wanted to know was the list of ID's and the decisions for those applications The project_operator is written using the Greek pi symbol and now the subscript is a list of the column names that we would like to extract So we write ID sorry student_ID and decision and we apply that to the apply relation again And now what we'll get back is a relation that has just two rows It's going to have all the tuples of apply but it's only going to have the student_ID and the decision columns So the general case of a project_operator is the projection and then a list of attributes can be any number and then a relation name Now what if we're interested in picking both rows and columns at the same time So we want only some of the rows and we want only some of the columns Now we're going to compose operators Remember that relational queries produce relations So we can write a query say with the select operator of the students_whose GPA is greater_than And this is how we do that And now we can take that whole expression which produces a relation and we can apply the project_operator to that and we can get out the student_ID and the student name Okay So what we actually see now is that the general case of the selection and projection operators weren't quite what I told you at first I was deceiving you slightly When we write the select operator it's a select with the condition on any expression of the relational_algebra and if it's a big one we might want to put parenthesis on it and similarly the project_operator is a list of attributes from any expression of the relational_algebra And we can compose these as much as we want We can have select over project over select select project and so on Now let's talk_about duplicate values in the results of relational_algebra queries Let's_suppose we ask for a list of the majors that people have applied for and the decision for those majors So we write that as the project of the major and the decision on the applied relation You_might think that when we get the results of this query we're going to have a lot of duplicate values So we'll have Computer_Science yes Computer_Science yes Computer_Science no EE yes EE no and so on You can imagine in a large realistic database of applications there's going to be hundreds of people applying for majors and having a yes or a no decision The semantics of relational_algebra says that duplicates are always eliminated So if you run a query that would logically have a lot of duplicate values you just get one value for each result That's actually a binary_digit of a difference with the Structured_Query_Language language So Structured_Query_Language is based_on what's_known_as multi sets or bags and that means that we don't eliminate duplicates whereas relational_algebra is based_on sets themselves and duplicates are eliminated There is a multi set or bad relational_algebra defined as_well but we'll be fine by just considering the set relational_algebra in these videos Our first operator that combines two relations is the cross_product operator also known_as the Cartesian product What this operator does is it takes two relations and kinda glues them together so that their schema of the result is the union of the schemas of the two relations and the contents of the result are every combination of tuples from those relations This is in fact the normal set cross_product that you might have learned way back in the elementary school So let's talk_about say doing the cross products of students and apply So if we do this cross products just to save drawing I'm just gonna glue these two relations together here So if we do the cross_product we'll get at the result a big relation here which is going to have eight attributes The eight attributes across the student and apply now the only small little trick is that when we glue two relations together sometimes they'll have the same attribute and we can see we have SID on both_sides So just as a notational convention when cross_product is done and there's two attributes that are named they're prefaced with the name of the relation they came from So this one would be referred to in the cross_product as the student dot SID where this one over here would be referred to as the apply dot SID So again we glue together in the Cartesian product the two relations with four attributes each we get a result with eight attributes Now let's talk_about the contents of these So let's_suppose that the student relation had s tuples in it and that's how_many tuples while the apply had tuples in it the result of the Cartesian products is gonna have S times A tuples is going to have one tuple for every combination of tuples from the student relation and the apply relation Now the cross_product seems like it might not be that helpful but what is interesting is when we use the cross_product together with other operators And let's see a big example of that Let's_suppose that we want to get the names and GPAs of students with a high_school_size greater_than a thousand who applied to Computer_Science and were rejected Okay so let's take a look We're going to have to access the students and the apply records in order to run this query So what we'll do is we'll take student cross apply as our starting point So now we have a big relation that contains eight attributes and all of those tuples that we described previously But now we're going to start making things more interesting because what we're going to do is a big selection over this relation And that selection is first of all going to make_sure that it only combines student and apply tuples that are referring to the same student So to do that we write student dot SID equals apply dot SID So now we've filtered the result of that cross_product to only include combinations of student and apply by couples that make sets Now we have to do a little_binary_digit of additional filtering We said that we want the high_school_size to be greater_than a thousand so we do an and operator in the high_school We want them to have applied to Computer_Science so that's and major equals Computer_Science We're getting a nice big query here And finally we want them to have been rejected so and decision equals we'll just be using R for reject So now we've_got that gigantic query But that gets us exactly what we want except for one more thing which is as I said all we want is their names and GPAs So finally we take a big parentheses around here and we apply to that the projection operator getting the student name and the GPA And that is the relational_algebra expression that produces the query that we have written in English Now we have seen how the cross_product allows_us to combine tuples and then apply selection conditions to get meaningful combinations of tuples It turns_out that relational_algebra includes an operator called the natural_join that is used pretty_much for the exact purpose What the natural_join does is it performs a cross_product but then it enforces equality on all of the attributes with the same name So if we set_up our schema properly for example we have student_ID and student_ID here meaning the same thing and when the cross_product is created it's only going to combine tuples where the student_ID is the same And furthermore if we add college in we can see that we have the college name here and the college name here If we combine college and apply tuples we'll only combine tuples that are talking_about the same college Now in addition one more thing that it does is it gets rid of these pesky attributes that have the same names So since when we combine for example student and apply with the natural_join we're only combining tuples where the student SID is the same as the apply SID Then we don't need to keep two_copies of that column because the values are always going to be equal So the natural_join operator is written using a bow tie that's just the convention You will find that in your text editing programs if you look carefully So let's do some examples now Let's go_back to our same query where we were finding the names and GPAs of students from large high_schools who applied to Computer_Science and were rejected So now instead of using the cross_product we're gonna use the natural_join which as I said was written with a bow tie What that allows_us to do once we do that natural_join is we don't have to write that condition that enforced equality on those two attributes because it's going to do it itself And once we have done that then all we need to do is apply the rest of our conditions which were that the high_school is greater_than a thousand and the major is Computer_Science and the decision is reject again we'll call that R And then since we're only getting the names and GPAs we write the student name and the GPA Okay And that's the result of the query using a natural_join So as you can see that's a little_binary_digit simpler than the original with the cross_product and by setting up schemas correctly natural_join can be very useful Now let's add one more complication to our query Let's_suppose that we're only interested in applications to colleges where the enrollment is greater_than So so far in our expression we refer to the student relation and the apply relation but we haven't used the college relation But if we want to have a filter on enrollment we're going to have to bring the college relation into the picture This turns_out to perhaps be easier than you think Let's just erase a couple of our parentheses here and what we're going to do is we're going to join in the college relation with the two relations we have already Now technically the natural_join is the binary operator people often use it without parentheses because it's associative but if we get pedantic about it we could add that and then we're in good shape Now we've joined all three relations together And remember automatically the natural_join enforces equality on the shared attributes Very_specifically the college name here is going to be set equal to the apply college name as_well Now once we've done that we've_got all the information we need We just need to add one more filtering condition which is that the college enrollment is greater_than And with that we've solved our query So to summarize the natural_join the natural_join combines relations It automatically sets values equal when attribute names are the same and then it removes the duplicate columns The natural_join actually does not add any expressive_power to relational_algebra We can rewrite the natural_join without it using the cross_product So let_me just show that rewrite here If we have and now I'm going to use the general case of two expressions One expression natural_join with another expression that is actually equivalent to doing a projection on the schema of the first expression I'll just call it E now union the schema of the second expression That's a real union so that means if we have two_copies we just keep one of them Over the selection of Now we're going to set all the shared attributes of the first expression to be equal to the shared attributes of the second So I'll just write E A equals E A and E A equals E dot A Now these are the cases where again the attributes have the same names and so on So we're setting all those equal and that is applied over expression one cross_product expression two So again the natural_join is not giving_us additional expressive_power but it is very convenient notationally The last operator that I'm going to cover in this video is the theta_join operator Like natural_join theta_join is actually an abbreviation that doesn't add expressive_power to the language Let_me just write it The theta_join operator takes two expressions and combines them with the bow tie looking operator but with a subscript theta That theta is a condition It's a condition in the style of the condition in the selection operator And what this actually says it's pretty_simple is it's equivalent to applying the theta condition to the cross_product of the two expressions So you_might_wonder why I even mention the theta_join operator and the reason I mention it is that most database_management systems implement the theta_join as their basic operation for combining relations So the basic operation is take two relations combine all tuples but then only keep the combinations that pass the theta condition Often when you talk to people who build database_systems or use databases when they use the word join they really mean the theta_join So in conclusion relational_algebra is a formal language It operates on sets of relations and produces relations as a result The simplest query is just the name of a relation and then operators are used to filter relations slice them and combine them So far we've learned the select operator for selecting rows the project_operator for selecting columns the cross_product operator for combining every possible pair of tuples from two relations and then two abbreviations the natural_join which a very useful way to combine relations by enforcing a equality on certain columns and the theta_join operator In the next_video we'll learn some additional operators of relational_algebra and also some alternative notations for relational_algebra expressions This is the second of two videos about the relational_algebra In the first video we learned about the select and project operators in various types of joins This video will cover set operators union difference and intersection the renaming operator and different notations for expressions of relational_algebra Just as a reminder we apply a relational_algebra query or expression to a set of relations and we get as a result of that expression a relation as our answer For our examples we're using an imaginary database about college_admissions We have a relation of colleges a relation of students and a relation with information_about students_applying to colleges We'll keep at the bottom of the video these three depictions of those relations with a few abbreviations used so that names aren't too long Let's move_ahead to our first operator The first of three set operators is the union_operator and it's a standard set union that you learned about in elementary school Let's_suppose for example that we want a list of the college and student names in our database So we just want those as list For_example we might want Stanford and Susan and Cornell and Mary and John and so on Now you might think we can generate this list by using one of the operators we've already learned for combining information from multiple relations such as the cross_product operator or the natural_join operator The problem with those operators is that they kind of combine information from multiple relations horizontally They might take a tuple T from one relation and tuple T from the other and kind of match them But that's not what we want to do here We want to combine the information vertically to create our list And to do that we're going to use is the union_operator So in order to get a list of the college names and the student names we'll project the college name from the college relation That gives_us a list of college names We'll similarly project the student name from the student relation and we've_got those two lists and we'll just apply the union_operator between them and that will give_us our result Now technically in relational_algebra in order to union two lists they have to have the same schema that means that same attribute name and these don't but we'll correct that later For now you get the basic_idea of the union_operator Our_next set operator is the difference_operator and this one can be extremely useful As an example let's_suppose we want to find the IDs of students_who didn't apply to any colleges It sounds like a complicated query but we'll actually write it in a very_simple fashion We'll start_by projecting the student_ID from the student relation itself and that will give_us all of this student IDs Then lets project the student_ID from the apply relation and that gives_us the IDs of all students_who have applied somewhere All we need to do is take the difference_operator written with the minus sign and that gives_us the result of our query It will take all IDs of the students and then subtract the ones who have applied somewhere Suppose instead that we wanted the names of the students_who didn't apply anywhere not just their IDs So that's a little_binary_digit more_complicated You_might think Oh just add student name to the projection list here but if we do that then we're trying to subtract a set that has just IDs from a set that has the pair of ID names And we can't have the student name here because the student name isn't part of the apply relation So there is a nice trick however that's going to do what we want Let_me erase these here What we're going to do is we're going to take this whole expression here which gives_us the student IDs who didn't apply anywhere and watch this Pretty clever We're gonna do a natural_join with the student relation And now that's called a join back So we've taken the IDs a certain select set of IDs and we've joined them back to the student relation That's going to give_us a schema that's the student relation itself and then we're just going to add to that a projection of the student name And that will give_us our desired answer The last of the three set operators is the intersection operator So let's_suppose we want to find names that are both a college name and a student name So perhaps Washington is the name of a student and a college To find those we're going to do something similar to what we've done in the previous examples Let's start_by getting the college names Then let's get the student names and then what we're going to do is just perform an intersection of those two expressions and that will give_us the result that we want Now like our previous example technically speaking the two expressions on the two sides of the intersection ought to have the same schema and again I'll show you just a little_binary_digit later how we take care of that Now it turns_out that intersection actually doesn't add any expressive_power to our language and I'm going to show that actually in two different_ways The first way is that if we have two expressions let's say E and E and we perform their intersection that is exactly equivalent to writing E minus using the difference_operator E minus E Now if you're not convinced of that immediately let's go_back to Venn diagrams again a concept you probably learned early in your schooling So let's make a picture of two circles And let's say that the first circle Circle represents the result of expression E and the second rear circle represents the result of expression E Now if we take the entire circle E Let's shade that in purple And then we take the result so that's E here and then we take E the result of the expression E minus E here we'll write that in green so that's everything in E that's not in E that's this Okay And if we take the purple minus the green you will see that we actually do get the intersection here So that's a simple property of set Operations but what that's telling us is that this intersection operator here isn't giving_us more expressive_power because any expression that we can write in this fashion we can equivalently right with the difference_operator in this fashion Let_me show you a completely different way in which intersection doesn't add any expressive_power So let's go_back to E intersect E and as a reminder for this to be completely correct these have to have the same schema as equal between the two E intersect E turns_out to be exactly the same as E natural_join E in this particular case because the schema is the same Remember what natural_join does Natural join says that you match up all columns that are equal and you eliminate duplicate values of columns So I'll just let you work out for yourself that this is indeed an equivalence and a second reason that the intersection doesn't add any expressive_power Nevertheless the intersection can be very useful to use in queries Our last operator is the rename_operator The rename_operator is necessary to express certain queries in relational_algebra Let_me first show the form of the operator and then we'll see it in use The rename_operator uses the Greek symbol rho And like all of our other operators it applies to the result of any expression of relational_algebra And what the rename_operator does is it reassigns the schema in the result of E So we compute E we get a relation as a result and it says that I'm going to call the result of that relation R with attributes A through An and then when this expression itself is embedded in more complex expression we can use this schema to describe the result of the E Again we'll see shortly why that's useful There are a couple of the abbreviations that are used in the rename_operator this form is the general form here One abbreviation is if we just want to use the same attribute names that came out of E but change the relation name we write row sub R applied to E and similarly if we want to use just the attribute names if we want to change I'm_sorry just the attribute names then we write attribute list here and it would keep the same relation name This form of course has to have a list of attributes or we would not be able to distinguish it from the previous form But again these are just abbreviations and the general form is the one up here Okay so now let's see the rename_operator in use The first use of the rename_operator is something I alluded to earlier in this video which is the fact that when we do the set operators the union difference and intersect operators we do expect the schemas on the two the sides of the operator to match and in a couple of our examples they didn't match and the rename_operator will allow_us to fix that So for example if we're doing the list of college and student names and let_me just remind_you how we wrote that query We took the C name from college and we took the s name from students and we did the big union of those Now to make this technically correct these two attribute names would have to be the same So we're just going to apply the rename_operator Let's say that we're gonna rename the result of this first expression to say the relation name C with attribute name And let's make the result of the second expression similarly be the relation C with attribute name And now we have two matching schemas and then we can properly perform the union_operator Again this is just a syntactic necessity to have well_formed relational_algebra expressions Now the second use of the rename_operator is a little_more complicated and quite a binary_digit more important actually which is disambiguation in self joins and you probably have no idea what I'm talking_about when I say that but let_me give an example Let's_suppose that we wanted to have a query that finds pairs of colleges in the same state Now think_about that So we want to have for example Stanford and Berkeley and Berkeley and UCLA and so on So that as you can see unlike the union_operator we're looking for this horizontal joining here So we're going to have to combine essentially two_instances of the college relation And that's exactly what we're going to do We're effectively going to do college join college making the state equal So let's work on that a little_binary_digit So what we wanna do is we wanna have college and we want to let's just start with say the cross_product of college And then we want to somehow say Well the state equals the state But that's not gonna work Which state are these And how do we describe the two_instances of college So what we're going to do and let_me just erase this is we're going to rename those two_instances of colleges so they have different names So we're going to take the first instance of college here and we're going to apply a rename_operator to that And we'll call it C and we'll say that that has name state and enrollment And then we'll take the second instance here We'll call it C so N S E of college and now we have two different relations So what we can do is we can take the cross_product of those two like that and then we can select where S equals S okay And that gives_us pairs of college in the same state Actually let_me show you an even trickier simpler way of doing this Let's take away the selection operator here okay And let's take away this And let's make this into a natural_join Now that's not gonna work quite yet because the natural_join requires attribute names to be the same and we don't have any attribute names that are the same So the last little trick we're gonna do is we're gonna make those two attribute names S be the same And now when we do the natural_join it's gonna require equality on those two S's and everything is gonna be great Okay Now things are still a little_binary_digit more_complicated One problem with this query is that we are going to get colleges paired with themselves So we're going to get from this for example Stanford Stanford If you think_about it right Berkeley Berkeley as_well as Stanford Berkeley Now that's not really what we want presumably Presumably we actually want different colleges but that's pretty easy to handle actually Let's put a selection condition here so that the name one is not equal to name two Great We took care of that So in that case we will no_longer get Stanford Standford and Berkeley Berkeley Ah but there's still one more problem We'll get Stanford Berkeley but we'll also get Berkeley Stanford Now let_me pause for a moment and see if you can think of a simple way to solve this problem Actually there's a surprisingly simple way kind of clever We're gonna take away this not equals and we're going replace it with a less_than And now we'll only get pairs where the first one is less_than the second So Stanford and Berkeley goes away and we get Berkeley Stanford And this is our final query for what we wanted to do here Now what I really wanted to show aside from some of the uses of relational_algebra is the fact that the rename_operator was for this query absolutely necessary We could not have done this query without the rename_operator Now we've_seen all the operators of relational_algebra Before we wrap_up the video I did want to mention that there are some other notations that can be used for relational_algebra expressions So far we've just been writing our expressions in a standard form with relation names and operators between those names and applying to those names But sometimes people prefer to write using a more linear notation of assignment statements and sometimes people like to write the expressions as trees So I'm just gonna briefly show a couple of examples of those and then we'll wrap_up So assignment statements are a way to break down relational_algebra expressions into their parts Let's do the same query we just finished as a big expression which is the pairs of colleges that are on the same state We'll start_by writing two assignment statements that do the rename of the two_instances of the college relation So we'll start with C colon equals and we'll use a rename_operator and now we use the abbreviated form that just lists attribute names So we'll see say C one S E one of college and we'll similarly say that C gets the rename and we'll call it C SE of college and remember we use the same S here so that we can do the natural_join So now we'll say that college pairs gets C natural_join C and then finally we'll do our selection condition So our final answer will be the selection where N is less_than N of CP And again this is equivalent to the expression that we saw on the earlier slide It's just a notation that sometimes people prefer to modularize their expressions The second alternate notation I'm going to show is expression trees And expression trees are actually commonly used in relational_algebra They allow you to visualize the structure of the expression a little_binary_digit better And as it turns_out when Structured_Query_Language is compiled in database_systems it's often compiled into an expression tree that looks very much like what I'm gonna show you right now So for this example let's_suppose that we want to find the GPAs of students_who are applying to Computer_Science in California So that's going to involve all three relations because we're looking_at the state is in California and we're looking_at the student GPA's and we're looking_at them applying to Computer_Science So what we're going to do is we're going to make a little tree notation here where we're going to first do the natural_join of these three relations So technically the expression I'm going to show you is going to stop down here It's not going to actually have the tables So the leaves of the expression are going to be the three relations college students and apply And in relational_algebra trees the leaves are always relation names And we're going to do the natural_join of those three which as a reminder enforces equality of the college name against the college name here against the college name here and the student_ID here and the student_ID here That enforcement means that we get triples that are talking_about a student applying to a particular college And then we're going to apply to that and so that's going to be written as a new note above this one in the tree The selection condition that says that the state equals California and the major equals Computer_Science And finally we'll put on top of that the projection that gets the GPA okay Now actually this expression is exactly equivalent to if we wrote it linearly project the GPA select etc of the three college join student join apply I'm just abbreviating here That would be an equivalent expression But again people often like to use the tree notation because it does allow you to visualize the structure of the expression and it is used inside implementations of the Structured_Query_Language language Let_me finish up by summarizing relational_algebra Let's start with the core constructs of the language So a relation name is a query in relational_algebra and then we use operators that combine relations and filter relations So we have the select operator that applies a condition to the result of an expression We have the project_operator that gives_us a set of attributes that we take from the result of an expression We have the expression one cross_product expression two And again those can be any expressions Then we have expression one union expression two And we have expression one minus expression two And finally we have the rename_operator that takes an expression and renames the result of that the schema in the result of that expression Now you probably noticed that I skipped a few of our favorite operators but this is the core of the language and all the other operators are actually abbreviations that don't increase the expressive_power of the language but they can be very useful performing queries And the abbreviations that we learned were expression one natural_join expression two They were expression one theta_join expression two And finally expression one intersect expression two All of those where we had a method of rewriting them using the core operators Just a small aside about parentheses A parentheses are used in relational expressions for relational algebraic expressions for disambiguation similarly to arithmetic expressions I was a little cavalier about whether I included parentheses or not but as you write your relational_algebra expressions you will see that it's pretty straightforward to figure_out when disambiguation is needed So to conclude relational_algebra entirely it's a formal language It's based_on sets set operators and other operators that combine data from multiple relations It takes relations as input it produces relations as answers and it does form the formal foundation of implemented relational database_management This video provides an introduction to the Structured_Query_Language query language Structured_Query_Language like the relational_model has been around for decades and supports a many billion dollar market The first thing you might be wondering is how you pronounce it Is it Structured_Query_Language or is it sequel My friends in industry tell me that sequel is the in pronunciation so that's the one I'll be using Now Structured_Query_Language is supported by all major commercial database_systems It has been around a long time and it is a standardized language The standard started out relatively simple but over the decades it's really ballooned There are currently thousands of pages in the Structured_Query_Language standard But the essence of the language which is what we'll be learning in these videos is still relatively simple We will be learning primarily the SQL standard also known_as Structured_Query_Language along with some constructs from the SQL standard When Structured_Query_Language is used it can be used in a database system interactively through a graphical user interface or a prompt so you type Structured_Query_Language queries or commands and you get results back or Structured_Query_Language can be embedded in programs So the most common use is to embed Structured_Query_Language in programs but for the demos in our videos naturally we'll be submitting queries through a Graphical User Interface interface The last thing I wanted to mention about Structured_Query_Language is that it is a declarative language That_means that in Structured_Query_Language you'll write pretty_simple queries that say exactly what you want out of the database and the queries do not need to describe how to get the data out of the database The language is also based_on relational_algebra and I_hope you've watched the relational_algebra videos Now the declarative nature of Structured_Query_Language leads to the component of the database system called the query optimizer to be extremely important What the query optimizer does is it takes a query written in a Structured_Query_Language language and it figures out the best way the fastest way to execute that on the database Now let's talk briefly fully about some terminology and the commands that are in the Structured_Query_Language language There's two parts of the language the Data Definition Language or Data Definition Language and the Data Manipulation or Definitive Media Library The Data Definition Language includes commands to create a table We saw that in a previous_video It also includes commands to drop table and to create and drop other aspects of databases that we'll be learning_about in later_videos such as indexes and views The Data Manipulation Language is the language that's used to query and modify the database So in the Structured_Query_Language language the Data Manipulation Language includes for querying the database the select statement and then for modifying the database an insert statement a delete statement and an update statement There are many other commands in Structured_Query_Language for indexes constraints views triggers transactions authorization all of which we'll be learning_about in later_videos For now let's just take a look in a little_more detail at the select statement which is really the bread and butter of the Structured_Query_Language language and it's what we use to query the database So the select statement consists of three basic clauses There's the SELECT clause the FROM clause and the WHERE clause The best order to think of these actually is first the FROM clause then the WHERE and then the SELECT and just the basic_idea is that the FROM identifies the relations that you want to query over the condition is used to combine the relations and to filter the relations And finally the SELECT tells you what to return Now if you're familiar with relational_algebra this expression here this Structured_Query_Language query is equivalent to the relational_algebra expression that you project the set of attributes A through AN And then you select and by the way it's different from this select here In fact this selection corresponds to the WHERE You select the condition on the cross_product of the relations that are listed in the from clause So that's the equivalent in relational_algebra And the last thing I wanted to mention is that as you know the relational query languages are compositional That_means when you run a query over relations you get a relation as a result So the result of this select statement is a relation It doesn't have a name but the schema of that relation is the set of attributes that are returned We'll learn much more about the SELECT statement in future videos In conclusion the Structured_Query_Language language is very prominent It's supported by all major commercial database_systems It's been standardized over time It can be used through programs It can be used interactively and it's a declarative high_level language whose foundations are based_on the relational_algebra This is the first of seven videos where we're going to learn the Structured_Query_Language language The videos are largely going to be live demos of Structured_Query_Language queries and updates running on an actual database The first video is going to focus_on the basics of the SELECT statement As a reminder the SELECT statement selects a set of attributes from a set of relations satisfying a particular condition We will see in the demo that even with the these three clauses we can write quite_powerful queries All of the seven demos are going to be using the simple_college_admissions database that we learned about in the relational_algebra videos As a reminder we have three relations We have the college relation college relation contains information_about the name of the colleges the state and the enrollment of those colleges We have the student relation which contains student IDs their names their GPA and the size of the high_school that they come from And finally the application information that tells_us that a particular student applied to a particular college for a particular major and there was a decision of that application Now as a reminder in the relational_model when we underline attributes that means we're designating a key for the relation So the underlying attributes in our example say that the knowledge name is going to be unique within the college relation The student's idea is unique within the student relation and in the applied relation the combination of these three attributes is unique That_means that student can if he or she wishes apply to a college many_times or apply for a major many_times but can only apply to a college for a particular major once Let's turn to the demo Let's start_by looking_at the actual data that we're going to be querying over We have a set of four colleges Stanford Berkeley Massachusetts_Institute of Technology and Cornell We have a bunch of students And a reminder each student has an ID a name a GPA and a size of high_school And finally we have a set of application records where a student with a particular ID applies to a college for a particular major and there's a yes or no decision on that application So let's go to our first Structured_Query_Language query This query is going to find the ID name and GPA of students_whose GPA is greater_than So very_simple it's the basic SELECT FROM WHERE structure The SELECT gives our table name the WHERE gives our filtering condition and the SELECT tells_us what we want to get out of the query We'll execute that query and we will find here all of our students with a GPA greater_than Now it's not necessary to include the GPA in the result to the query even if we filter on the GPA So I could just take GPA away from the SELECT clause run the query again and now we see the same result but without the GPA Okay Let's go to our second query Our second query is going to combine two relations In this query we're going to find the names of the students and the majors for which they've_applied So now we're involving both the student table and the apply table and the condition we see here is the join condition that tells_us we want to combine students with apply records that have the same student_ID This is what would happen automatically in a natural_join of the relational_algebra but in Structured_Query_Language we need to always write the join condition explicitly and finally we get the student name and the major And if we execute the query we get expectedly a bunch of students and the majors that they've_applied for Now we do notice here that we have several duplicate values We have two_copies of Amy applying to Computer_Science and two_copies of Craig applying to Bio Engineering As we discussed in the relational_algebra video in relational_algebra which underlies Structured_Query_Language it's by default the set model we don't have duplicates But in the Structured_Query_Language language we do have duplicates it's based_on a multi set model If we don't like the duplicates in our results Structured_Query_Language provides us a convenient way to get_rid of them We simply add the keyword distinct to our query after the word select we execute and now we get the same result but with the duplicate values eliminated Our_next query is going to be a little_more complicated it's going to find the names and GPAs of students_whose size_high_school is less_than a thousand they've_applied to Computer_Science at Stanford and we're going to get the decision associated_with that So again we have two relations two tables involved the student and the apply We have the join condition making sure we're talking_about the same student and the student and apply tuples Very important to remember that one We are going to filter the result based_on size_high_school major and the college to which they're_applying So let's run this query and we will see the result that we have two students_who have applied to Computer_Science at Stanford from a small high_school Our_next query is again a join of two relations This time we're going to find all large campuses that have someone applying to that campus in Computer_Science So this time we're going to join the college table and the apply table And again we need to be careful to make_sure we only join tuples that are talking_about the same college So we have college cname equals apply cname We have an enrollment that's greater_than and a major that equals Computer_Science Let's run this query Oops we got an_error Well actually I knew that was coming but I wanted to show you what_happens here So the error is that we have an ambiguous column name and that's the one right here the C name So I haven't pointed it out explicitly but whenever I've referred to attributes where there's an attribute from both of the relations we're querying I prefaced it with the name of the relation that we cared about the college here in the apply So the attribute name here in the select_clause is actually ambiguous because there's a C name attribute in college and there's one there in apply Now we happen to set those equal but in order for the query to actually run we have to choose So let's just say we're going to take that C name from college Now everything should be fine and here we go So those are the colleges where we have at_least one Computer_Science major and their enrollment is greater_than Again we see duplicates so if we don't like the two_copies of Berkeley we simply add distinct and we run the query again And now we have Berkeley and Cornell Now let's do a query with a bigger result This time we're finally going to join all three of our relations Student college and apply And we're going to apply the joint conditions that ensure that we're talking_about the same student and the same college And then from the result of that big cross_product that big join we're going to get the student_ID their name their GPA the college that they're_applying to and the enrollment of that college So just a whole_bunch of information associated_with this students' applications And we execute this and here we get the result with all the attributes that we asked for Now one thing I haven't mentioned yet is the order of the results that we get when we run Structured_Query_Language queries SO Structured_Query_Language is at its heart an unordered model That_means that we can get the results of our queries in any order and in fact we could run a query today and get our results in a particular order And then run the query tomorrow and get a different order And that's permitted with the specification of Structured_Query_Language on relational_databases If we care about the order of our result Structured_Query_Language provides a clause that we can ask for a result to be sorted by a particular attribute or set of attributes So let's say we want our application information here sorted by descending GPA Then we add another clause called the order by clause We tell the attribute we'd_like to be ordering by and then if we want it to be descending we write DESC The default behavior is actually ascending So if we run this query now we get our results by descending the GPA we see all the 's and so_forth Now we might still want to further sort within all the s if we want to do that we can specify another attribute to sort each group_by So for example if we decide from that we want to sort by enrollment and ascending we won't put anything because ascending is the default And we execute Now we still have GPA as descending as our primary sort order and then within each of those will be sorting by ascending enrollment This query introduces the like predicate Like is a built in operator in Structured_Query_Language that allows_us to do simple string matching on attribute values Let's_suppose for example that we wanted to find all students_who were applying for a major that had to do with bio Instead of listing all the biology majors we can simply pattern match the major against the special string here which says match any major where there's some set of characters followed_by bio followed_by some set of characters we execute the query and we'll find the students_who have applied for various bio type majors Now I want to introduce another construct I'm going to use the same query to do it which is the construct select star So far we've always listed explicitly the attributes that we want to get in the result of a query But if we simply want to get all attributes then we can just write select star And when we do that we don't project away any attributes but we get all the attributes in the result of the from and where expression While we're at it let's do a gigantic query We'll just do the cross_product and student college without any combination and we'll do select star to get all the attributes out So here goes and you can see we get all the attributes and we get a whole lot of tuples as_well Our last query is going to demonstrate the ability to use arithmetic within Structured_Query_Language clauses So we see here a query that selects all the information from the student relation but adds to it a scaled GPA where we're going to boost the student's GPA if they're from a big high_school and reduce it if they're from a small one Specifically we'll take their GPA multiply it by the size_high_school divided by a thousand So let's run this query and you can see that we have the whole student table here with an additional column that has scaled their GPA based_on the size of their high_school Now if we don't like the label on this column we could change it and so I'll use this query as an example to demonstrate the 'as' feature which allows_us to change the labeling of the schema in a query result Let's say as scaled GPA and we should get the same result with a more nicely labeled attribute That_concludes our video introducing the basic select statement We'll see many other features in the upcoming six videos on Structured_Query_Language In this demo we'll be learning some more features of the Structured_Query_Language language Specifically we'll be learning_about table variables and about set operators We already learned the basic select statement which can be quite_powerful for writing queries but we'll learn some constructs in these demos that will give_us even more expressive_power The first construct is table variables Table variables are in the FROM clause and they actually serve two uses One is simply to make queries more readable as we'll see But a second purpose is to rename relations that are used in the FROM clause particularly when we have two_instances of the same relation This is exactly what we needed in the relational_algebra when we wrote joins that included two_instances of the same relation The second construct we'll be learning actually a set of constructs in this video are the set operators And we'll be learning the same three set operators we had in relational_algebra the union_operator the intersect_operator and the except operator which is the minus operator We'll be doing a demo and the demo will use the same college_admissions_database that we've been using in previous demos where we have tables about college information student information and students_applying to colleges Let's move to the demo Let's start with a big join query that we'll use to introduce table variables This query involves all three relations It joins the three relations on their shared attributes and then it selects a bunch of information So here we see the result of that query So the main point of this query is not the result but just to show you how table variables are used in the FROM clause We can add two each of our relation names a variable We'll use S for student and C for college and A for apply And then everywhere else in the query instead of writing the full relation name we can just use the variable In this case we're not changing the expressiveness we're not changing the outcome of the query we're really just making it a binary_digit more readable and we can do the same thing here in this left clause We'll take S and A and so on Then we'll run the query and we'll get exactly the same result no change Now let's look_at where table variables are actually useful What we want to get in this query is all pairs of students_who have the same GPA This is kind of similar to the relational_algebra query we did where we found all pairs of colleges that are in the same state In order to do that we need to have two_instances of the student relation So we'll call one instance S and we'll call the other instance S And the FROM will do the cross_product of those two so it will consider every every possible pair of students from the student relation From all those pairs we'll take the pairs where the student had the same GPA and will return the ID name and GPA for each of the two students So let's go_ahead and execute the query and here we can see the result Now this result is exactly what we wrote It literally is every pair of students that have the same GPA but it might not be what we intended Amy and Amy the same student Well Amy has the same GPA as herself but more likely we just wanted different students_who had the same GPA So to do that we'll add an and that says these are two different students The SIDs of the students are different Now let's run the query and see what_happens Now we see that we no_longer have Amy and Amy and every student is paired with a different student We do have two Amy's here but don't be alarmed this Amy is and this Amy is So things are looking quite a binary_digit better but there's still one thing that we might not want in the result of the query which is that we have Amy paired with Doris and then we have Doris paired with Amy So we're actually getting every pair of students twice in the two different orders As it turns_out that's very easy to fix We only need to erase one character to make that work Maybe you can think_about what that character is Here it is Instead of looking_at not equals we'll just make it less_than And then we'll get every pair of students only once because we'll always be listing the one with the smaller SID first and finally we get the answer that we probably intended in the first place Now let's take a look_at the set operators and we'll start with union Just like in our relational_algebra video let's use the union_operator to generate a list that includes names of colleges together with names of students So here's the query that will do it for us and we go_ahead and execute the query and we see our result Now I left the schema as having the C name in the first part of the union and the S name in the second Structured_Query_Language allowed me to do that and it chose to use the C name to label the result If I want to unify the schemas of the two sides of the union and give a new label for the result I use the as as we saw earlier for re naming attributes in the result of queries So I'll add as name to both_sides of the union run the query and now I see name in the result Now one thing you might have noticed is that this result is actually sorted We didn't ask for it to be sorted but for some reason the system sorted it for us And I can actually explain why that happened I'll also mention that if I ran this same query on another system it might not come out sorted In fact it will not come out sorted because I tried it Here's the deal The union_operator in Structured_Query_Language by default eliminates duplicates in its results So if we have two Amy's which in fact we do we only get one Amy in our result And similarly for Craig we have two of those as_well So that's the default and it so happens the system I'm using today which is called SQLite eliminates duplicates gets by sorting the result So it sorts the result looks for adjacent pairs that are the same and eliminates all but one of those and then it gives_us the answer But again I want to emphasize that's not something one can count on when one runs the same query on a different system or even on the same system on a different day Now if we want to have the duplicates in our result that's something we can do quite easily We add to union the word all that will turn the set operator into what's technically a multi set operator that retains duplicates We run the query Well the first thing we notice is it's not sorted anymore That's because it didn't need to eliminate the duplicates But if we look closely we'll also see that the duplicates are now there We have two Amys for example and we have two Craigs as_well If we want this result to be sorted and to guarantee that the other one's sorted we would add an order by clause So we can just say order by name We run the query and now we have the result in sorted order Our_next query demonstrates the intersect_operator This query is going to get the IDs of all students_who have applied to both Computer_Science for a major and EE for a major So very_simple query We get the IDs of students_who applied to Computer_Science the IDs of students_who applied to EE and then we perform the intersect_operator on the result of those two queries We execute it and we find that there are indeed two students_who applied to Computer_Science and EE Some database_systems don't support the intersect_operator They don't lose any expressive_power We just have to write our queries in different_ways So this next query is computing exactly the same thing The sIDs of students_who have applied to both Computer_Science and EE but this time we're doing it by doing two_instances of the apply relation One of these self joins so we have to use table variables again so we take every pair of apply tuples we look_at cases where it's the same student and in one case they're_applying for Computer_Science in the other case they're_applying for EE and we'll return the sID of those students So we run the query and we get sort of the same answer but not exactly because we have a whole_bunch of duplicates now that we didn't get when we did it with an intersect_operator Now where did those duplicates come from Let's take a look_at the apply relation itself Here we see that student applied to that there are indeed two students_who applied to Computer_Science and EE Some database_systems don't support the intersect_operator They don't lose any expressive_power We just have to write our queries in different_ways So this next query is computing exactly the same thing the SIDs of students_who have applied to both Computer_Science and EE but this time we're doing it by doing two_instances of the apply relation one of these self joins so we have to use table variables again So we take every pair of apply tuples we look_at cases where it's the same student and in one case they're_applying for Computer_Science In the other case they're_applying for EE and we'll return the SID of those students So we run the query and we get sort of the same answer but not exactly because we have a whole_bunch of duplicates now that we didn't get when we did it with an intersect_operator Now where did those duplicates come from Let's take a look_at the apply relation itself Here we see that student applied to Computer_Science and to EE and to Computer_Science again and to EE again And we're gonna get all pairs of tuples where one pair pair of the tuples is Computer_Science and the other is EE So we'll get Computer_Science with EE Computer_Science with EE and so on Going back to our query result Here it is We can see that we got the four 's when we ran the query Well that's easy to get_rid of We just write select distinct and that will get_rid of duplicates and now we're back to our original query result Now instead of finding students_who applied to both Computer_Science and EE let's find students_who applied to Computer_Science but did not apply to EE For that we need the difference_operator It's called difference in relational Algebra Sometimes it's called minus The word that's used in the Structured_Query_Language standard is the word except So here's our query We find the student IDs who applied to Computer_Science and then we take away from those the IDs of students_who applied to EE We run the query and we find that there are three students_who applied to Computer_Science and not to EE Some database_systems don't support the except operator either and here things get a little tricky So let's try to rewrite that query without using the except operator So as a reminder we want to find students_who applied to Computer_Science but did not apply to EE So here's my attempt at writing that query I again do a self join of apply with apply and I find all pairs where it's the same student we're talking_about and the major in one of the tuples of Computer_Science and the major in the other one is not EE Well it looks pretty good Let's see what_happens Whoa we got a lot of results Okay well that's probably just that problem with duplicates again so let's just add distinct and go for it It still seems like a lot of results Let's go_back to our previous query that uses except and then we found that there were three students in the result where here we're still getting five in the result Well if we think_about exactly what we wrote what we wrote is finding all pairs of apply records where it's the same student and they applied to Computer_Science in one of the pairs and they didn't apply to EE in the other So it could be for example biology or geology But the problem is that when we consider these pairs that doesn't mean there's not another pair with the same student where they applied to Computer_Science and EE All this is actually finding is students_who applied to Computer_Science and also applied to another major that's not EE So that's quite different from the query we're shooting for And actually the interesting thing is that with the constructs we've_seen so far in Structured_Query_Language it's not possible to write the query we had earlier without using the except operator But in later_videos we will see additional constructs in Structured_Query_Language that do allow_us to write that query This Structured_Query_Language video which as usual will consist mostly of running live queries introduces sub queries in the where clause As_usual we start with our basic select from where expression and we're going to add to it now the ability in the condition part of the select from where expression to include sub queries Sub queries are nested select statements within the condition and we'll see they're actually quite_powerful As_usual we'll be using a sample demonstration database that consists of colleges students and students_applying to colleges As always we'll have our four colleges a bunch of students and a bunch of records that show students_applying to colleges So let's launch right into our first query that shows a sub query in the where clause What this query finds is the ID's and names of all students_who have applied to major in Computer_Science to some college So here's the sub query in the where clause This expression right here in the where clause finds the ID's of all students_who have applied to major in Computer_Science So now we have that set of ID's our outer query says let's take from the students those students_whose ID is in that set and let's select their ID and their name So we go_ahead and execute the query and we find out that five students have applied to major in Computer_Science Now we actually can do this query without a sub query in the where clause So let's take a look We can do it instead by joining the student relation with the apply relation So here we do what we learned in previous_videos We take student and apply we write the joined condition to make_sure we're talking_about the same student we make_sure they're majoring in Computer_Science and we get their ID and their name Let's run the query Whoops an_error I knew that was coming This is just to remind_you about disambiguating attributes The ID here in the select_clause could have come from student or apply and even_though the value is equal we do have to disambiguate by putting one of those So let's put student dot SID and let's run the query Okay Now we see we got more students back than we got back when we ran the query using the sub query Let's go_back and look We got five results here and we got several more here but the additional results are actually duplicate values So we have two_copies for example of Amy The reason for that is that Amy actually applied to major in Computer_Science at multiple colleges So if we go_back and we look_at the apply data we'll see that Amy who is applied to major in Computer_Science at Stanford as_well as Berkeley Let's hope she selects Stanford In any case that's why we got Amy twice in the join because she applied twice Back here where we used the sub query we were just looking_at students and whether their ID was in the set Okay so when we do the join we get basically the same answer but we have some duplicate values Of_course we can fix that by adding distinct We run the query and now we have the same result that we got when we used the sub query Let's look_at some other similar queries and let's focus again on the duplicates issue because it gets a little tricky as we'll see This query is very similar to the previous one finding students_who are applying to major in Computer_Science but this time we're only getting the names of the students and not their ID's So we run the query and we find our same five students Just want to mention that these two Craigs are two different Craigs If we go_back to our original result there's three four five Craig and five four three Craig So coming back here we find the names of the students_who majored in Computer_Science Now similarly to what we did previously let's write this query using a join instead of using the sub query in the where clause So here's the join We're joining student and apply on the student's ID majoring in Computer_Science as always The only difference is that we're just selecting the name We run the query and we get again many more results than we got previously because we get two_copies when a student has applied to to major in Computer_Science at two different places And just as we did before we can add distinct to get_rid of the duplicates Well something different happened this time This time when we get_rid of the duplicates we only have four results where previously we had five And the reason is that previously we included the student_ID in the result and so the two_instances of Craig were two different Craigs and didn't cause duplicates to be eliminated We can see that back here Craig and Craig But in this result because we only kept the names the two_copies of Craig turned into one result Now we might_wonder_why do we care about duplicates so much Let's see an example where duplicates really do matter quite a binary_digit We're going to do exactly the same query again finding students_who have applied to major in Computer_Science but now we're not retrieving the IDs or names we're retrieving the GPAs of those students Presumably what we're interested in doing is some analysis of the GPA's of students_who choose to apply for Computer_Science So let's run the query As always we get our five results And here's the GPA's of the five students_who have applied to major in Computer_Science Once again this will be the last time I promise we'll do this query using a join instead of the sub query So here we go We've got student and apply join on SID majoring in Computer_Science and returning the GPA Once again because we have students_who applied multiple_times for Computer_Science we're getting more_than our five results So we get a rather large number of results here and again we have duplicates So here's where the problem comes in If we use this result to compute the average_GPA we'll be counting some students multiple_times And presumably that's not what we want to do Presumably we want to count each student who's applied to Computer_Science once in the computation of say the average_GPA That worked in the previous query when we got the five results for the five students_who applied to Computer_Science When we do the join we get too many results But this time again we're going to have a problem when we do select distinct because some of these students have the same GPA And now we only have four GPAs instead of the five that we should have And if we compute the average_GPA now then we're not factoring in one of the student's GPAs So in this case neither the version with distinct nor the version without distinct gives_us the right number of GPAs Neither of those will give_us the correct average The only way to get the correct number of duplicates is to use the version of the query that has the sub query in the where clause Now let's move to some different examples that also use subqueries in the where clause You_might remember from the previous_video when we were learning the difference_operator that we had a query that we could write using the difference_operator which in Structured_Query_Language is called accept but we were unable to write that query without the difference_operator And the query we were trying to write is to find students_who have applied to major in Computer_Science but have not applied to major in EE Now that we have sub queries in the where clause we can write that query without using the except operator And here it is The query looks for students where their ID is among the set of ID's of students_who have applied to Computer_Science but their ID is not among the set of ID's of students_who are applying to major in EE So let's run the query And we discover that there are three students_who have applied to major in Computer_Science but not applied_anywhere in EE By the way let_me just show you a slightly different way to write exactly the same query You can see that we use this key word combination not in to specify that the idea is not in this set We can actually write it by writing the SID is in the set and then applying a not to that result We'll execute and we'll get exactly the same result It's fairly common in Structured_Query_Language for there to be multiple ways to write the same query and we'll see more examples of that later in this video So far our examples of sub queries have used in and not in for testing membership in the sets that are produced by sub queries Our_next examples are going to show that we can apply the exists operator to sub queries just to test whether they're empty or not empty Here's the query This query uses exists to check_whether a subquery is empty or not empty rather_than checking whether values are in the subquery The other new construct that's begin to introduce here's what's_known_as a correlated reference But inside the subquery we're going to refer to a value C that comes from outside the subquery So let's talk_about exactly how this query works First let_me tell you what the query is trying to return The query is going to find all colleges such that there's some other college that is in the same state Okay So in our example just a reminder we have Stanford Berkeley Massachusetts_Institute of Technology and Cornell So the two Colleges we should get back are Stanford and Berkeley because in each case there's another college in the same state So how does this query work It says we're gonna to take the colleges and for each college we're going to check_whether their exists another college and we're going to call that one C where the state of C is the same as the state of C This is sort of similar to what we saw when were doing self joins with table variables but now the variables are appearing in the outer query and the inner query Okay So let's run the query and we see that we get the wrong answer That was again intentional Well here's the problem When we were in this query C and C could be bound to the same college So every college is in the same state as another college of the other college could be the same college What we need to do is add inside the sub query a further condition that says that C and C are different colleges Specifically C Cname is not equal to C Cname Let's run the query and now we get the correct answer Now let's look_at some other uses of the exists construct and sub query If you happen to already know Structured_Query_Language a little_binary_digit and someone asks you to write a query where you were going going to get a largest value of some type the first thing you would think of is probably using the max operator but we don't know max yet we'll be learning that later_videos And as it happens a number of queries that are effectively computing a max can be written using sub queries And here's our first example This example is finding the college that has the largest enrollment And we're going to do that with the sub query and with the not exists operator Specifically we're going to find all colleges where there does not exist another college whose enrollment is higher than the first college So let's go_ahead and run the query And not surprisingly we get Berkeley as a result So this is a form of query that we can write any time we're looking for something that's the largest or the smallest Let's for example look for the student with the highest_GPA So we'll change it to the student name and we'll be looking instead of colleges at students Otherwise this form of this query is going to stay very similar We're going to look for students here as_well And finally we're going to have GPA instead of enrollment So the way this query works is it says I want to find all students such that there does not exist another student who's GPA is higher We run the query and we get four results Seems odd Actually it's that odd Let's just add the GPA to our query and we'll see exactly what_happened We can see that these four students all have a GPA of so they're all tied for having the highest_GPA and very specifically it faithfully runs the query that there does not exist another student whose_GPA is higher than these students Now let's see if we can write this same query without using a sub query So as usual if we want to write a query without a sub query we'll need to do some type of joint So we're looking for students with the highest GPAs So we need to join two_instances of the student relation as we've done here and then we'll apply a condition that the GPA of the first one is greater_than the GPA of the second one We run the query Wow we got a lot of answers there Maybe our problem is just one of duplicates So first thing we do when it looks too big we add select distinct Nope that doesn't solve the problem either Actually this query is fundamentally wrong and we cannot write the query that finds the student with the highest_GPA just by using joins What this query actually does is it finds all students such that there is some other student whose_GPA is lower In_other_words it's finding all students except those who have the lowest GPA Now let's see another new construct we can use with sub queries in the where clause and we'll continue with the same query of finding the student with the highest_GPA This query uses the all keyword What all tells_us is that instead of checking whether a value is either in or not in the result of a sub query we're going to check_whether the value has a certain relationship with all the results of the sub query And here in this case we're checking to see if the GPA is greater_than or equal to all elements of the sub query that returns the GPA's of all students If the student's GPA is indeed greater_than or equal to all GPA's then the student has the highest_GPA in the database We'll run the query and we'll get the same four students with the GPA Now let's try writing this in a slightly different fashion just for fun to illustrate some concepts Once again we're going to try to find the students_whose GPA is highest in the database Here's how we're going to do it this time Instead of using greater_than or equal to all we're going to use greater_than all So we're going to find all students where their GPA is higher than every other student by saying GPA is greater_than all GPA's of students in the database who are not the student we're looking_at By saying that the ID's are not equal Let's run the query Well we got an empty result Let's think_about it for a second That is actually the correct result to the query The query itself was incorrect The query is looking for all students where nobody else has the same GPA as that student Everybody else's GPA is lower Well remember we had four students with a GPA so none of those students are going to satisfy the query and nor will any other student So this is an incorrect formulation of the query that we wanted Now this query would be correct if we knew that every student had the same GP I'm_sorry This query would be correct if we knew that every student's GPA was unique because there would be then one student who had the highest_GPA So let's modify the query to instead look for the colleges that have the highest enrollment Because it turns_out in our database every college has a unique enrollment So we'll change it to getting the C name of college instead of student And we'll want the enrollment to be greater_than all other enrollments So we'll enrollment college almost done here Grab a cup of coffee if you want I just have to make this be C name and this one be C name and we're all set So what are we asking for here We're asking for all colleges whose enrollment is greater_than all of the enrollments of colleges that are different than the one we're looking_at We run the query and we get Berkeley as a result which is exactly what we'd expect So far we've_seen the keyword all for checking whether a value has relationship with all of the results of a sub query There's also a key word any that is a companion but instead of having to satisfy a condition with all of the elements of a set any says you must satisfy the condition with at_least one element of the set So what we're going to do now is going to do the same query but we're gonna write it a little differently Let_me just type it in and then explain Here what we're going to say is get me all colleges where it's not the case that the enrollment is less_than or equal to any other college Okay So in other_words there's_no other colleges have bigger enrollment Just think_about it for a second this is if you happen to know predicate logic This is an equivalence where when you say for all it's equivalent to not exists not In any case if you didn't follow that don't let that bother you Let's run the query and we see we again get the result Berkeley so again all tests a condition against every element in the result of a sub query and the condition is true if it's satisfied with every element whereas any is true if the condition is satisfied with one or more elements of the sub query Let's use any for another query This query finds all students_who are not from the smallest high_school in the database So what we are looking for here is all students where the size of their high_school is greater_than any high_school_size In_other_words a student is going to be returned if there's some other student whose size_high_school is smaller than this student We run the query and there's the result And you can verify if you look back at the data but there is in fact a few there are students_who are from high_schools that have students so here we have in our result every student that comes from a high_school bigger than that Some systems notably at the current time SQLite do not support the any and the all operators We do not lose expressive_power we just have to write those queries a little differently typically using exist or not exists so let's look_at the same query written without using any Here's what we do We look for students where there_exists some other student okay whose high_school is smaller than the student we returned So this is going to give_us exactly the same result and we will see we will get the same set of students No students from a high_school with two hundred but we get all the rest of the students without using any or all And just to emphasize any or all are very convenient for writing queries but they aren't necessary It turns_out we can always write a query that would use any or all by using the exists operator or not exists instead As a grand finale query let's revisit the query that finds students_who have applied to major Computer_Science and have not applied to major in EE And now we're going to write that query using the any operator two_instances of it Now let's go_back and look when we wrote the query using in and not in and we see that there are three results Helen Irene and Craig And now let's look_at our query using any and not equal to any So we find students where their SID is in the set of SID's of students_who have applied to major in Computer_Science and their SID is not equal to any of the students_who have applied to major in EE and let's run the query Well we got the wrong answer and that was once again intentional Let's look very closely at what this query asks for This query asks for students students where their ID is in the set of ID's that applied to Computer_Science that's all fine and good but this is where things got tricky What we're saying here is that we want the condition to check_whether there's any element in the set of EE's that are not equal to this SID So in fact this second condition is satisfied as long as there's anybody who applied to EE that's not equal to the student we're looking_at and that of course is commonly the case so we made a mistake here and it's actually quite tricky to use the any and all operators and always get the right answer It's especially tricky when you can't just eyeball the answer and check but the correct formulation that we want here is that it's not the case that the SID is equal to any member of the set In_other_words for each member of the set of the EE application that value is not equal to the SID of the student we're going to retrieve And now we run the query and we get the right answer The next set of queries will show introduced the concept of the aggregation in Structured_Query_Language Once again we start with the basic select from where construct and this time we're going to introduce what are known_as aggregate or aggregation functions These are function that will appear in the select_clause initially and what they do is they perform computations over sets of values in multiple rows of our relations and the basic aggregation functions supported by every Structured_Query_Language system are minimum maximum some average and count Now once we've introduced the aggregation functions we can also add two new clasues to the Structured_Query_Language select from where statement the group_by and having clause The group_by allows_us to partition our relations into groups and then will compute aggregated aggregate functions over each group independently The having condition allows_us to test filters on the results of aggregate values The where condition applies to single rows at a time The having condition will apply to the groups that we generate from the group_by clause All of these constructs will become very clear when we show some examples As_usual our examples we use are simple_college_admissions database that has a table of colleges a table of students and information_about students_applying to colleges As_usual we have four colleges a bunch of students and applications of students to colleges for specific majors Our first aggregation query is a very_simple one computes the average_GPA of the students in the database Here is the query but just to explain the query let_me first replace the aggregation with star When we run select star from students we get our result here Specifically when we look_at the GPA column what the aggregation is going to do is perform a computation over all values in the column and then produce just one tuple in the results that has that value So we'll write average_GPA We'll run the query and we see now the average_GPA in the database Our second querie is a binary_digit more_complicated It involves a join What it finds is the minimum GPA of students_who have applied for a Computer_Science major So we do the join of student and apply on student_ID Filter for major equal Computer_Science Once again let_me just remove the aggregation first so we can see the result without aggregation Here we see all of the information_about students who've applied to a Computer_Science major The aggregation is going to look_at the GPA column and it's going to take the lowest value in that column So we write min GPA Run the query and we discover that the lowest GPA is three point four Now let's go_back to the average aggregate function again So let's compute the average_GPA of the students_who have applied to computer_science run the query and we see the result is about three point seven In fact this result is probably not precisely what we were looking for In a previous_video we talked in some detail about this particular form of query versus using a sub query to find the GPA of students_who have applied to Computer_Science The issue with this particular query and let's go_back to select star version is that if a student applied to Computer_Science multiple_times for example student Amy applied to both Stanford and Berkeley Then When we compute their average_GPA we're going to be counting their GPA twice Presumaly what we actually want is to count the GPA once for each student who applied to Computer_Science no matter how_many times they applied so in order to do that we use the sub query form where we select from student and then we just check for each student whether their ID is among those who apply to Computer_Science So let's just do a binary_digit of editing here from apply where major equals Computer_Science I'm not a very good typist Let's just first take the sub query form and we see that we made a mistake here and it says that we forgot a word here which is in Apologize for that Okay now we have the students_who applied to Computer_Science and in this case we only have one instance of each student So now if we run the aggregation when we compute the average_GPA we'll correctly count students GPA only one time So remember with three point seven approximately before now we run the query and we see that the correct result is three point six eight not that different but this one is the numerically correct result So far we've_seen the average and min aggregate functions This query shows the count function So not surprisingly the count function just returns the number of tuples in the result So this particular query finds the number of colleges in our database whose enrollment is greater_than fifteen thousand just for it to be thorough let's run the query without the aggregate function and we see that there are two of them and the aggregate function simply counts those number of tuples We run the query and the result is two as expected Let's do another count query that looks very similar In this case we are counting the number of students_who have applied to Cornell So the query looks the same will run the query and we discover that six students have applied to Cornell Well in reality what we've actually counted in the number of applications to Cornell not the number of students_who have applied to Cornell If a student applied three times then we're counting them three times in this result So in some sense this is similar the previous one we saw we were over counting and we could try to fix it with the sub query and the in and so_forth but actually Structured_Query_Language provides a very nice way for us to perform the query that we want In the count function we can put a special keyword distinct and then the name of one or more attributes and in this case what the count will do is will look_at the result and then it will count the distinct values for the particular attribute so we run the query and we are find that there are three distinct student IDs If we ran it without just like this When we see we would look and again we would find that there are three distinct student IDs So count distinct actually turnes out to be a very useful feature in Structured_Query_Language Here's a rather complicated looking query and I have to admit it computes something fairly obscure but it does demonstrate some features What this query computes is it gives_us back all students where the number of other students_who have the same GPA as the student is equal to the number of other student that have the same high_school_size at the student Again I admit it's a little_binary_digit of obscure but let's take a look_at how it works So it looks at the student relation and for each student it counts the number of other students that's by testing that the ID is different that have the same GPA It also counts the number of other students that have the same size_high_school and if those two values and the sub queries both produce a single value which is why would contest equality if those two values are the same then the student comes out in the result Let's run the query We get our answer I'll leave it to you to verify by looking_at the data that this is indeed the correct result Here's another complicated looking query although this one computes something quite a binary_digit more intuitive What this query computes is the amount by which the average_GPA of students_who apply to computer_science exceeds the average_GPA of students_who don't apply to computer_science and we are going to assume it does in fact exceed So we are using in this example sub queries in the from clause which I_hope you remember from the previous_video A sub query in the from clause allows you to write a select from where expression and then use the result of that expression as if it were an actual table in the database So we are going to compute two sub queries in the from clause one of them which is the average_GPA of Computer_Science applicants and one the average_GPA of non Computer_Science applicants Let's look a little closer here so this query here says let's find the students_who have applied to major in Computer_Science let's compute their average_GPA and we'll call it average_GPA We'll take the whole result and we'll name it Computer_Science Similarly the second new relation that were computing in the from clause computes the average_GPA of students_who did not apply to Computer_Science so their student_ID is not in the set of students_who applied to Computer_Science We'll call that one non Computer_Science So now with this from clause we have a relation called Computer_Science with an attribute called average_GPA and a relation called non Computer_Science with an attribute called average_GPA and in the select_clause we'll simply do the subtraction of the non Computer_Science GPA from the Computer_Science GPA We run the query and we discover that the Computer_Science applicants exceed on average the non Computer_Science by point one nine Now just for demonstration purposes let_me show that we can write the same query but using sub queries and the select_clause You_might remember from the previous_video that in a select_clause we can write a sub query as long as it returns a single value We're going to go even further we're going to write two sub queries that return single values and subtract them So I'm going to replace this from with select I'm going to take up this whole first line here And then I'm going to Let's see What else do I need to do I'm going to take out the as here and I'm going to replace it with a minus I'm going to take out the as here So now what we've_got is we've_got the average_GPA of Computer_Science student in the select_clause producing a value The average_GPA of non Computer_Science students We perform the subtraction as part of the select_clause The last thing we need is something in the from clause so we'll say from student will call the result of this subtraction d as in for difference we run the query and we get almost the same result except for a bunch of duplicates The reason for the duplicates is that we compute this result once for each tuple in student We can add distinct and now we get the single result Now let's learn_about the group_by clause which is only used in conjunction with aggregation Our first query is going to find the number of applicants to each college and it's going to do so by using grouping Effectively what grouping does is it takes a relation and it partitions it by values of a given attribute or set of attributes Specifically in this query we're taking the apply relation and we're breaking into groups where each group has one of the college names so it'll be the Stanford group the Berkeley group and so_forth and then for each group we will return one tuple in the result containing the college name for that group and the number of tuples in the group So to illustrate what's happening with grouping very clearly let_me start_by replacing the select_clause with uh select star removing the aggregation and doing an order by on the college name So that illustrates the groups that we're going to be using in in the actual query so we see there are three tuples for Berkeley there six tuples for Cornell and so_forth Within each group then for the particular query we're doing we're simply going to count the number of tuples So going back to the group_by form we return the college name and we return the count of a number of tuples Now before I actually run the query one thing to notice is what it makes_sense to put in that select_clause here We're grouping by the college name so the college name is going to be the same for all of the tuples of the group so it's sensible to include that but it wouldn't really be sensible to include when the other attributes that differs although we'll see an example later where we include that and interesting things happen For now we'll just put the name of the grouping attribute and then we can perform aggregation over that In this case count We run the query and we see that there are three applicants to Berkeley six to Cornell and so_forth Here's a very similar query just as a second example and this example we're finding the total enrollment of college students for each state Now we only happen to have three states and four colleges in our database but what this query does is it takes the college relation it breaks it up into partitions by state and then within each partition or group it will return the state for that partition and the sum of the enrollment We run the query and we see the expected result Here is a more_complicated group_by query In this case we're grouping by two attributes We also have a join involved and we're going to compute two aggregate functions in our result What this query computes is for each college and major combination the minimum and maximum GPAs for the students who've applied to that college As_usual before we do the aggregation let's replace the query with one where we can see the groups explicitly So I'll put GPA here and instead of grouping I'll order by and we run the query and we see for Berkeley biology we actually only have one applicant for Berkeley Computer_Science we have two I think Stanford Computer_Science has the largest number of applicants at three So once we put the aggregation back each of these combinations of college and major is going to be considered individually and for each one will compete the minimum and maximum GPA So let's go_ahead and do that All those parentheses Ok change this to group_by Run the query and here we see for Berkeley biology the minimum and maximum are the same because we saw that there was only one uh student uh who applied to Berkeley biology Stanford Computer_Science we had three we can see the spread of their GPA's and so_forth for each college major combination Now what if we were interested in finding information_about the spread of GPAs for each college and major uh what the differences between the minimum and the maximum So let's_suppose in fact what we wanted to find was largest spread but let's take that in steps I'm going to take this current query here and I'm just gonna slap it into the from clause So now in the from clause I'm going to have a relation we'll call it M that's going to have the college major combinations and the minimum and maximum GPA This M is going to now have the result that we see below Now once we have that result to work with in the select_clause we can access the attributes here Let_me call this one mn and this one mx and if we write just mx minus mn here what we're going to get is a list of the spreads of the min and max GPA for each collage and major So we see many of them are zero Those are all of the combination that had just one student but in some cases we actually have a spread but what we want to find it is the largest spread watch this it's very cute we just simply put a max function right here Run the query and we find that the largest spread between min and max is approximately one Our_next sequence of queries introduces some subtleties of the group_by clause What our query finds is the number of colleges that have been applied for the number of colleges that have been applied to by each student So the query join student and apply It groups the result for the student in the student_ID and then for each student_ID finds the number of distinct colleges in that group So once again just to illustrate let's take away the aggregation let's replace the group_by by an order by just so we can see the data that we're working on and run the query and we see that student one two three has four applications to Stanford Berkeley and Cornell Three distinct colleges Two three four has one and so_forth So now let's return to our original query group_by student we'll count the distinct college names Run the query and we get the expected answer student one two three applied to three distinct colleges two three four to one and so_forth Maybe we'd_like to have the student's name in the result as_well That's easy to do We just add the student name to the select_clause and we run query and now we've added Amy is one two three Bob is two three four and so_forth That worked out fine in this case but we're gonna see in a moment where simply adding an attributes to the select_clause in the case of a group_by does something a little strange In fact the only reason that it worked nicely in this case is that when we group buy the student_ID within each group we have the same student name Let's see that Let's just take a look_at that Again we'll replace the group_by with an order by We're sorting by student_ID and we see for the first group which is one two three we have all Amys for the three four five group we have all Craigs and so on So when we put student name in the select_clause for our group_by query it was ok because for each group we had the same name of the student When that's not the case we should presumably get an_error but let's see what_happens So let's say that we're interested now not only in finding the uh student name in our query but we're further gonna add a college name So let's go_back to the original query Distinct college name Let's add college name in our result and let's go_back to grouping by student So now for each student we're gonna return the I for each group which is identified by the student_ID we're gonna return the ID the name the number of distinct colleges and then a college Let's run the query and we do get a result we don't get an_error So we've_got Amy one two three She applied to three distinct colleges but for whatever_reason the system decided to tell_us about Stanford Craig applied to two colleges and the system tells_us about Massachusetts_Institute of Technology Let's go_back again revert now to our order by form of this query and take away the aggregation and when we run the query we see there's Amy and she did apply to four different colleges but when we put the college name in the grouping query it chose to tell_us about Stanford What this system is actually doing is when we include in the select_clause of a grouping query again if we go_back to group_by and we put in the select_clause an attribute that's not one of the grouping attributes It actually chooses a random value from the group to include in the case of the student name the random value was always the same because the student name is always the same for the same student_ID In the case of the college it shows a random value among the colleges I'm personally not crazy about this behavior Among three open_source database_systems that we've been using for our demos two of them allow this behavior SQLite and MySQL I happen to be using MySQL today If we were using Postgre it would actually throw an_error in this case Let's go_back to the original query that finds a number of colleges applied to by each student Here it is as we originally wrote it and here is the result Now there are may some students in the database who_haven't applied to any college at all and they're not going to be represented in our result because they're not going to be in the results of the join and we don't see any zero counts here What if we wanted to actually lift the student IDs of students_who_haven't applied_anywhere and include a zero in the result This is actually one of my favorite queries to give as an exercise or an exam question in my class because it takes a little innovation to figure_out how to write it The way we write it actually is by using the union_operator which I_hope you remember from a previous_video We're going to take the union of the students_who have applied somewhere and the number of places they've_applied together with the students_who_haven't applied_anywhere and for those students we're going to put a cero in the result directly So let_me just write the beginnings of this so we're going to find those students_who_haven't applied_anywhere at all so those are the students_whose sID is not in at all the set of sIDs in the apply relation So that finds the students_who didn't apply anywhere for each student will return their student_ID and then we want to have a zero in the result Very simple to do we simply write zero We run the query and now we scroll down we'll see that in addition to all the students_who have applied somewhere we get zeros in the result for the students_who_haven't applied_anywhere The last clause that we're going to introduce in this video is the having clause and the having clause is also only used in conjunction with aggregation What the having clause allows_us to do is apply conditions to the results of aggregate functions So the having clause is applied after the group_by clause and it allows_us to check conditions that involve the entire group In contrast the where clause applies to one tuple at a time So this particular query is finding colleges that have fewer_than five applicant So we look_at the apply relation we group it by college name so we're going to have one group for each college Then we're only going to keep those groups were the number of tuples in the group is less_than five indicating that we have fewer_than five applicants to the college We'll run the query and we'll see that Berkeley and Massachusetts_Institute of Technology each have fewer_than five applicants So presumably Cornell and Stanford have greater_than or equal to five applicants and you can check that by examining the data Now it is possible to write the same query without that having clause and in fact without the grouping clause and let_me show you how that's done We're going to select again the cName from apply and here's what we're going to do We're going to add an attribute there sorry a table variable We're gonna erase this whole business and we're going to find all applicants so all applications where let_me just type this out for you five is greater_than the count from apply A where A dot cName equals A dot cName and let's take a look_at what this is doing So what we're doing here is we're looking_at each application record and we're checking whether the number of other applications to the same college were gonna find all the applications to the same college we're going to count them and we're going to see if that number is less_than five So it's a little_binary_digit non intuitive Now remember the result should be Berkeley Massachusetts_Institute of Technology Let's run the query and we do get Berkeley Massachusetts_Institute of Technology We again get duplicates however The reason is that we're running uh we're checking this condition for each application record So for each application to Berkeley we checked the condition that will be many of them and for each one to Massachusetts_Institute of Technology and so on Of_course as usual we can add distinct We can run the query and now we get the result It does happen actually that uh every query that can be written with the group_by and a having clause can be written in another form sometime it can be in extremely contorted but another sort of complicated exercise I like to give sometimes in my classes to prove the fact that you can write every group_by having query without using those clauses Now let's go_back to our original form of the query were we used the group_by and the having So we'll just get_rid of all this I'm_sorry I'm such a terrible typist Group by college name having count star less_than five and we'll run the query will get the same result What if we were interested not in those colleges that have fewer_than five applications but rather those colleges that have fewer_than five applicants It's a subtle point but instead of counting the number of apply tuples for each college what we want count is the number of distinct student ID's that appear in the group of apply tuples for that college So again now we're looking for the number of colleges who have fewer_than five distinct people distinct students_applying to them and now we see that one more college joins the group so Cornell had more_than five applications but at_least five applications but it has fewer_than five applicants Here's our grand finale query it has a having clause and group_by and in addition to that is has a sub query within the having clause that also uses aggregation What this query finds is all majors represented in the database where the maximum GPA of a student applying for that major is lower than the average_GPA in the database So we join student and apply and we group_by the major so we can look_at each major one at a time Within the major we find the maximum GPA of a student applying for that major and we check_whether it's lower than the average_GPA in the student relation We run the query and we discover that bioengineering and psychology at this point in time are garnering applicants whose uh highest_GPA is lower than the average in the database Of_course that does not represent reality In this video we'll be learning_about null_values in Structured_Query_Language As_usual we'll be considering a simple_college_admissions database and we'll be exploring the select from where query expressions Now instead of extending what we can write in queries in this video we'll be looking_at extending what's in the data Specifically in relational_databases unless specified otherwise any value in an attribute can take on the special value null Null is usually used to mean that the value is undefined or unknown For_example we might have a student whose_GPA is unknown or who doesn't have a GPA and we would represent that in our data with null Or we might create our reply relation with the decisions not yet decided and those decision values would also be null So what we're going to look_at in our demo queries is what_happens when we have null_values and we run queries over the database Here is the set of students in our database In order to explore what_happens when we have null_values let's insert two new students Kevin and Laurie each of which have a null GPA We go_back and we look_at our student table and now we see Kevin and Laurie as part of that table and in this particular interface null_values are depicted as blanks Now let's run some queries Our first query finds students_whose GPA is greater_than It's a simple query to write We run the query and we find a number of students in our result We don't have Kevin and Laurie because their GPAs are null so we can't determine that they're greater_than Let's run another query that says GPA less_than or equal to We'll get a different set of students as we see but we still don't get Kevin and Laurie Now let's run a query where we ask for the GPA to be greater_than or less_than or equal to And this is the one that's a little_binary_digit funny Actually we talked_about this very same query back when we were talking_about the relational_model When you look_at the query you would think that you would get every student in the database because everybody's GPA obviously is either greater_than or less or equal to Or from a more logical point of view this is an expression that's always true But you might not be surprised when we run the query we still don't get Kevin or Laurie because we can't determine that their GPA satisfies either of these conditions So even when we have a query that looks_like the where clause is a tautology meaning a logical expression that's always true we still don't get all the data in the result If we want this query to return all students in the database we can do that by adding one more clause to the where which is or GPA is null Now this is truly a condition that will be matched by every tuple because either the GPA is greater_than less_than or equal or it's a null value Is null is a keyword phrase in Structured_Query_Language that matches values that are null Now when we run the query we finally get all of the students in the database including Kevin and Laurie Just to drive the point home a binary_digit further let's continue looking_at queries where we have null_values involved in where clauses In this query we're looking for students where their GPA is greater_than or their size_high_school is less_than So let's run the query and see who we get We get all of these students including Kevin So although his GPA is null and therefore we can't determine that it satisfies this part of the condition because since his high_school_size is he does satisfy the second part of the condition and he does come out in our result Now let's add one more condition which is or size_high_school is greater_than or equal to So now we have again something in the condition that looks_like a tautology It looks_like it should always be true Everyone's high_school is either smaller or larger than Furthermore in this case we have no null_values for the high_school_size so when we run this query we should get back everybody and indeed when we run the query we do get back all of the students Now I'm not going to go into details here because you can certainly read about it The way the where clause is evaluated when null_values are involved is using a three valued logic where every expression is either true or false or unknown These logical values are combined to finally yield a value for the where expression that determines whether a couple is in the result Our last series of queries explores the interaction between null_values and aggregate functions specifically in this case the count function There are actually quite a few subtleties about null_values and aggregation and also null_values and sub queries We're not going to do an exhaustive exploration of these subtleties here but just give one example And we encourage_you if you are using a database with null_values to be very_careful when you write your queries to make_sure you understand how the null_values are going to influence the result OK so our query here is counting the number of students_whose GPA is not null for starters So we run the query and we see that there twelve students_who have a non null GPA Those are the students we had originally in the database before we added Kevin and Lorrie with their null GPA's Now instead of just counting the students let's count the distinct GPAs So among these twelve students how_many different GPAs are represented It turns_out there are seven distinct GPAs among those that are not null Let's further drop the not null condition and just count the the distinct GPAs among the students That will tell_us whether count distinct includes nulls or not in its count We'll run the query and we get the same answer so we still have GPAs and we are not counting the null_values Now let's do one last query which is to take away the and just look_at the distinct GPAs themselves So we had seven when we did the count We run the query now and we get eight What happened Well this first tuple is hard with the visualization This first answer is empty and that's actually the null value So when we right select the distinct GPA We do include the null value in our result if there is a null GPA However when we count the distinct values we don't include the null And that's really just one example of the type of subtleties and possibly unexpected behavior we might get when we have null_values So again I encourage_you to be very_careful when writing queries over databases that include nulls that you know exactly what you're going to get in your result In this final video we'll learn the modification statements of Structured_Query_Language There are statements for inserting data for deleting data and for updating existing data For inserting data there are two methods The first method allows_us to enter one tupple into the database by specifying it's actual value So that's the command here We say insert_into a table we specify the values of a tuple and the result of that command will be to insert one new tuple into the table with that value The other possibility is to run a query over the database as a select statement That select statement will produce a set of tuples and as long as that set of tuples has the same schema as the table we could insert all of the tuples into the table So those are the two methods of inserting data and we'll see those shortly in our demo To delete data we have a fairly simple command It says we delete from table where a certain condition is true So this condition is similar to the conditions that we see in the select statement And every tuple in the table that satisfies this condition will be deleted Now this condition can be fairly_complicated It can include sub queries It can include aggregation over other tables and so on again to be seen in our demo Finally we're interested in updating existing data and that's done through a command similar to the delete command It similarly operates on one table It evaluates a condition over each tuple of the table And now when the condition is true we don't delete the tuple Instead we modify the tuple We take the attribute that's specified here and we reassign it to have the value that's the result of the expression As we'll see in the demo this condition here can be fairly_complicated It can have sub queries and so on And this expression can also be quite complicated It can involve queries over other tables or the same table in the database Finally I wanted to mention that we can actually update multiple attributes in a tuple So if we're updating a table again exactly the same a single table A condition identifies the tuples to update but now we can update simultaneously any number of attributes each by evaluating an expression and assigning the result of that expression to the attribute As always our demos will use the simple_college_admissions database with colleges students and applications As_usual we have four colleges a bunch of students and a bunch of applications for the students for the colleges We'll first see some insert commands then some delete commands and finally some update commands As I_mentioned there's two different forms of insert command One that inserts a tupple at a time by specifying the values and another that uses subqueries to insert tuples So let's start with a simple one Let's add a new college Carnegie_Mellon to our database We do that by saying we want to insert_into college we use the keyword values and we simply list the values we want to insert the name of the college the state and the enrollment We run the query and we go take a look now at the college relation Let's go to college Here we go and we see now that Carnegie_Mellon has been added to our database Now let's do little_more complicated insert commands Now that Carnegie_Mellon is in our database let's have some students apply What we're going to do is have those students_who_haven't applied_anywhere yet apply to Carnegie_Mellon to be a computer_science major Let_me take it step by step Let's start_by finding those students_who_haven't applied_anywhere so this is a review of what we saw earlier We're going to find those students_whose ID is not in the sub query that selects all of the IDs in the apply relation We run the query and we discovered that we have four students_who_haven't yet applied anymore The next thing that we're going to do is turn this query into something that constructs the couples that we want to insert_into the apply relations As a reminder the apply relation has the student's ID the name of the college the major that they're_applying for and the decision So we'll construct the student's ID of course We want them to apply to Carnegie_Mellon We want them to major in Computer_Science And for now let's say that we don't know what the decision is so we'll put a null value in for the decision So let's run this query and now we see for our four students we've constructed a tuple four tuples that are ready to be inserted into the apply relation they have the appropriate schema So now that we have that query all ready to go we say insert_into apply and we run the query and we'll see that tuples are inserted in the apply relation let's go take a look_at the relation Let's try again here we've_got apply and now we see we have our four new tuples and as a reminder in this user interface a blank cell is what's used as a null value Now let's see a little_more action for Carnegie_Mellon Let's find students_who have applied for an EE Major at other colleges and have been turned down and let's have them apply to Carnegie_Mellon and let's accept them right away So again I'm going to do this step by step The first thing we'll do is we will find students_who have applied for EE at another college so we'll change this to N where the major equals EE and we want students_who were rejected so the decision equals no We have y n for our decision And let's just check how_many students we have in that category Let's run the query here And now we see there's two students And if we went back and looked at the apply relation we would find that indeed they'd applied to EE and been rejected Now let's turn this into a query that constructs tuples that we'd_like to insert_into the apply relation So we need the student_ID We want them to apply to Carnegie_Mellon Excuse my slow typing as usual This time they're going to apply to EE and we're going to accept them right away So let's just check that query So now we've produced two tuples that are ready to be inserted into the apply relation We say insert_into apply with the sub query we run it and we see that two rows have been inserted into apply Let's go take a look_at that And we need to refresh here And we look down and we see that indeed the two students have applied to EE and they've been accepted Now let's explore the delete command What we're going to do in this example is we're going to find all students that have applied to more_than two different majors and we're going to decide that they are unreliable students and we are going to entirely delete them from the database So let's start_by forming the query that finds the students_who have applied to more_than two majors Here it is You may want to remember the group I_am having clause So it says we go to the apply relation We form groups or partitions by SID's So we're going to consider the set of application for each student individually We're going to count how_many distinct majors there are in each group And if that number is greater_than we're going to return the student's ID and for now let's look_at how_many majors they applied for So we find in our database that there are two students_who have applied for three majors And we don't like those students so were just going to get_rid of them Here's how we do it we say delete from student where and as a reminder the delete command is of the form delete from the table where and then you can have a simple condition or a complicated condition so this is a sort of complicated condition We're going to see where their SID is in and we're going to turn this into a sub query We don't need to count distinct here and let_me just format this a little better Is in the set of student ids who have applied to more_than one more_than more_than two majors So we should be deleting students three four five and eight seven six if all goes well Let's run the query We did delete two rows let's go_back and take a look_at the students and we should find that three four five and eight seven six are gone and indeed they are Now we've deleted them from the students but we haven't deleted them from the apply relation so we can write the same deletion statement exactly as a matter of fact and just delete from applied Now I want to mention that not all database_systems allow this particular deletion Some database_systems don't allow deletion commands where the sub query includes the same relation that you're deleting from and it can be a little tricky but some do PostGRES does that's what we're running today and so we'll run the query and this time eight rows were affected so we had eight applications for students_who had applied to more_than two distinct majors and again it will be those same students Or we can go_back and check if we want to look in the apply relation we'll see that and are now gone from the apply relation as_well Now going back to our query as I_mentioned some database_systems don't support this form of deletion and so it's a binary_digit more_complicated to do this exact deletion in those systems The way to do it would typically be to create a temporary table put in the results of this sub query and then delete from apply where the student_ID is present in that temporary table You can give that a try on Structured_Query_Language light or MySQL and see if you get the same results you ought to Let's see one more deletion In this example we have decided that any college that doesn't have a Computer_Science applicant is probably not worth keeping in the database and we'll delete it We'll start_by doing the select command and then we'll translate it to a delete So this select command finds colleges where their college name is not among the college names in the apply relation where the major is Computer_Science In_other_words this finds all colleges where nobody has applied for Computer_Science We run the query and we discovered that nobody's applied for Computer_Science at Cornell If we want to now delete the tupple What we do it's very_simple transformation here We just say delete from college and then the rest is the same where the college name is not among those where someone has applied to Computer_Science We run the query it's successful and if we go_back and now we look_at our college relation we will see that Cornell is indeed gone Finally let's take a look_at some update commands In this first command we're going to find students_whose GPA is less_than three point six and who have applied to Carnegie_Mellon We're going to accept those students but we're going to turn them into economics majors Let's start_by finding the students_who have applied to Carnegie_Mellon with a GPA of less_than three point six here the query again this is a review from previous_videos we find in the apply relation where college is Carnegie_Mellon and where the student_ID is among those students_whose GPA is less_than three point six We run the query and find that there are two such applications So now what we want to do is update the apply relation and we're going to accept these students but for an economics major so we change the select to an update and this is were going to update the apply relation and we're going to update every tuple that satisfies the conditions we happen to know it's these two tuples and we're going to set the decision for the students to be equal to yes and we're going to set the major to be equal to economics We run the query it succeeded and we go now and we look_at the apply relation and we find these two students have now applied to economics and they've been accepted Here's a more_complicated and highly motivated update_command In this command we're going to find the student who has the highest_GPA and has applied to major in EE and we're going to change them from an EE major to a Computer_Science and Engineering major a computer_science and engineering rather_than just having them waste their time on electrical engineering All right so let's take a look_at this query which is going to find the applications that we're going modify This query is an excellent review from previous_videos because it has triply nested sub queries So again our goal is to find students_who have applied to Major in EE So here are students_who have applied to major in EE but we want them to have the highest_GPA of anybody who's applied to major in EE So in the innermost we find the students_who have applied to EE their student ID's then moving out one level we find the GPA's of all the students_who have applied to major in EE Now we find all students_whose GPA is at_least as high as the highest EE major EE applicant and finally we find all students_who have applied to EE and have that highest_GPA Whew OK let's run the query and see who this is We discover it is student And that student has applied three times actually to EE to Stanford Cornell and Carnegie_Mellon So now that we have identified those students transforming this command to do the update is actually relatively simple We update apply and instead of having the major be EE then we have the major be CSEE that major equals sorry Computer_Science and Engineering That's it we run the query And it succeeded and we go_back and we look_at the apply relation and we refresh and here we see that student has now applied to Computer_Science and Engineering rather_than EE at those three universities Let's see a couple more updates that are a little silly but demonstrate some features In this example we're going to give every student the highest_GPA and the smallest high_school in the database So what it's demonstrating is that in the set command the right_hand_side of the equals can itself be a sub query Here we're updating every student There's no where clause so that means every tuple is going to satisfy the condition and for each student we're going to set their their GPA to the result of finding the maximum GPA in the database and we're going to set their size_high_school to the result of finding the minimum size_high_school We run the query and we go take a look_at the student relation and we will see now that every student has a GPA of and every student has a size_high_school of two hundred what were formerly the largest and smallest values in the database Our very last query says we're in a great mood today let's go to our apply relation and let's accept everybody Very simple query just says update apply there's_no where conditions so every tuple satisfies the query We set the decision equal Y Here we go make a lot of students happy let's take a look_at the apply relation and we will see now that everybody has been accepted Now we'll learn_about the joined family of operators in Structured_Query_Language Going to our select from Where statement In the From clause we list tables separated by commas and that's implicitly a clause product of those labels but it's also possible to have explicit join of tables and this follows the relational Algebra style of join There's a few different types One of them is what's_called the inner join on a condition And we'll see that that's equivalent to what in relational Algebra we were calling the theta_join where the theta here is a condition So it's effectively taking the clause product but then applying the condition and only keeping the tupples in the clause product that satisfy the condition The separate type of join we can use in Structured_Query_Language is the natural_join and that is in fact exactly the natural_join in relational Algebra where it equates columns across tables of the same name so it requires the values in those columns to be same to keep the tupples in the clause product and then it also eliminates the duplicate columns that are created We'll see this very clearly when we get to the demo The third type of join operator and Structured_Query_Language is again interjoin but with a special clause called using and listing attributes and that's kind of again the natural_join except you explicitly list the attributes that you want to be equated And finally the fourth type and actually the most interesting type is the other join and there's a left outer join right outer join and full outer join and this is again combining tupples similar to the theta_join except when tupples don't match the theta condition They're still added to the result and patted with no values Now I will say right off that none of these operators are actually adding expressive part of Structured_Query_Language All of them can be expressed using other constructs But they can be quite useful in formulating queries and especially the outer join is a fairly_complicated to express without the outer join operator itself So as usual we'll be doing our demo with our simple_college_admissions database with college tables student table and applied table So let's move_ahead to the demo As_usual we'll have four colleges a bunch of students and students_applying to colleges Let's start with the simple parade that we've_seen before which matches students names with majors to which they've_applied So that combines the student in apply relation making sure the student_ID is the same across the two relations and gives_us the names and major back Now if you remember your relational Algebra you can see clearly that this is a join of the student on apply relation Actually a natural_join but we'll come to them in We're gonna first rewrite it using the equivalent of a theta_join operator which is called inner join in Structured_Query_Language and so this does the theta_join or the combination of student and apply on a specific condition so we'll change the Where to on and it is effectively the cross_product of the two tables But then when it does the cross_product it checks this condition and only keeps the tuples that satisfy the condition So let's run that query and of course we get the same result The two queries we saw are exactly equivalent we're just expressing them a little_binary_digit differently Now the inner join is the default join operator in a Structured_Query_Language So we can actually take away the word inner and when we run that we again get the same result because join is an abbreviation for inner join happens when we have a joint operator with additional conditions besides the one on the two tables So this is also a query that we've_seen before This times the name and GPA of students_who came from a high_school with less_than a thousand students They've applied to major in Computer_Science at Stanford So we ran the query and we find just to a students in our result So now let's rewrite that using the join operator So we type join instead of comma the comma being the cross_product and the join condition is again combining the student and apply records where the student_ID matches and the rest of this becomes our Where condition Go ahead and run the query and we get the same result Now it turns_out that we can actually put all of these conditions into our On clause so we can make this Where back into an And and our On clause now is the And of all all three conditions We run the query and we get the same result Now you're probably thinking how do I know what to put in the On clause and what do I put in the Where clause because this are obviously equivalent Well first of all there are many equivalent queries in Structured_Query_Language We can write things in different_ways In theory Structured_Query_Language query processor should execute them all in the most efficient possible way but the join clause in particular is often used as a hint to the query processor on how to execute the query So if we put all of these in the On condition We're sort of saying as the query processor does the join it should be all the conditions when we make this aware It's sort of a hint saying here's the condition that really applies to the combination of the tuples and the rest of the conditions apply to separate attributes Now let's take at what_happens The second way to check is based_on a set of axioms a set of rules called Armstrong's axioms We saw some rules for functional_dependencies earlier but Armstrong's Axioms are a specific set of rules that are what's_called complete It's guaranteed that if one thing about functional_dependencies can be proved from another then it can be proved using the Armstrong's Axioms I'm not going to cover Armstrong's Axioms in the videos but you can look_at any of the recommended readings and find them there So you_might_wonder why did I introduce this notion of one set of functional_dependencies following from another and for that matter why did I introduce trivial and non_trivial functional_dependencies Well I'm going to sum up in one sentence what we're looking for when we specify the set of functional_dependencies for a relation So we have a notion of the real_world data we have our attributes but we need to specify the functional_dependencies in order to get a good designer for some of the reasons that I_mentioned What we would like to find is a minimal set of completely non_trivial functional_dependencies such that all functional_dependencies that hold on the relation follow from using the technical definition I gave the dependencies in this set Wow that seems like some very complicated thing but the fact is when you start specifying functional_dependencies you'll discover that you will actually get this definition pretty naturally So to conclude functional_dependencies are a generally useful concept in database_systems They 're a key ingredient of doing relational_design by decomposition because we use the functional_dependencies to get Boyce_Codd_Normal_Form which is what we'll cover in the next_video but they're also useful for the system to determine how to store data to compress data and also to reason about query processing In this sequence of videos we'll learn_about designing good schemas for relational_databases So let's_suppose we're building a database for an application or set of applications and we have to figure_out what schema we want to store our data Usually there are many different possible schema designs for a database and databases do tend to get quite complicated And some designs are much better than others So how do we choose what design to use Now the reality is that people often use higher_level tools to design relational_databases and don't design the schemas directly themselves But some designers do go straight to relations and furthermore it's useful to understand why the relations that are produced by design tools are what they are Furthermore from an academic point of view it turns_out there's a very nice theory for relational data base design So let's consider the process of designing the schema for our database about students_applying to colleges Specifically for a given student let's_suppose we have their social_security_number and their name the colleges that student is applying to the high_schools they attended and what city those high_schools were in and the student's hobbies So if that's what we want we can create a single relation called apply that has one attribute for each of those pieces of information Now let's take a look_at how that database would be populated Let's_suppose that we have a student Anne with Social_Security_number she went to different high_schools in Palo_Alto she plays tennis and the trumpet and she's applying to Stanford Berkeley and Massachusetts_Institute of Technology So let's look_at some of the tuples that we would be having in the apply relation to represent this information_about Anne So we'll have Anne her name she's applying to Stanford she went to Palo_Alto High_School and that's in Palo_Alto and one of her hobbies is tennis And then we also have and she applied to Berkeley and went to Palo_Alto High_School in Palo_Alto and tennis there as_well Of_course she also has a tuple representing the fact that she's applying to Berkeley and and we'll stick with Palo_Alto High_School and she played the trumpet And as you can see we'll have more tuples we'll have various Stanford and Berkeleys we'll have some for her other high_schools called Gunn High_School also in Palo_Alto and so on So if we think_about it we will need a total of tuples to represent this information_about Ann Now do we think that's a good_design I'm going to argue no it's not a good_design There are several types of anomalies in that design First of all we capture information multiple_times in that design and I'll give some examples of that For_example how_many times do we capture the fact that the Social_Security_number is associated_with a student named Ann We capture that twelve times in our twelve tuples How many_times do we capture that Anne went to Palo_Alto High_School We're going to capture that six times And we're going to capture the fact that she plays tennis six times And we're going to capture the fact that she went to apply to Massachusetts_Institute of Technology four times so for each piece of information in fact we're capturing it many many_times So that doesn't seem like a good feature of the design The second type is an update anomaly and that's really a direct effect of redundancy What update anomalies say is that you can update facts in some places but not all all or differently in different places So let's take the fact for example that Ann plays the trumpet I might decide to call that the coronet instead but I can go_ahead and I can modify say three of the incidences where we captured the fact about her playing the trumpet and not the fourth one and then we end up with what's effectively an inconsistent database And the third type of anomaly is called a deletion anomaly and there's a case where we could inadvertently completely do a complete deletion of somebody in the database Let's say for example that we decide that surfing is an unacceptable hobby for our college applicants and we go_ahead and we delete the tuples about surfing If we have students_who have surfing as their only hobby then those students will be deleted completely Now you may argue that's the right thing to do but probably that isn't what was intended So now let's take a look_at a very different design for the same data Here we have five different relations one with the information_about students and their names one where they've_applied to colleges one where they went to high_school where their high_schools are located and what hobbies the students has In this case we have no anomalies If we go_back and look_at the three different types they don't occur in this design We don't have redundant information we don't have the update anomaly or the deletion anomaly Furthermore we can reconstruct all of the original data from our first design so we haven't lost any information by breaking it up this way So in fact this looks_like a much better design Now let_me_mention a couple of modifications to this design that might occur Let's_suppose for example that the high_school name alone is not a key So when we break up the high_school name and high_school_city we no_longer can identify the high_school In that case the preferred design would be to move the high_school up here so we'll have that together with the high_school name and then we don't need this relation here And actually that's a fine design It does not introduce any anomalies that's just based_on the fact that we need the name of the high_school together with the city to identify it As another example suppose a student doesn't want all of their hobbies revealed to all of the colleges that they are applying to For_example maybe they don't want Stanford to know about their surfing If that's the case then we can modify the design again and in that case we would put the hobby up here with where they're_applying to college And so that would include the hobbies that they want to reveal to those particular colleges and we'll take away this one So it looked like we were taking our nice small relations and moving back to a design that had bigger relations But in this case it was very well motivated We needed these attributes together to identify the high_school and we want it to have our hobbies specific to the colleges So what that shows is that the best design for an application for relational_databases depend not only on constructing the relations well but also in what the data is representing in the real_world So the basic of idea of what we're going to do is design by decomposition specifically we're going to do what we did at the very beginning of this example which is start_by creating mega_relations that just contain attributes for everything that we want to represent in our database then we're going to decompose those mega_relations into smaller ones that are better but still capture the same information And most importantly we can do this decomposition automatically So how does automatic decomposition work In addition to the mega_relations we're going to specify formally properties of the data The system is going to use the properties to decompose the relations and then it's going to guarantee that the final set of relations satisfy what's_called a normal_form And we'll be formalizing all of this But the basic_idea behind normal_forms is that they don't have any of those anomalies that I showed and they don't lose any information So specifically for specification of properties we're going to begin by looking_at something called functional_dependencies And once we specify functional_dependencies the system will generate relations that are in what's_called Boyse Codd_normal form And Boyse and Codd by the way were two early pioneers in relational_databases in general Then we're going to look_at another type of specification called multi_valued dependencies which will add to functional_dependencies and when we have both functional and multi_valued dependencies then we can have what's_called fourth_normal_form and again that would be relations that are generated by the system that satisfy the normal_form Boyce_Codd_normal_form is stricter than fourth_normal_form Specifically if we make a big Venn diagram here of all the relational designs that satisfied Boyce_Codd_Normal_Form which by the way is very often abbreviated Boyce_Codd_normal_form then that contains all of the relations that satisfy fourth_normal_form normally abbreviated Fourth_Normal_Form So every relation that's in fourth_normal_form is also in Boyce_Codd_normal_form but not vice versa You_might be wondering what_happened to first second and third normal_forms So first normal_form is pretty_much just a specification that relations are real relations with atomic values in each cell Second normal_form is specifying something about the way relations are structured with respect to their keys Neither of those is discussed very much anymore Third normal_form is a slight weakening of Boyce_Codd_normal_form and sometimes people do like to talk_about third_normal_form So you can think of third_normal_form as a little_binary_digit of a even bigger circle here We're not going to cover third_normal_form in this video because Boyce_Codd_normal_form is the most common normal_form used if we have functional_dependencies only and fourth_normal_form if we have functional and multivalued_dependencies So what's going to happen next is I'm going to give some examples to motivate these four concepts functional_dependencies Boyce_Codd_normal_form multivalued_dependencies normal_form and then later_videos will go into each one in much greater depth So let_me just give the general idea of functional_dependencies and Boyce_Codd_Normal_Form And we'll use a very_simple for example an abbreviated version of our apply relation that has students' social_security numbers the student's name and their colleges that the student is applying to Even this small relation actually has redundancy and update and deletion anomalies Specifically let's say that our student Ann applies to colleges Then there will be tuples and there will be instances where we know that a student with the social_security_number is named Ann Specifically we're going to store for every student the name and social_security_number pair once for each college that they apply to So now let_me explain what a functional_dependency is and then we'll see how functional_dependencies are used to recognize when we have a bad design like this one and to see how we can fix it A functional_dependency in this case from social_security_number to name and we're saying social_security_number functionally determines the student name says that the same social_security_number always has the same name In_other_words every time we see we're going to see Ann Now it doesn't necessarily go in the other direction It might not be that whenever we see Ann it's but whenever we see it is Ann And so what we'd_like to do is store that relationship just one time One time say that for the name is Ann Now what Boyce_Codd_Normal_Form says is that whenever we have one of these functional_dependencies then the left_hand_side of that functional_dependency must be a key And think_about what that's saying Remember a key says that we have just one tupple with each value for that attribute So if we have say social_security_number to name as a functional_dependency and we satisfy Boyce_Codd_Normal_Form then we're going to say that social_security_number has to be a key in our relation and we'll only have one tupple for each social_security_number Specifically we can go_back to our original relation We have this functional_dependency social_security_number here is not a key right So then we know that this is not in Boyce_Codd_Normal_Form So we're going to use functional_dependencies to help us decompose our relation so that the decomposed_relations are in Boyce_Codd_Normal_Form And here's what would happen in this example Our functional_dependency would tell_us to pull out the social_security_number and student name into its_own relation where the social_security_number is a key and then we have just one time for each social_security_number that students name and then separately we'll have the information_about the students and which colleges they applied to Again we'll completely formalize this whole idea the definition of functional_dependencies their properties the normal_form and how we do the decomposition in a later video Now let's similarly motivate the concept of multi_value_dependencies and fourth_normal_form It is actually a little_binary_digit more_complicated but it follows the same rough outline Now let's look_at a different portion of the information_about applying and let's_suppose for now that we're just concerned about students what colleges they're_applying to and what high_schools they went to We still have redundancy and update and deletion anomalies For_example a student who applies to Stanford is going to have that fact captured once for every high_school that they went to A student who went to Palo_Alto high School will have that fact captured once for every college they apply to In addition we get a kind of multiplicative effect here Because let's say a student applies to C colleges and they went to H high_schools I know students don't go to a lot of high_schools but let's_suppose that this is one that had moved a lot In that case we're going to have C times H tuples What we'd really like to have is something more on the order of C plus H because then we'd be capturing each piece of information just once Now the interesting thing is that the badness of this particular design is not addressed by Boyce_Codd_Normal_Form in fact this relation is in Boyce_Codd_Normal_Form because it has no functional_dependencies It's not the case that every instance of a social_security_number is associated_with a single college name or a single high_school As we will see later if there are no functional_dependencies then the relation is automatically in Boyce_Codd_Normal_Form but it's not in and fourth_normal_form So fourth_normal_form is associated_with what are called multi_value_dependencies When we specify a multi_value_dependency as we've done here with the double arrow what this is saying is that if we take a particular social_security_number in the relation we will have every combination of college names that are associated_with that social_security_number with every high_school that's associated_with that social_security_number We'll actually see that when we have this multi_value_dependency we automatically have this one too I know it seems a binary_digit complicated and we will formalize it completely but for now now just think_about the English statement that multi_valued dependency is saying that we are going to have every combination of those two attributes and values in those attributes for a given social_security_number In_other_words those values are really independent of each other So if we have that situation then what we should really do is store each college name and each high_school for each social_security_number one time and that's what fourth_normal_form will do for us Fourth normal_form similarly to Boyce_Codd_normal_form says if we have a dependency then the left_hand_side must be a key In this case it's a multi_value_dependency we're looking_at so it's really saying something different but the basic_idea is the same which is that we want only one tuple that has each value that's appears on the left_hand_side of a multi_value_dependency So let's see what would happen in this example if we use our multi_value_dependencies to decompose the relation based_on the idea of fourth Normal_Form Well it is the intuitive thing that happens We separate the information_about the college names that a student applies to from the information_about the high_schools themselves and then we'll see that that we only store each fact once and we do get the behavior of having C plus H tuples instead of having C times H tuples Like with functional_dependencies and Boyce_Codd_Normal_Form we'll be completely formalizing all of this reasoning and the definitions in later_videos To summarize we're going to do relational_design by decomposition We're going to start_by specifying mega_relations that contain all the information that we want to capture as_well as specifying properties of the data usually reflecting the real_world in some fashion The system can automatically decompose the mega_relations into smaller relations based_on the properties we specify and guarantee that the final set of relations have certain good properties captured in a normal_form They will have no anomalies and they'll be guaranteed not to lose information We'll start_by specifying properties as functional_dependencies and from there the system will guarantee Boyce_Codd_Normal_Form and then we'll add to that properties specified as multi_value_dependencies and from there the system will guarantee fourth_normal_form which is even stronger than Boyce_Codd_Normal_Form and is generally thought to be good relational_design This video covers functional_dependencies First a quick recap of relational_design by decomposition The idea is that the application designer writes mega_relations that contain all the information that we want to have and properties of the data that we're storing And then the system will automatically decompose those based_on the properties that are specified The final set of decomposed_relations will satisfy what's_called the normal_form and normal_forms are good relations in the sense that they have no anomalies and they don't lose information from what was specified in the original mega_relations Now the properties themselves are defined either as functional_dependencies in which case the system will generate Boyce_Codd_Normal_Form relations or multi_value_dependencies which will then yield fourth_normal_form relations So this video as you can tell is about functional_dependencies themselves And let_me say that functional_dependencies are actually a generally useful concept in databases not only for relational_design So for functional_dependencies as we'll see soon are a generalization of the notion of keys and they allow the system to for example store the data more efficiently when the system knows about functional_dependencies Compression schemes can be used based_on functional_dependencies for storage and also functional_dependencies as a generalization of keys can be used to reason about queries and for query optimization Which is a reminder of a very_important aspect of database_systems Which allows declarative queries to be executed by the system efficiently By the way a third use of functional_dependencies is for exam questions in database courses because there is a very nice theory functional_dependencies as you'll see It's quite easy to write questions about them The remainder of the video will cover functional_dependencies in general as a general concept and not specifically to relational_design and then later_videos will tie functional_dependencies back to design by decomposition As always we'll be using as a sample a college application database and this case I've expanded the information that we're including quite a binary_digit We'll be using these same two relations as examples throughout this video and subsequent videos on relational_design In this case we're gonna look_at two relations one with information_about students and then a separate one with information_about where they're_applying The student information will have social_security_number the student's name their address and then three attributes about their high_school We'll assume there are unique codes for high_schools but they also have a name and are in a city finally the student's GPA and a priority field for admissions that we'll see in a moment For applications we'll have the student's social_security_number the college name they're_applying to the state of the college the date of application and the major Not all of these attributes will even be used in this video but like I said this will permeate several videos as our running example To motivate functional_dependencies let's focus_on the student relation and specifically on the GPA and priority attributes Let's_suppose that a student's priority is determined by their GPA For_example we might have a rule that says if GPA is greater_than then priority is If the GPA is say in between let's say and Then we'll set priority to be equal and let's say if the GPA then is less_than then the priority value is three So if this relationship is guaranteed in our data then what we can say is that any two tuples that have the same priority are guaranteed to have the same GPA And let's formalize that concept So I'm going to write a little logical statement here to formalize the concept I'm going to use the for all symbol from predicate logic and I'm going to say if we have any pair of tuples So for all T or U those are tuples in the student relation then if the student if the T and U have the same priorities I'm_sorry the same let_me fix that they have the the same GPA So if T GPA equals U GPA then and this is the logical implication symbol then T priority will equal U priority So this logical statement is in fact a definition of a functional_dependency and we would write that functional_dependency as GPA arrow priority so that says the GPA determines the priority any tuples with the same GPA must have the same priority So that was a specific example Now let's generalize our definition So let_me replace GPA and priority here with just two attributes A and B E of say a relation R And then we'll also need to modify our definition So you can see I've erased the specific attributes and relation And I'll just say for every T and U in our relation R If T dot A equals U dot A then T dot B equals U dot B and that's the definition of the functional_dependency A_determines B for a relation R Actually I'm gonna generalize this definition even further because functional_dependencies don't always have to have one attribute on each side they can actually have a set of attributes So now I write A A dot_dot dot AN on the left_hand_side these will all be attributes of relation R And on the right_hand_side B B comma BM again attributes of R Modifying the formal definition in red now I can't use the dot notation anymore so I'll use a square_bracket and I'll write A through An equals U square_bracket A through AN so what I'm saying here in this case is that the two tuples T and U have the same values for all of the attributes A through A N and if they do then they will also though have the same values for B through B M We'll be getting to some concrete examples soon Just one last binary_digit of notation before we move on For simplicity I'm going to often in the video abbreviate a list of attributes or set of attributes by using a bar So I'll write A_bar to indicate a set of attributes A and B bar to indicate a set of attributes B And again this is just for convenience So we've_seen the motivation for a functional_dependency in a relation A functional_dependency for a relation is based_on knowledge of the real_world data that's being captured And when we specify one just like specifying keys all instances of the relation must adhere to the functional_dependency So just to summarize functional_dependencies we say that attribute a set of attributes A functionally determines a set of attributes B if again any time tuples agree in their A values they also agree in their B values And let's say that our relation here R has the tuples A the tuples and B and also a few more attributes we'll call those C So let_me draw a picture of a relation now here that has those attributes in it So we'll have here lets just three columns but again these are multiple attributes And these are the attributes A these are the attributes B and these are the attributes C And if we put in a couple of tuples then what we'll say is if we have two tuples here that have and I'm gonna use a bar even for the values in the tuples If we have two tuples whose A values are the same then their B values must also be the same And we're going to be using this type of template for some reasoning later on But we're not saying their C values have to be the same so we could have C and and different C values here as_well But again if we specify this functional_dependency we are saying that every instance of our relation must satisfy the condition that if the A values are the same then the B values are also the same Finally let's come_back to our example And I think when we start writing functional_dependencies for our actual relations it'll give you a good idea of what they're really capturing So let's write a few functional_dependencies for our student relation based_on what we expect to be true in the real_world in the data that we're capturing in the relation So here's a first example Social_Security_number functionally determines S name the student's name So what we say we have multiple tuples about a particular student and they have the same social_security_number say two tuples about student we're expecting them to have the same name in fact we're requiring them to have the same name And presumably because to is sort of identifying the student that would be a natural functional_dependency that would hold in this case and similarly we would expect social_security_number to determine address although we're already making an assumption about the real_world here if we have this particular functional_dependency then we're saying a student doesn't move They don't have multiple addresses Every tuple that describes that student by their social_security_number will have the same address Let's go to the high_school and see what might be going on there So I_mentioned that the high_school_code what I'm trying to capture there is a unique code for each high_school that might be filled in college application then we would expect the high_school_code to determine the high_school name Every time we have the particular high_school_code maybe for different students it would have the same name and also it would have the same city So that's an example of a functional_dependency with two attributes on the right_hand_side Now let's look_at one that's a little_more complicated which is one that has two attributes on the left_hand_side instead That actually turns_out to be a more interesting case In fact in this particular case we can probably reverse the arrow and have a functional_dependency in the other direction If we have a combination of high_school name and high_school_city I'm going to assume that's unique that there aren't there's never two high_schools with the same name in the same city And if that's the case if that's unique then we would expect a functional_dependency to the high_school_code Any time we have the same name and city we're talking_about the same high_school so we should have the same code What other examples do we have If we assume that there's one GPA for each student then we'd have the social_security_number determines the GPA and we already talked_about GPA determines_priority and another example actually if we put these two together we should see well if we have the same social_security_number twice we should have the same priority And you may be thinking well that's kind of a transitive rule if it takes these two and produces that one And indeed it is And we'll talk_about rules for functional_dependencies later And there may be more in this case Now let's take a look_at functional_dependencies for our apply relation Actually this one is a little trickier it's even possible there are no functional_dependencies at all It really depends on the real_world data the real_world constraints One possibility for example is that every college has a particular single date on which it receives its application So if that were the case then we'd have the college name determines the date In_other_words every application for a particular college must have the same date Another constraint might be that students are only allowed to apply to a single major at each college they apply to So if that were the case this is another one with two attributes on the left_hand sid we'd say that the social_security_number together with the college implies the major In_other_words we cannot have a student and college combination with two different majors and that captured that constraint Maybe we have a constraint that students are only allowed to apply to colleges in one state That seems rather unlikely but I was struggling to find functional_dependencies for this case In that case we'd have this function dependency again saying a student could only apply to colleges in a single state For the apply relation specifically again it's really the real_world constraints that drive which functional_dependencies hold for the relation But it's important to understand those constraints so they can be translated to functional_dependencies which then can drive a good relational_design So I've alluded a few times to the fact that functional_dependencies generalize the notion of keys And let's make that connection explicit now Let's_suppose we have a relation R and R has no duplicate tuples R has some attributes A and it has some other attributes Let's call those B And let's_suppose that we have a functional_dependency that A_determines all of the attributes in the relation Now let_me draw a picture of that So here's our relation R with attributes A and attributes B And now let's_suppose that we have two tuples with the same values for A Now we'll just write those as little a bar And now let's see what_happens with the B values We'll make them B bar and B bar Because we have the functional_dependency the equal values here for A say that B and B have to be equal So B equals B Let's just erase the little one and two But now we've generated duplicate tuples So what the functional_dependency tells_us in that case is that these two tuples are actually the same or rather we cannot have two separate tuples with the same A values So we cannot have two separate tuples with the same A values is exactly what it means for A to be a key Now again this is only the case when we have no duplicates in R But if we have no duplicates if a set of attributes determines all the other attributes then those attributes are a key So here are a few other definitions related to functional_dependencies We have a notion of a trivial functional_dependency A functional_dependency is trivial A to B if B is a subset of A And let's draw a little picture of what that means So here we have our attributes A and then we're saying and that's all of the attributes here and what we're saying is that B is a subset of A So in other_words some portion of these attributes here we'll just mark that in purple here are attributes B Well it's pretty obvious that if two tuples have the same values across their entire expanse here for A's then obviously they're also going to have the same values for just this portion here the B portion So that's why it's called the trivial functional_dependency So a nontrivial functional_dependency is a functional_dependency that's not a trivial one By the way FD is a common abbreviation for functional_dependency So if we have A_determines B then that is non_trivial if it's not the case that B is a subset of A Going to our picture let's have here our attributes A And now we're saying there are some attributes in B that are not part of A So we can say well maybe B is partially part of A but there's some portion that is not part of A So let's say that these are our B attributes here So now our functional_dependency is actually saying something It's saying we have two attributes that agree in these values then they're also going to agree in these values over here And the last definition is a completely nontrivial functional_dependency and that's A_determines B where A and B have no intersection at all So in that case again going back to our picture we'll have our A attributes here and then our B attributes are going to be some portion of the remaining_attributes And here we're saying a lot We're saying if these two have the same value then these two have the same value as_well And the reality is that completely nontrivial functional_dependencies are the ones that we're most interested in specifying I_mentioned that there are some rules that apply to all functional_dependencies and I'll give a couple of those right now The first one is the splitting_rule The splitting rules says that if we have a set of attributes that determine another set of attributes and this time I'm going to write it out instead of using the bar notation then we also have this implies that we have A_determines B and A_determines B and so on In_other_words we can split the right side of the functional_dependency And if you think_about it this is pretty If we say that the when the A value are the same all of the B values have to be the same then certainly when the A values are the same the B values have to be the same independently Now you_might_wonder if the splitting_rule also goes the other way So let's say we have I'll put the left_hand_side I'll write out the attributes explicitly so let's say we have A through A N determines_B then is it from that the case that A_determines B and A_determines B on its_own and so on Well the answer to that is no And I'll give a simple example from our college application database So let's say that we have the functional_dependency high_school name and high_school_city determines high_school_code We talked_about that one before Oops here that's an arrow there So that says that when we have a particular name and city combination for a high_school that's identifying a single high_school and so it will always have the same code So that makes a lot of sense but it is not the case I'll put a big X here necessarily that if we split the left_hand_side that high_school name alone will determine a high_school_code So for example I would venture to guess that there are a lot of Jefferson High Schools all over the country and they won't all will be the same high_school So it's really the combination of attributes on the left that together functionally_determine the right_hand_side and so we do not then have the splitting_rule as a general principle The combining rule is the inverse of the splitting_rule It says if we have A_determines B and we have A_determines B and so on up to A_determines B N then we also have A_determines B through B N together Next we have two trivial dependency rules Let_me remind remind_you what a trivial dependency is It's A_determines B where B is a subset of A So in other_words every left_hand_side determines itself or any subset of itself and that's what drives the two trivial dependency rules One of them says that if we have A_determines B then we also have A_determines A union B so in other_words we can add to the right_hand_side of every dependency what's already on the left_hand_side Sort of as a converse we can also say that if A_determines B then A_determines A intersect B Actually this one is also implied by the splitting_rule So we have two different rules in this case that are doing the same thing And finally the transitive rule which is the one we alluded to earlier It says if we have A_determines B and and we have B determines C then we have A_determines C Now all of these rules can actually be proven formally and I'm going to go through a sketch of this particular one So here's my relation R and I'll have attributes A B C and then let's let D be the left of our attributes And my goal is to prove that A_determines C And the information I have to prove that is these two functional_dependencies here So to prove that A_determines C I have to prove that if two tupples have the same A values we'll put little bars there then they also have the same C values So I need to show that these two C values here are going to be the same So you can see what's going to happen Using the first functional_dependency because these two A values are the same I know their B values must be the same And then I just use the second functional_dependency and because the two B values are the same I then know that the two C values are the same and that has shown that this functional_dependency holds And you can do a similar thing to prove the other rules to yourself Now I'm going to introduce the concept of closure of attributes Let's_suppose we're given a relation a set of functional_dependencies for that relation and then a set of attributes A_bar that are part of that relation I'm interested in finding all attributes B of the relation such that A_bar functionally determines_B And this is what's_called the closure and I'll show in a binary_digit why we might want to compute that Notationally the closure is written using the sign So the closure of A_bar is A_bar Let_me be a little_more explicit let_me write out A_bar because remember whenever we write bar we're actually talking_about a list of attributes So we're going to write it as A through A N and I'm interested in computing the closure of that set of attributes In_other_words the entire set of attributes that are functionally determined by the attributes A through A N And I'm going to give an algorithm for doing that My algorithm says start with the set itself So I'm going to start with A through AN except I'm not going to close that I'm going to leave a little space there And then I'm going to repeat until there's_no change I'm going to add attributes to that set until I get to the closure So I'm going to repeat If A_determines B and now we'll put bars in here and all of A are in the set then add B to the set So I might have my attributes here A through AN and it might be the case that A you know determines attributes C and D so I'll add C and D to the set I repeat Maybe there's a C goes to E and I'll add E to the set and so on And when there's_no more change then I've computed the complete closure Now if you happen to be someone who likes to think in terms of rules instead what we're effectively doing is applying the combining and transitive rules to the attributes in the set until there's_no change So let's run an example of the closure algorithm Let's go to our complete student table and let's add three functional_dependencies One that says that student's social_security_number determines their name address and GPA and that would be a normal GPA determines_priority and high_school_code determines high_school name and high_school_city Again these are all examples we gave earlier that would be natural functional_dependencies for this particular relation Let's_suppose that we're interested in computing the closure of the two attributes social_security_number and high_school_code So in other_words we want to find all attributes in the student relation that are functionally determined by these two attributes So the algorithm says start with the two attributes themselves social_security_number and high_school_code and then add attributes that are functionally determined by ones in the set So if we start with our first functional_dependency here because we have social_security_number that allows_us to add the student name the address the GPA and that's it for that one Now let's repeat again Because we have the GPA our second functional_dependency tells_us we can add the priorityAnd that's it for that one And then since we have the high_school_code in this set our third one tells_us that we can add the high_school name and the high_school_city and at that point we certainly know we're done because we've actually got the entire relation at this point Now I didn't pick this particular example at random We took two attributes social_security_number and high_school_code we computed their closure and we discovered that they together functionally_determine all attributes of the student relation Now remember just a few slides ago when a set of attributes functionally_determine all the attributes then those attributes together form a key for the relation And in fact if you think_about it social_security_number and high_school_code together are a natural key for the relation A student might go to multiple high_schools but there's_no reason to have more_than one tupple with the combination of a particular student and the high_school they attended So let's formalize a binary_digit this relationship between the notion of closure and keys Let's_suppose we're interested in answering the question is a particular set of attributes a key for a relation we can use closure to do that Remember we have the relation and we have a set of functional_dependencies for the relation so what we do is we compute the closure of A that's going to give_us a set of attributes and if that equals all attributes then A is the key As a more general question let's_suppose we're given a set of functional_dependencies for a relation how can we use it to find all of the keys for the relation So use the same basic_idea of computing closure but this time we have to do it for every subset of the attributes So let's call each subset A_bar and we just check if the closure of A_bar determines all attributes And if it does then it's a key And by considering every subset of the attributes in R then we're considering every possible key and we'll just check for each one whether it's actually a key Now that seems fairly inefficient and actually we can be a little_more efficient than that We can consider these subsets in increasing size order So for example if we determine that A and B together determine all attributes functionally_determine all attributes in the relation That tells_us AB is a key and that actually also tells_us that every superset of AB is also a key And if you think_about it that fills out naturally So the real algorithm would say let's start with single attributes and determine if they are key If a single attribute say C is a key then so is every super set of C and then we go to pairs of attributes and so on Finally let's talk_about how we specify the set of functional_dependencies for a relation First I'll define a notion of one set of functional_dependencies following from another one So let's_suppose we have a relation R and we have two sets of functional_dependencies that aren't identical S and S We see that S follows from S if every relation instance satisfying S also satisfies S As a simple example suppose S is social_security_number determines_priority and suppose S is the two functional_dependencies social_security_number determines GPA and GPA determines_priority Then it's certainly the case that in an for this example S follows from S Every time we have a relation that satisfies social_security_number determines GPA and GPA determines_priority then that relation will also satisfy social_security_number determines_priority and we kind of proved that actually in an earlier part of this video So one question you might have is how do we test whether one set of functional_dependencies follows from another That really boils down to testing whether one functional_dependency follows from a set So and let_me just make this A_bar B bar here to make clear they can be sets of attributes Because if we have S and S then we just check_whether every functional_dependency in S follows from the functional_dependencies in S There's actually two ways of going about this test One of the ways is to use the closure We'll compute A based_on the functional_dependencies that are in S and then we'll check if B is in the set Reminder computing the closure tells_us every attribute that's functionally determined by the attributes in A based_on the functional_dependencies in S If those include B then A_determines B does indeed follow from S Now that we've learned about functional_dependencies let's talk_about how they're used to create relations that are in Boyce_Codd_Normal_Form Very quick reminder about relational_design by decomposition The database designer creates mega_relations that contain all the information to be captured and specifies properties of the data to be captured The system uses the properties to decompose the relations into smaller ones and those final decomposed_relations satisfy what is known_as a Normal_Form They don't have anomalies and they don't lose information Functional dependencies are used to create relations in Boys Codd_Normal Form and multi doubt value dependencies are used to create relations in Fourth_Normal_Form This video talks_about the process of using functional_dependencies to create relations in Boyce_Codd_Normal_Form Let's start_by defining what it means to do a decomposition of a relational schema Let's_suppose we have a relation R with a set of attributes A through A N We can decompose R into two relations We'll call them R and R such that R has I'll just label them B through B K and C through C and let's say let_me use the notation for the list of attributes A B and C that I've been using in other videos So R and R are a decomposition of R First of all if the attributes that capture B union C are equal to the set of attributes we started with A In_other_words recovering all of the attributes and furthermore this is the tricky part R natural_join R equals R So let_me draw this pictorially So here's our relation R and all of our attributes together are the A attributes And then we're going to decompose R into R and R So let's say this first set of attributes here are the B attributes and the second bunch of attributes here with some overlap are the C attributes So now R consists of this portion of R and the purple part here now is R So clearly the B and C attributes are equal to the original attributes and then is the join of R and R giving_us R Now remember all of this is logical We don't have R itself and we don't have the data and we don't have R and R so everything is being done at the schema level And we will explore later how we can guarantee that this join does return R and not something else Now just using a little_binary_digit more relational_algebra here let_me_mention that R can be defined as the projection on the B attributes of R and then in purple R is the projection of the C attributes of R So again all of this is logical but the idea is that when we do the projection if there are duplicates that are present simply because we have say different values in the remaining_attributes those duplicates don't have to be retained in the projection We saw in some of our examples in other videos where we had redundancy because we were capturing information multiple_times that we didn't need to And we're going to see that what Boyce_Codd_Normal_Form really does is separate the relations so that we capture each piece of information exactly once I know that's very abstract now but when we see examples we'll see how that works So let's look_at two possible decompositions of the student relation and see which ones are correct So let's start with a decomposition where we take we're gonna decompose student into S and S and in S we'll put the social_security_number name address let_me abbreviate a little_binary_digit here and the high_school_code but no more high_school information the GPA and the priority And then in relation we'll put the high_school_code and we'll put the high_school name and the high_school_city So you can see what I've done done here is I've separated out the high_school information into a separate relation So first of all is this a correct decomposition in the sense that A union B equals C Certainly all of the attributes are still present and furthermore if you think_about it and we'll formalize this concept later S join S and that's going to occur by the way based_on this highest school_code value S join S in this case will for the data that you would expect equal student Again we'll formalize that momentarily Now let's look_at a second possible decomposition of the student relation again into two relations S and S In the first one we'll put the first bunch of attributes So we'll put the social_security_number the student's name their address their high_school_code Let's say high_school name and high_school_city And then in the second relation we'll put again the student name We'll put the high_school name and we'll put say the GPA and lastly the priority So again is this a decomposition Well certainly again we have the case that the A union B equals C in other_words we've captured all of the attributes of the student relation in our decomposed relation and do we think it's the case that if we join S and S then we'll get the student relation back and I'll put a question mark here and you know of course the answer is going to be no When we join back we'll be joining in this case on the student name here and the high_school name and likely these are not unique values so when we join back we may be getting information together that doesn't really belong together And again we'll be formalizing that and seeing additional examples momentarily So now let's dig a little further into the actual process of decomposition So first of all we definitely want good decomposition So as we saw a good decomposition must capture all of the attributes of course But the more important property is that this reassembly by the join produces the original relation Sometimes that's called by the way a lossless join property But the second thing that we want is not only that we have a good decomposition but that the relations that we decompose into are good relations And those relations are going to be the ones that are in Boyce_Codd_Normal_Form So let_me first define formally Boyce_Codd_Normal_Form and then we'll go_back to figure_out an algorithm for automatically decomposing relations using good decompositions into decomposed_relations that are in Boyce_Codd_Normal_Form So here's the formal definition of when a relation is in Boyce_Codd_Normal_Form usually abbreviated B C and F A relation R with functional_dependencies is in Boyce_Codd_Normal_Form if every functional_dependencies is such that it's left_hand_side is a key ok Let's see what_happens when it's not the case that the left_hand_side of a functional_dependency is not the key and we'll see why that's a bad design So here's our relation R and here's a set attributes A on the left_side of the functional_dependency attribute B and the rest And let's just put in some values So let's_suppose that we have two tuples here with the same A value Then by our functional_dependency we're going to have the same B value and the rest can be anything What has happened here is that we've captured the piece of information the connection between A and B twice And the reason that's allowed to happen is because A is not a key if A were a key we would not be allowed to have the situation where we have these two tuples both present in the relation So this relation is not in voice cod normal_form And this functional_dependency here is what we would call a B C and F violation That violation is causing us to have redundancy in our relation and that also give_us as we've_seen the update anomalies and deletion anomalies Let_me clarify a little_binary_digit the requirement that the left_hand_side of functional_dependencies have to be key so that's what tells_us we're in Boyce_Codd_normal_form Now I'm not saying that the left_hand_side of every functional_dependency has to be declared as the primary_key for a relation only that it is in fact a key And as you might recall the definition of a key is an attribute that determines all other attributes if you're thinking about functional_dependencies or if you don't have any duplicates in your relation then a key is a value that is never duplicated across tacets So if you think_about it for a second you'll realize that whenever a set of attributes is a key so is any superset of those attributes So if A is a key then so is ac and so is a c d and so on So sometimes you'll see in the definition of Boyce_Codd_normal_form this wording not is a key but will be contains a key which in fact is exactly the same thing or sometimes it will even say is a super key and a super key is a key or a super set of a key Again all of those are saying exactly the same thing but I just wanted to clarify because different wording and sometimes different notation is used for that concept So far things have been pretty abstract Let's try to get a binary_digit more concrete here Let's look_at two examples and determine if those examples are in B C and F Remember to determine is something is in B C and F we need the relational schema and a set of functional_dependencies So here we have our student relation and this is a set of functional_dependencies we had in earlier examples where the social_security_number is determining the name address and GPA That_means that if there's two tuples with the same social_security_number they will have the same name address and GPA That's the same student and they only live in one place The GPA determines the priority so any two students with the same GPA will have the same priority and finally the high_school_code determines the high_school name and city So the high_school is a unique identifier for a particular high_school in a city So those are our three functional_dependencies in order to test whether this is relation is in normal_form with respect to the functional_dependencies we need to know what the key of the relation is or the set of keys of the relation and we worked on this in an earlier video using the closure idea so I'll just remind_you now that for this relation this set of functional_dependencies there's one key or one minimal key and that's the social_security_number together with the high_school_code those two attributes do functionally_determine all other attributes in the relation and therefore they are together forming a key So now to check if we're in Boyce_Codd_Normal_Form we have to ask the question Does every functional_dependency have a key on its left_hand_side and the answer of course is no not all In fact the reality is that no functional_dependency in this case has the key on the left_hand_side We have three left_hand sides and no of them have or contain our one key If you've given any thought at all to this database design you will see that it's not a good one It's combining too much information in one place which is our basic_idea that we start with a mega relation and break it down And so what we're going to do is use these functional_dependencies and specifically the fact that those are Boyce_Codd_normal_form or Boyce_Codd_Normal_Form violations to break this relation down into one that is a better design Now let's look_at a second example our apply relation to see if this one is in Boyce_Codd_Normal_Form So in this case as a reminder we have social_security_number college state date and major So the date is the date of application the major is major the student is applying for at that particular college and we'll have one functional_dependency which effectively says in English that each student may apply to each college only once and for one major Now let's compute the key for this relation or keys for this relation based_on the functional_dependency Well it's pretty straightforward that these three attributes form a key because they determine the other attributes in the relation and therefore they determine all the attributes of the relation Furthermore we can see that our one and only functional_dependency obviously has a key on its left_hand_side and so so this relation is in fact already in Boyce_Codd_normal_form and we'll see there's_no way to decompose this relation further into a better design So here we are again talking_about our decomposition process so now we know what good relations are They are in Boyce_Codd_normal_form And we saw earlier what good decompositions are so now we're going to give an algorithm that's going to perform good decompositions and those decompositions are going to yield decomposed_relations that are in Boyce_Codd_normal_form So here's the algorithm The input is a relation r and the set of functional_dependencies for that relation and the output is going to be our good decomposition into good relations So let's just go through it step by step The first thing we're going to do is compute keys from r and were going to do that using functional_dependencies as we have seen and we saw in the actual algorithm for doing that in a previous_video Then we are going to start doing the decomposition process and we're going to repeat the decomposition until all of the relations are in Boyce_Codd_normal_form though we're going to take r and break it up into smaller relations and we might further break those into smaller relations and so on until every relation is good So the breaking down process says pick a relation that's bad So we're going to pick a relation r prime in our current set and again we're starting with only r in our set And we're going define a situation where that relation has a functional_dependency and I guess technically speaking this would be with lines on top that violates Boyce_Codd_Normal_Form and that violation is what's going to guide us to do the decomposition into better relations So our decomposition then in the second step is going take our prime and and it's going to put the attributes involved in the functional_dependency into one relation and then it's going to keep the left_hand_side of the functional_dependency and put the rest of the attributes along with that left_hand_side into a separate relation So let's draw this as a picture So here's our relation r prime and of course the first time through our loop that relation would have to be r self and then because we have a functional_dependency from A to B and A is not a key that means it's Boyce_Codd_normal_form violation we're going to decompose this into R and R So R will contain the attributes in the functional_dependency R will contain the left_hand_side of the functional_dependency and the rest We can see clearly that this is a decomposition that keeps all attributes and what we'll see soon is that this is also a good one in that logically the join of these two relations is guaranteed to give_us back what we had originally Now the remaining two lines of the algorithm compute after the decomposition the set of functional_dependencies for the decomposed_relations and then the keys for those I'm going to come_back to this particular line here after doing an example This is our same example although squished to give me more space to write And as a reminder in this example we've computed a couple of the times that the key given the functional_dependencies is the combination of the social_security_number and the high_school_code So our goal is to take the student relation and iteratively break it down into smaller relations until all of the relations are in Boyce_Codd_normal_form So let's start the iterative process We pick some functional_dependency that violates Boyce_Codd_normal_form and we use it guide our decomposition So all three of these functional_dependencies actually violate Boyce_Codd_normal_form because none of them have a key on the left_hand_side So let's start with the high_school_code one So to do the decomposition based_on this violating_functional_dependency we're going to create two relations The first one is going to contain just the three attributes that are in the functional_dependency itself so it's high_school_code high_school name and high_school_city And the second one is going to have all remaining_attributes in the relation plus the left_hand_side so we'll have the social_security_number the name the address We will have the high_school_code because it's on the left_hand_side of the functional_dependency we're using but we won't have the name and city from the right side hand_side and I'll have a GPA and the priority Now at this point our algorithm computes the functional_dependencies for the decomposed those relations For this particular example we can just see what they are they're the same functional_dependencies that we had for the non decomposed relation Sometimes there's a little_binary_digit of computation you have to do and I'm going to talk_about that in a binary_digit But in this case we can see for example for our relation S the only functional_dependency is this functional_dependency here That tells_us that high_school_code here is a key for S So our only functional_dependency has a key on the left_hand_side and that tells_us that this relation is in Boyce_Codd_normal_form So we're done with that one but we're not done with S So for S the key is still the social_security_number and high_school_code together so we still have these two functional_dependencies that are Boyce_Codd_normal_form violations So let's take the GPA priority one and let's guide that to decompose S further We'll decompose S into S and S S will contain the GPA and priority from the functional_dependency we're using and then S will take the remaining_attributes in S together with the left_hand_side of the GPA So we'll keep our social_security_number name address high_school_code and GPA but we don't keep the priority So at this point S is completely gone and let's take a look_at the remaining relations S now just has one functional_dependency that applies and the left_hand_side is now a key And so now we're done with S It's in Boyce_Codd_Normal_Form but we're not done I'm_sorry we're done with S but we're not done yet with S S still has social_security and high_school_code as its key and so we still have a violating_functional_dependency so let's decompose S further We decompose into S and S S contains the attributes in the functional_dependency that we're using now so it's the social_security_number name address and GPA and then as contains the remaining_attributes plus the left_hand_side so that's the social_security_number and the high_school_code And I will just tell you right now because you might be getting bored with this example we're done with S S and S are now both in Boyce_Codd_normal_form So this is our final schema It contains relations S with the information_about high_schools S with the information_about GPA's and priorities S has student with their name address and GPA and S a student with the high_school they went to And if you think_about it this really is a good schema design and it's what's produced automatically by the Boyce_Codd_normal_form decomposition algorithm using our functional_dependencies Let_me mention a few more things about the algorithm First of all I left this step kind of mysterious which is how we compute the functional_dependencies for the decomposed_relations We can't just take the functional_dependencies we already have for the bigger relation and throw away the ones that don't apply exclusively to one or the other of the decomposed we actually have to compute the functional_dependencies that are implied and that apply to these relations So in the video on functional_dependencies we learned all of implied functional_dependencies and we would use the closure as we did in that video to determine the implied functional_dependencies Now the reality is for many examples it's pretty obvious we saw in the previous example it is And the other thing is that this is being done by a computer so we don't actually have to worry_about it except when we're doing exercises for our class Second let_me_mention that there is little nondeterminism in this algorithm It says pick any of our relations with a violating_functional_dependency and use that to guide the next decomposition step So the fact is that you can get a different answer depending on which one you choose at this point in time All of the results will be Boyce_Codd_normal_form decomposition but they might not be the same one And in fact if you go_back to the example that I did and you pick the functional_dependencies in a different order you might get a different final schema But again it will be in Boyce_Codd_normal_form And lastly some presentations of the Boyce_Codd_normal_form decomposition algorithm actually have another step here which is to extend the functional_dependency that is used for the decomposition So we're using A to B but if we have A to B we also have A to B and we can add more attributes those in the closure of A If you remember the closure and that's also a correct functional_dependency By doing this extension we will still get a correct Boyce_Codd_normal_form answer but we'll tend to get relations that are larger than the ones we get if we don't do the extension first In some cases larger relations are better because you don't need to join them back when you're doing queries but that can depend on the query load on the data base So to conclude does Boyce_Codd_normal_form guarantee a good decomposition Well of course the answer is yes or I wouldn't have spent all this time teaching you about it Does it remove the anomalies that we looked at in our first example in another video of bad relational_design Yes it does remove anomalies When we have multiple instances of the same piece of information being captured that's what's squeezed out by the decomposition into Boyce_Codd_normal_form and that's fairly easy to see through the examples that we've done already It's a little less obvious seeing why Boyce_Codd_normal_form composition does allow_us to have a breakdown of a relation that logically reconstructs the original relation So let's look_at that a little_binary_digit more So we're taking a relation in R we're producing R and R and we want to guarantee that when we join R and R we get R back We don't get too few tuples and we don't get too many tuples Too few is pretty easy to see If we break R into R and R and their projections of R then when we join them back certainly all the data is still present If they're too many tuples that's a little_binary_digit more_complicated to see Let's just use a simple abstract example So here's a relation R with three attributes Let's have two tuples and Let's_suppose that we decompose R into R and R R is going to contain AB and R is going to contain BC So let's fill in the data in R we have And in R we have Now let's see what_happens when we join R and R back together When we do that you might see what's going to happen We're actually going to get four tuples We're going to get and That is not same as our original relation so what_happened Well what_happened is we didn't decompose based_on a functional_dependency The only way we would decompose with B as the shared attribute were if have a functional_dependency from B to A or B to C And we don't In both cases there's two values of B that are the same where here the A values are not the same and here the C values are not the same So neither of these functional_dependencies hold N B C and F would not perform this decompostion So in fact B C and F only performs decompositions when we will get the property that they're joined back Again that's called a lossless join So B C and F seems great we just list all our attributes a few functional_dependencies and the systems churns on and out comes a wonderful schema And in general that actually is what_happens In our student example that worked very well There are however shortcomings of B C and F and we will discuss those in a later video This video continues the topic of relational_design talking specifically about multivalued_dependencies and Fourth_Normal_Form I know I've reminded you many_times about relational designed by decomposition so let_me do it very quickly The designer specifies mega_relations with all the information they want to store and properties of the data The system decomposes the mega_relations into smaller relations that have good properties no anomalies and no lost information When we have functional_dependencies as are properties of the data we get Boyce_Codd Normal_form and then when we add to the functional_dependencies multi_value_dependencies we get fourth_normal_form And the specification of multi_value_dependencies and decomposition into Fourth_Normal_Form is the topic of this video As a reminder from earlier Fourth_Normal_Form is stronger than Boyce_Codd_Normal_Form so if we have here all of the relations that are in Boyce_Codd_normal_form some subset of those are also in fourth_normal_form When we have functional_dependencies we can guarantee Boyce_Codd_normal_form and then when we add multi_value_dependencies that's what allows_us to narrow down to the stronger property of fourth_normal_form So let's start with an example We have information_about students_applying to colleges The student is identified by their social_security_number They may apply to several colleges and in this video we're not going to have college states just college names We'll assume they're unique And then the student may have hobbies And they may apply as I've said to several colleges and have several hobbies but let's assume for now those are independent So do we have any functional_dependencies for this relation Actually we don't have any all The social_security_number does not determine the college name or the hobby or anything in the other direction With no functional_dependencies the only key for the relation then is all attributes of the relation So is this relation in Boyce_Codd_Normal_Form As you might remember from the previous_video Boyce_Codd_Normal_Form says all functional_dependencies have a key on the left_hand_side Well since we have no functional_dependencies then the answer is yes It is in Boyce_Codd_normal_form On the other_hand do we think this is a good_design I'm going to say no this is not a good_design Why not Well let's_suppose that somebody applies to five colleges and they have say six hobbies Then to have all combinations of colleges and hobbies that would yield tuples in the relation and clearly that's not a good idea We'd rather separate the college and hobbies if they are independent So the separation of independent facts is what fourth_normal_form is about And now let's get a little_binary_digit more formal Like functional_dependencies multivalued_dependencies are specified based_on knowledge of the real_world constraints on the data being captured and all instances of a relation with a multivalued_dependency must adhere to the dependency Now let's define exactly what a multi_value_dependency is For relation R we write a multi_value_dependency using a double headed arrow and we say 'A' multi_determines 'B' In this case again with 'A' and 'B' possibly being sets of attributes so that would be A one through A N and B one through B M which I'm abbreviating with A_bar and B bar So let_me write the formal definition of A_multi_determines B Again using first order logic similarly to what we did with functional_dependencies but this one's a binary_digit more_complicated It says for all tuples T and U that are in relation R if T with the attributes A of T equal U for the attributes A of U Again these are lists of attributes So if the two tuples agree on their A values then and remember for functional_dependencies it was simple we just said they agreed on their B values But now it gets more_complicated We're going to say that there_exists a third tuple V in R that has the following properties V has the same A values as T and U So V sub A equals T sub A furthermore V has its B value okay drawn from T so it's equal there And finally it has its rest so those are all the attributes other than A and B equal to U rest Okay so that's a mouthful but let's look_at that pictorially So here's our relation R and we'll have the set of attributes A the set of attributes B and the rest of the attributes And now let's make some tuples So let's say that this is tuple T and this is tuple U And we said that T and U agree on their A values So they have the same A values and then they don't have to have the same B values So we'll call the first one B and the second one B and then for the rest we'll call this R and R So what the multi_value_dependency says is that we have a third tuple V and V again has the same A and it has its B value from tuple T So it has B but it has its rest value from tuple U so then it has R here So again what we're saying is that if we have these first two tuples T and U then we also have tuple V Now let_me do something a little tricky Let_me swap the roles of T and U and show that we also with this definition are guaranteed to have a fourth tuple and we'll call that fourth tuple W By swapping the roles of T and U W has again the same A value Now it will take its B value from U and that will give_us B and we'll take its rest value from T and that gives_us R So what we can see here is that when we have the first two tuples that have this particular combination of B values and rest values it tells_us we must have the other combinations as_well We must have B with R and B with R What it's really saying is those B values and the rest values are independent of each other and we'll have all combinations So that might get you thinking back to our colleges and hobbies Incidentally sometimes multi_value_dependencies are called tuple generating dependencies And that's because the definition is is about having additional tuples when you have some existing tuples unlike functional_dependencies which just talk_about the relationships among existing tuples So let's go_back to our example Now we have students_applying to colleges and having hobbies Those are independent facts about the student We'll write our multi_value_dependency as 'social security_number multi determine C name' and now lets use some example data to see our definition and how it works here Here's our apply relation with the social_security_number the college name and the hobby Let's_suppose that we have a student who's applied to Stanford and plays the trumpet Now let's_suppose that same student has applied to Berkeley and plays tennis So what our multivalued_dependency says and let's make this tuple T and tuple U is that there's a further tuple V V takes the same social_security_number and it takes the first value for the college name and the second for the hobby It says if we have a playing trumpet at Stanford and tennis at Berkeley then that same person will be playing tennis at Stanford Furthermore I show that the same definition will generate automatically A fourth tuple with the other combination which would be Berkeley and Trumpet By the way one thing you might notice here is that we also have the multivalued_dependency social_security_number multi_determines hobby This is actually one of the rules for multivalued_dependency saying that when you have A_determines B then you A multidetermines B then you also have A_multi_determines rest and we'll see some rules for multivalued_dependencies later Let's look quickly at a modification of our example where the real_world assumptions about the data are different So we still have exactly the same relation with the same attributes But let's_suppose that we don't want to reveal every hobby to every college Maybe we'll decide that we don't want Stanford to know that we're a surfer or Berkeley to know that we're on the speech and debate team So if that's the case then what multivalued_dependencies do we have in this relation We actually have none And we don't have any functional_dependencies either by the way And is this a good_design Well actually I would argue yes In this case this design is a good one because we're not going to have that multiplicative effect of information Every tuple that we have in the applied relation will be an independent piece of important information Let's look_at one more example before we go on to talk_about properties of multivalued_dependencies I've extended the apply relation now to not only include colleges and hobbies but also the date of application to a college and the major or majors that are being applied for Let's continue to assume that hobbies are revealed to college selectively We don't need to have same hobbies for each college that a student applies to Secondly lets assume that we restrict students to apply only once to each college but what I what we mean by that is just on one day A student can still apply to multiple majors at a single college and to different majors at different colleges Let's also assume that majors are independent of hobbies which seems to make_sense It takes some thinking to come up with the right functional and multivalued_dependencies to capture these constraints but here they are The first one when we say that we reveal hobbies to college selectively is actually the absence of a multivalued_dependency on hobbies and colleges The second one says as we apply once to each college or on one particular day to each college so that would say that when we have a particular student and a particular college that always going to have the same date so any two tuples for a student and college combination will be on the same date The last dependency that we will have involves the independence of the majors that are being applied for and the hobbies that a student has so we'll write that as the multivalue_dependency social_security_number plus college name plus date multidetermines major and remember what that's saying is that major for a given student college and date the majors that they apply for are independent of what we call the rest which in this case is the hobbies So you might take some time to look_at the formal definitions of functional_dependencies multivalue dependencies and maybe write out some sample data to convince yourself that these are the dependencies that are capturing the assumptions that we make about the real_world Like with functional_dependencies we have a notion of trivial dependency those that always hold we also have some rule for multi_valued dependencies The definition for a trivial multi_valued dependency A_multi_determines B is in this case that either B is a subset of A or A union B are all attributes a multi_value_dependency is non_trivial if that's not the case So let's take the look_at why these multi_value_dependencies are trivial So let's start with the first case where we have our attributes A and the rest and then attributes B are a subset of A so lets say that these are attributes B So what are definition of multi_valued dependencies says that when we have the same values for A in two tuples so here A and A then we have every combination of the B values and the rest well obviously we do since the B's are subsets of the A's here the B values are going to be the same as_well and we clearly have every combination For the other case of trivial multi_value_dependencies We have A and B together being all attributes of the relation so in that case there's_no rest so clearly we have every combination of values of A and B and rest because there's_no rest to combine with Like with functional_dependencies there are a whole_bunch of rules that hold for multi_valued dependencies We will just talk_about three of them and the first one is the most_important and interesting and that's the rule that says if we have a functional_dependency from A to B then we also have a multi_valued dependency from A to B And I'm gonna go_ahead and prove that rule for you again because this is an important one I'm going do this proof using a template for the relation similar to the what I did with rules for functional_dependencies So let's say we have our A attributes our B attributes and our rest and what we need to prove to prove the multi_value_dependencies is when there are tuples T and U with a certain form there_exists a tuple V of another form So let's fill in some values first for the tuples So Let's say that we have A and A here that's what we need for the equality of the A values Then we have B and R and we have B and R and in order to prove this multi_value_dependency I need to prove that there_exists a tuple V that has the same A value that it has B from tuple T and R from Tuple U and what I have in order to prove that is the fact that we have a functional_dependency from A to B Because we have the functional_dependencies and because T and U have the same A value What that tells_us is that B equals B here And so if B equals B then we know that this value B here is equivalent to B and in order to prove the existence of this tuple well we have that tuple here already and we're done So you might check that again but what that says is using the knowledge of a functional_dependency we can prove that we always have a corresponding multivalued_dependency there are a couple more rules for multivalued_dependencies that you can prove for yourself if you're so inclined The first one is the intersection rule it says that if we have A_multi_determines B and A_multi_determines C then we have A_multi_determines B intersects C The transitive rule is slightly different than from exact transitivity What it says is if we have A multi determine B and we have B multi_determines C then we have A multi determined not C exactly but C minus B And you might work some examples because it yourself why we don't have just A_multi_determines B and to subtract the attributes for B although it's fairly_complicated So again these rules can be proven and there are many other rules of multivalued_dependencies that you can read about in any of the readings provided on our website By the way regarding rules let's come_back to the fact that every functional_dependency is a multivalued_dependency So we can use another Venn diagram This is different than our previous one We can list all of our multivalued_dependencies here and the functional_dependencies are a subset of those so what that tells_us is if we ever have a rule that applies for multivalued_dependencies here that will cover the entire Ven diagram and so that rule will apply for functional_dependencies as_well So every rule for MVDs is also a rule for functional_dependencies On the other_hand if we have a rule that applies for functional_dependencies that rule does not necessarily have to apply all multivalued_dependencies because it might be specialized just for this portion of the Venn diagram So an example of such a rule is the splitting_rule The splitting_rule is a rule that applies to functional_dependencies but does not always apply to multivalued_dependencies And again you could work an example to convince yourself of that fact Woo So after all that set_up of multivalue dependencies we're finally ready to talk_about fourth_normal_form The definition of fourth_normal_form looks very similar to the one for Boyce_Codd_normal_form Says we take a relation and we take now a set of multivalued_dependencies for that relation and the relation is in fourth_normal_form if every non_trivial multivalued_dependency has on it's left_hand_side a key Remember for functional_dependencies it looks exactly the same except we have the functional_dependency all here instead of multivalued_dependencies So let's see exactly what fourth_normal_form telling us and why it's a good thing So we have A B and rest as usual and let's_suppose that we have a non_trivial multivalued_dependency So that's telling us that if we have tuples T and U and we'll put in some values for B and the rest then we're going to have the combination of those as_well So that's kind of the proliferation of tuples we get when we squish independent facts in the same relation But if the left_side is a key so if the A attributes are a key here then we won't have those tuples and will never have to worry_about the proliferation Now remember that I said fourth_normal_form implies Boyce_Codd_Normal_Form Or if you prefer it in Venn diagram format Fourth_Normal_Form is stronger than Boyce_Codd_Normal_Form and let's see why that's the case If we have a fourth_normal_form and we want to show that we're in Boyce_Codd_normal_form then we have to show that if we have a functional_dependency then the left_hand_side A is a key That would tell_us we're in Boyce_Codd_normal_form Well if we have a functional_dependency we had a rule that tells_us we also have the multivalued_dependency and then since we're in fourth_normal_form we get that A as a key So again fourth_normal_form implies Boyce_Codd_normal_form Now let's take a look_at the decomposition of algorithm into fourth_normal_form It's extremely similar to the Boyce_Codd_normal_form decomposition algorithm The input is a relation A set of functional_dependencies and multi_value_dependencies and we need to separate them because we use the functional_dependencies to find keys The output is a decomposition of R into good relations in this case fourth_normal_form and it's a good decomposition in the sense that reassembling the relations gives you back the original As with Boyce_Codd_normal_form we start_by computing keys using the functional_dependencies and then we repeat the decomposition process until all of our relations are in fourth_normal_form Just as with functional_dependencies in Boyce_Codd_normal_form we pick a relation that has a violating dependency in this case a multi_value_dependency and we split the relation based_on that dependency So we create one relation that has the attributes of the dependency and another relation that has the left_hand_side of the dependency and the rest of the attributes After that we need to compute the functional_dependencies for the decomposed relation and the multi_value_dependencies for it and then we can compute the keys Now finding these multi_value_dependencies is actually a fairly complex process Usually it's very intuitive but I'm going to refer you to the readings to read about the algorithm itself And in fact it can be so complicated in the general case that some of the readings don't even provide the algorithm But again in general it's very intuitive Our first example is going to be very fast to do As you remember this example has one multi_value_dependency social_security_number determines college name multi_determines college name and it has no keys other than all of the attributes So obviously this is a violating multi_value_dependency and so we decompose into two relations we'll call them A and A The first one has the attributes of the multivalue_dependency the social_security_number and the college name and the second one has the left_hand_side multivalued_dependency as_well as all the remaining_attributes which in this case is the hobby These two decomposed_relations actually have no FDs and no MVDs so in that case we're definitely in th normal_form and we're done with the decomposition and I think we can agree that this looks_like a good schema for the data at hand Our second example is quite a binary_digit more_complicated Remember in this example we have that the social_security_number and college name functionally_determine date That_means we have each student applies to each college on a specific date And secondly we assume that majors that were being applied for were independent of hobbies So we have social_security_number college name and date multi_determines the major And incidentally that would mean it multi_determines the hobby too Once again we have no keys for the relation except for all attributes So we have both a violating_functional_dependency in this case and we have a violating multivalue_dependency Let's use the multivalue_dependency for our first decomposition step So we'll create A and A A will contain all the attributes of our multivalued_dependency And then A will contain all the remaining_attributes along with the left_hand_side of our multivalue_dependency And that turns_out to be all of the attributes except the major Now let's look_at our decomposed_relations and see what we have in terms of functional_dependencies and multi_value_dependencies for them So after the decomposition we don't have any more multivalued_dependency but our functional_dependency actually applies to both of the decomposed_relations and we still don't have a key on the left_hand_side So we need to decompose further based_on the first functional_dependency So let's start_by decomposing A We'll turn A into A and A and A will have the functional_dependency all three attributes And then A will have the left_side of the functional_dependency and the remaining_attributes which in this case is the major So now we're finished with A and we have a similar problem with A And so we decompose A similarly although we'll discover that A is the same relation in the decomposition of A as we got with A So we actually only add one more relation now which is A That contains the social_security_number and the college name from the left_side of the functional_dependency and the hobby which is the remaining attribute And then we cross out A Now the only functional_dependencies are multi_value_dependencies we have left do have a key on the left_hand_side I'll let you verify that for yourself And these three relations are our final decomposition into th normal_form And I think you will agree that this is a good_design again for the data at hand So let's wrap_up this long unit on on dependencies and normal_forms with a quick summary If we have a relation RABC a functional_dependency from A to B tells_us that when we have the same A values we have the same B values and the Boyce_Codd_normal_form tells_us to factor that those attributes into their own relation so that we don't repeat that relationship over and over For multi_value_dependencies let's say we have the relation RABCD and if we have the multi_value_dependency A_multi_determines B what that tells_us is that we have every combination for a given A of B values and Compact Disc values we called those rest earlier and when we have that multiplicative effect of combinations again we take the A and B attributes and we put them in a separate relation so that we can separate out those facts from the independent fact of A and its Compact Disc values Finally in the design process multi_value_dependencies are something we add to functional_dependencies only they're stronger So fourth_normal_form is a stronger property than Boyce_Codd_normal_form Now usually this design process works very well and is very intuitive for many schemas I_hope for the examples that I gave here But there are actually a few shortcomings to using Boyce_Codd_Normal_Form or Fourth_Normal_Form and we'll cover those in the next_video The last several videos talked_about Boyce_Codd Normal_form and Fourth_Normal form as describing good relations As a remainder Boyce_Codd_normal_form is based_on functional_dependencies and it is says whenever we have functional_dependency on a relation the left_hand_side needs to be a key or contain a key Fourth normal_form uses multi_value_dependencies and says whenever we have a non_trivial multi_value_dependency again the left_hand_side is a key And as a reminder functional_dependencies are also multi_value_dependencies so Fourth normal_form implies Boyce_Codd_normal_form What we can see in this video is a few examples where Boyce_Codd Normal_form or Fourth normal are actually not necessarily desirable for the specific relations we are designing As_usual we'll be using our college application information database although each of our examples will be a binary_digit different For our first example let's take a look_at the relation And let's_suppose that we have the student's Social_Security_number the college that they are applying to the date of the their application and the major they are applying for And further more let's say that each student can apply to each college only once and for one major So that would be on one date for major Furthermore let's_suppose that all of the colleges in our database have non overlapping application dates And this is obviously contrived for the specific example that we want to demonstrate So under these assumptions the functional_dependencies for our relation will be that the social_security_number and the college name determine the date and the major And so that's based_on our first assumption And we have one more functional_dependency which is that the date determines the college name and that's based_on our second assumption So if we have two tuples with the same date they have to be the same college because colleges have non overlapping dates So using these functional_dependencies we can determine that the key for the relation is Social_Security_number and college name together since they determine the other two attributes So is this relation in Boyce_Codd_normal_form The answer is no and the reason is the second functional_dependency which does not have key on its left_hand_side So let's follow the decomposition process and create two relations from our big relation The first one will have the attributes from the violating_functional_dependency And the second one will have all the remaining_attributes plus the left_hand_side of the functional_dependency So that would be the social_security_number the date and the major So did we think this is a good_design It certainly is in Boyce_Codd_normal_form but I would argue it is not really a good_design We do correctly factor out the information connecting the application dates and the colleges On the other_hand we've now separated a student's application the date and the major of their application from the college they're_applying to so even to check this first functional_dependency here would require us to join these two relations together Intuitively we might just prefer to keep that information together even if we are capturing some information multiple_times So I'd say not necessarily good_design here And there is another normal_form it's called third_normal_form and I'm not going to define it here but third_normal_form actually allows this relation here to stay as is without being decomposed and you can read about third_normal_form in any of the readings mentioned on the website Just to put third_normal_form in context we'll go_back to our Venn diagram Remember that fourth_normal_form is stronger than Boyce_Codd_Normal_Form So everything that's in fourth_normal_form is also in Boyce_Codd_Normal_Form and third_normal_form is even weaker than Boyce_Codd_Normal_Form So if we have a relation that's in third_normal_form it's not necessarily in Boyce_Codd_Normal_Form and here's an example of one that's not but if we have something in Boyce_Codd_Normal_Form it is in third_normal_form And again you can read about third_normal_form on the readings from the website For our second example lets go_back to our student relation And now we're going to have a student's Social_Security_number the high_schools that they've attended their GPA and their priority for admission And let's assume that students might attend multiple high_schools and that the priority is determined from the GPA So in that case our functional_dependencies in this case are social_security_number determines GPA We're going to assume that students have one GPA We'll have GPA determines_priority and by the way we also have using the transitivity rule social_security_number determines_priority The key for this relation is the combination of social_security_number and high_school name Because students could go to multiple high_schools but once we have the social_security_number we do functionally_determine through these functional_dependencies the remaining_attributes the priority and the GPA So is this relation in Boyce_Codd_normal_form No it's clearly not And in fact all three of our functional_dependencies violate Boyce_Codd_normal_form So let's go_ahead and do the decomposition So we'll decompose into S and S Let's start with the social_security_number determines_priority So then we'll have social_security_number and priority in S And in the other one we'll have all the remaining_attributes and the left_hand_side high_school name and that would be the GPA Then the second one still needs to be decomposed further so that one will become S and S S will have the social_security_number and the GPA based_on the first functional_dependency and then S will have the social_security_number and the high_school name And then we take away S Now do we think this is a good_design Again I would say not necessarily It doesn't quite feel right for one thing we've lost the GPA to priority functional_dependency We can join the two relations S and S to check that dependency but it would be nice if that dependency were together in a relation and in fact in this case we can have a Boyce_Codd_normal_form relation that does maintain that dependency So if we had made S have GPA and priority together then we wouldn't have needed S and we're actually still in Boyce_Codd_normal_form So that would would have happened if we had started with the functional_dependency which is also implied that was SSN to GPA and priority Actually way back in the decomposition algorithm I_mentioned that sometimes when people use that algorithm they try to start with the biggest functional_dependencies they can to get a better end design And this is a case where we would want to do that if we prefer to end up with a design with just these two relations So overall with the first two relations that illustrated when you have Boyce_Codd Normal_form and Fourth_Normal form it's possible that after the decomposition there's_no guarantee that all of our original dependencies can be checked on the decomposed_relations They may require joins of those relations in order to check them Now lets look_at a different type of example Let's_suppose that we have a relation that contains the scores of students_who are applying to college so we'll have the student's Social_Security numbers for and their name And then SAT scores and ACT scores and they might have many SAT's and ACT's So the only functional_dependency we'd have in that case is the one that says social_security_number determines the student name and as far as keys go there's_no key Now in this example we do have a multi_valued dependency If a student has several SAT and ACT scores we can assume they're independent of each other And so that's captured in the multi_valued dependency that says for a given student identified by their social_security_number and name if they have a set of SAT scores those SAT scores are independent of the rest of the attributes You remember that from the MVD video which in this case would be ACT In that case we'll have for each student every combination of their SAT and ACT scores So is this relations in fourth_normal_form Well clearly it's not we have a violating multi_value_dependency here since it doesn't have a key on the left_hand_side and we also have a violating_functional_dependency So let's quickly do the decomposition We'll start with the multi_value_dependency so that would give_us in the first relation the attributes of the multi_value_dependency and then it will give_us the attributes that are remaining plus the ones on the left_hand_side So social_security_number student name and ACT So now we're done with the multivalue_dependency Actually in this case because it's now a trivial multivalue_dependency when it covers all attributes and now let's take a look_at the functional_dependency that's still a violation because the social_security_number is not a key on the left_hand_side for either of these relations So we'll split S into S and S S will continue the Social_Security_number and student name and S will contain the Social_Security_number and the SAT And actually similarly need to split S And that will give S which contains the Social_Security_number and the ACT score And so now regard of S and S An just in case you're getting confused at this point this is our final schema And this schema is in fourth_normal_form And that's good we've separated out all the relevant facts So actually it feels like a pretty good schema But let's_suppose now that all of the the queries that we're going to run on these relations Take a social_security_number and what they return is the student's name and some composite score that's based_on all of their SAT and ACT scores and again let's_suppose every single query does that If that's the case then every query is going to have to recombine all of these three relations in order to compute it's results So we might have been better off even if it included some redundancy and some additional storage with our original design because each query again is going to access all of the data in that one relation So there is something called a de normalize relation And when queries tend to access all attributes when every query would be reassembling the normalized relations it actually might be preferable to use one that is quote de normalized And here's our final example This time let's talk_about college information So we have a college and the state it's in Let's_suppose we have some other information and we decide to put it in separate relations We have the size of the college the number of students in the college and we might have the average SAT score for each college the average_GPA for each college and maybe some additional relations each containing one fact about the college So all of these relations in Boyce_Codd Normal_form and First Normal_form yes they are actually for each relation we have a functional_dependency from the college name to the other attribute but the left_side will be a key so were in a good shape Is this a good_design once again I say not necessarily we may not want to decompose so much I sort of feel like this relation is too decomposed because we can capture all of the information in one relation or a couple of relations still being in Boyce_Codd_normal_form So one of the problems I actually like to give in my database class is to create a theory of composition that's sort of a compliment to the theory of decomposition figuring out when you can take multiple relations and combine them while still staying in a desirable normal_form So you might give that a thought So in conclusion when designing a database schema there are often many different designs that are possible some of the designs are much better than others and we have to have a some way of choosing the design we want to use for our application We do have a very nice theory for relational database design that we've gone through in the past several videos We define normal_forms that tell_us when we have good relations We have a process for designing by decomposition and it's usually quite intuitive and works well There are however some shortcomings to the normal_forms that we've defined and in this video we saw some specific shortcomings involving and forcing dependencies in decomposed_relations about considering what the query workload might look like when one does one's database design and finally the possibility of over decomposing one's relations unnecessarily Now let's turn to the subject of querying eXtensible_Markup_Language First of all let_me say right up_front that querying eXtensible_Markup_Language is not nearly as mature as querying relational data bases And there is a couple of reasons for that First of all it's just much much newer Second of all it's not quite as clean there's_no underlying algebra for eXtensible_Markup_Language that's similar to the relational_algebra for querying relational data bases Let's talk_about the sequence of development of query languages for eXtensible_Markup_Language up until the present time The first language to be developed was XPath XPath consists of path_expressions and conditions and that's what we'll be covering in this video once we finish the introductory material The next thing to be developed was eXtensible_Stylesheet_Language_Transformations eXtensible_Stylesheet_Language_Transformations has XPath as a component but it also has transformations and that's what the T stands for and it also has constructs for output formatting As I've mentioned before eXtensible_Stylesheet_Language_Transformations is often used to translate eXtensible_Markup_Language into Hypertext_Markup_Language for rendering And finally the latest language and the most expressive language is XQuery So that also has XPath as a component plus what I would call a full featured query language So it's most similar to Structured_Query_Language in a way as we'll be seeing The order that we're going to cover them in is first XPath and then actually second XQuery and finally eXtensible_Stylesheet_Language_Transformations There are a couple of other languages XLink and XPointer Those languages are for specifying as you can see links and pointers They also use the XPath language as a component We won't be covering those in this video Now we'll be covering XPath XQuery and eXtensible_Stylesheet_Language_Transformations in moderate detail We're not going to cover every single construct of the languages but we will be covering enough to write a wide variety of queries using those languages To understand how XPath works it's good to think of the eXtensible_Markup_Language as a tree So I'd like you to bear with me for a moment while I write a little_binary_digit of a tree that would be the tree encoding of the book store data that we've been working with So we would write as our root the book store element and then we'll have sub_elements that would contain the books that are the sub_elements of our bookstore We might have another book We might have over here a magazine and within the books then we had as you might remember some attributes and some sub_elements We had for example the ISBN_number I'll write as an attribute here We had a price and we also had of course the title of the book and we had the author excuse me over here I'm obviously not going to be filling in the subelement structure here we are just going to look_at one book as an example The ISBN_number we now are at the leaf of the tree so we could have a string value here to denote the leaf maybe for the price for the title A First Course in Database Systems then our authors had further sub_elements We had maybe two authors' sub_elements here I'm abbreviating a binary_digit below here a first name and a last_name again abbreviating so that might have been Jeff Ullman and so on I think you get the idea of how we render our X and L as a tree And the reason we're doing that is so that we can think of the expressions we have in XPath as navigations down the tree Specifically what eXtensible_Markup_Language consists of is path_expressions that describe navigation down and sometimes across and up a tree And then we also have conditions that we evaluate to pick out the components of the eXtensible_Markup_Language that we're interested in So let_me just go through a few of the basic constructs that we have in XPath Let_me just erase a few of these things here that got in my way Okay I'm gonna use this little box and I'm gonna put the construct in and then sort of explain how it works So the first construct is simply a slash and the slash is for designating the root_element So we'll put the slash at the beginning of an XPath query to say we want to start at the root A slash is also used as a separator So we're going to write paths that are going to navigate down the tree and we're going to put a ' ' between the elements of the path All of this will become much clearer in the demo So I'll try to go fairly quickly now so we can move to the demo itself The next construct is simply writing the name of an element I put 'x' here but we might for example write 'book' When we write 'book' in an XPath expression we're saying that we want to navigate say we're up here at the bookstore down to the book sub_element as part of our path expression We can also write the special element symbol ' i ' and ' i ' matches anything i i So if we write ' i ' then i we'll match any sub_element of our current_element When we execute XPath there's sort of a notion as we're writing the path_expressions of being at a particular place So we might have navigated from bookstore to book and then we would navigate say further down to title or if we put a ' i ' then we navigate to any sub_element i If we want to match an attribute we write ' ' and then the attribute name So for example if we're at the book and we want to match down to the ISBN_number we'll write ISBN in our query our path expression We saw the single slash for navigating one step down There's also a double_slash construct The double_slash matches any descendant of our current_element So for example if we're here at the book and we write double_slash we'll match the title the authors the off the first name and the last_name every descendant and actually we'll also match ourselves So this symbol here means any descendant including the element where we currently are So now I've given a flavor of how we write path_expressions Again we'll see lots of them in our demo What about conditions If we want to evaluate a condition at the current point in the path we put it in a square_bracket and we write the condition here So for example if we wanted our price to be less_than that would be a condition we could put in square_brackets if we were actually better be the attribute at this point in the navigation Now we shouldn't confuse putting a condition in a square_bracket with putting a number in a square_bracket If we put a number in a square_bracket N for example if I write three that is not a condition but rather it matches the Nth sub_element of the current_element For_example if we were here at authors and we put off square_bracket two then we would match the second off sub_element of the authors There are many many other constructs This just gives the basic flavor of the constructs for creating path_expressions and evaluating conditions XPath also has lots of built in functions I'll just mention two of them as somewhat random examples There's a function that you can use in XPath called contains If you write contains and then you write two expressions each of which has a string value this is actually a predicate will return true if the first string contains the second string As a second example of a function there's a function called name If we write name in a path that returns the tag of the current_element in the path We'll see the use of functions in our demo The last concept that I want to talk_about is what's_known_as navigation axes and there's axes in XPath And what an axis is it's sort of a key word that allows_us to navigate around the eXtensible_Markup_Language tree So for example one axis is called parent You_might have noticed that when we talked_about the basic constructs most of them were about going down a tree If you want to navigate up the tree then you can use the parent access that tells you to go up to the parent There's an access called following_sibling And the colon colon you'll see how that works when we get to the demo The following_sibling says match actually all of the following siblings of the current_element So if we have a tree and we're sitting at this point in the tree then we the following_sibling axis will match all of the siblings that are after the current one in the tree There's an axis called descendants descendants as you might guess matches all the descendants of the current_element Now it's not quite the same as slash_slash because as a reminder slash_slash also matches the current_element as_well as the descendants Actually as it happens there is a navigation access called descendants and self that' s equivalent to slash_slash And by the way there's also one called self that will match the current_element And that may not seem to be useful but well see uses for that for example in conjunction with the name function that we talked_about up here that would give_us the tag of the current_element Just a few details to wrap_up XPath queries technically operate_on and return a sequence of elements That's their formal semantics There is a specification for how eXtensible_Markup_Language documents and eXtensible_Markup_Language streams map to sequences of elements and you'll see that it's quite natural When we run an XPath query sometimes the result can be expressed as eXtensible_Markup_Language but not always But as we'll see again that's fairly natural as_well So this video has given an introduction to XPath We've shown how to think of eXtensible_Markup_Language data as a tree and then XPath as expressions that navigate around the tree and also evaluate conditions We've seen a few of the constructs for path_expressions or conditions We've seen a couple of built in functions and I've introduced the concept of navigation axes But the real way to learn and understand XPath is to run some queries So I urge you to watch the next_video which is a demo of XPath queries over our bookstore data and then try some queries yourself In this video we'll demonstrate XPath by running a number of queries over our bookstore data Let's first take a look_at the data we've expanded it slightly over what we've been using in previous_videos but it continues to have pretty_much the same structure We have a number of books Books have attributes ISBN price sometimes in addition they have a title sub_element authors with first name and last_name So we have our first course book and our complete book And our complete book also has a remark as you may recall Then I've added a couple more books I've added Hector and Jeff's_Database_Hints by Jeffrey_Ullman and Hector Garcia Molina with a remark an indispensible companion to your textbook I've also added Jennifer's Economical Database_Hints for that at a mere price of you get some hints And then finally just to demonstrate certain expressions I've inserted three magazines two National Geographics and a Newsweek And finally a magazine seen called Hector and Jeff's_Database_Hints So with this data in mind let's move to the queries We'll start with simple queries and get more_complicated as we proceed In this window we'll be putting our XPath expressions in the upper pane then we'll execute the query and we'll see the results in the lower pane The way XPath works the first part of every expression specifies the document document over which the XPath expression is to be evaluated So we have the data that you saw in a document called bookstoreq xml and you'll see in each of our expressions that we begin by specifying that document and then move_ahead to the rest of the XPath expression Our first expression is a very_simple path expression It says navigate through the eXtensible_Markup_Language by going first to the root_element called bookstore then look_at all the books so the elements of book store and finally all the titles of elements Let's run the query and we will see our results below So as we can see our results here is actually written in eXtensible_Markup_Language little header appears And then we see the four titles of books that are in our database Now let's modify our path expression Instead of only getting book titles let's get book or magazine titles We do that by extending our middle matching element here to use a sort of regular expression like syntax book or magazine and we put it in parentheses So now it says match any path of the data that starts at the bookstore element follows either a book or magazine sub_element and then finally a title sub_element When we run the query we see now that we get not only the titles of our books but also the titles of our magazines So far we've mentioned element names explicitly in our path_expressions But as I_mentioned in the introductory_video we can also use what's_known_as a wild card symbol the symbol star Star says to match any element name So now we're going to start again with bookstore match any element below bookstore and finally find titles of elements below those any elements Now it so happens that the only elements below bookstore are books and magazines so when we run the query we will get exactly the same result So far we've been navigating with the single slash operator which tells_us to go one element at a time We're at a particular element and then we match sub_elements with the specific tag that we typed There is also the double_slash operator As you recall from the introductory_video double_slash says match myself or any descendants of myself to any length So if I put a double_slash title what we'll be matching is any title element anywhere at all in the eXtensible_Markup_Language tree We run the query and again we get exactly the same result because we had already been getting all of the titles which were sub_elements of books or magazines Now let's get something a little different Let's put slash_slash star Now that's kind of a wild thing to put because it says I'm going to match any element in the entire tree and furthermore it can be of any the element type let's run the query and see what_happens What we get is a huge result Let_me just scroll down so you can see the result In fact what we're getting is every element at every level of the tree including all of its sub_elements So in fact the first element in our result is our entire tree because it's our book store element And we'll go all the way down to the end of the book store The next element in our result is some children of the book store so we get the book elements And we're also going to get their children in the answer And as we keep scrolling down we'll see that we get every element of the entire database That's not a useful query but it does demonstrate the construct the double_slash matching any element in the tree and the star matching any tag of any element Now let's turn to attributes Let's_suppose we're interested in returning all the ISBN_number in the database so we'll go_back to saying book store books of elements and then we'll get the attribute ISBN So we type at sign and ISBN Let's run the query and we get an_error It turns_out that attributes cannot be what is called serialized in order to return them in an XML looking result So what we need to do actually is obtain the data from the attribute itself and once we do that we'll see that we're getting the answer that we desire So we'll ask for the data of the attribute run the query and now we see we have all its ISBN numbers Now the attribute data is just strings so we're returning the ISBN numbers as a set of strings with blanks between them So some of these are sort of peculiarities of how XPath works Again we were not able to return an attribute because it didn't know how to structure the result but once we extracted the data from the attribute it returned it as string values So far we've only seen path_expressions with no conditions involved so let's throw in a condition Let's say that we're interested in returning books that cost less_than So what we're going to do here is navigate to the book and then we're gonna use the square_bracket which says start evaluate a condition at the current point of the navigation So the condition that I'm going to put is that the price of the book is less_than We'll run that query and we'll see that we have two books whose price three books I apologize whose price is less_than Now here we return the book that satisfied the condition What if what we actually want to return is the title of the books whose price is less_than What we can do is after evaluating this condition on the book we can actually continue our navigation So we just put slash title here It says find the books only keep the ones that match the condition and then continue navigating down to their titles and return that as the result of the query We run the query and we see our result Now another type of condition that we can put in square_brackets is an existence condition instead of a comparison If we put for example just the label remark inside our square_brackets that says that we should match books that have a remark So putting an element name in square_brackets is an existence condition on that sub_element existing Once we've isolated the books that have a remark we'll return the title of the books We run the query and we discover that two of our books have a remark You can go_back and check the data and you'll see that's indeed the case Let's get a little_binary_digit more_complicated now Let's get_rid of this query and put in a whole new one In this query we're going to find the titles of books where the price is less_than and where Ullman is one of the authors So now we have in our square_brackets a longer condition The price is less_than ninety and there_exists and implicitly this is an exist there_exists a sub part from the book author slash author slash last_name where the value of that is Ullman If we satisfy both of those conditions it will return the title of the book so we run the query and we discover two books that are less_than where Ullman is one of the authors Now let's expand the query by adding another condition We want not only the last_name of the author to be Ullman but the first name to be Jeffrey So now we're looking for books where Jeffrey_Ullman is one of the authors and the books are less_than So we run the query and we get the same result not surprisingly since Ullman is always paired with Jeffrey But actually this is not doing quite what we're expecting and I'm gonna explain why by demonstrating some changes Let's say that we change our query to not look for Jeffrey_Ullman as an author but to look for Jeffrey Widom Hopefully we'll get no answers but when we run the query we see still get a book The First Course In Database Systems So the two authors of that book if you look back at the data are Jeffrey_Ullman and Jennifer Widom So let's see why that book was returned in this query The reason is if we look closely at this condition what we're saying is we're looking for books where the price is less_than and there_exists an author's author last_name path where the value is Widom and there_exists an author's author first name path where the value is Jeffrey Well that in fact is true We have one author whose last_name is Widom and another author whose first name is Jeffrey Let's try to formulate the correct that query now So instead of matching the entire path to the last_name and then the entire path to the first name separately through the author's sub_elements what we want to do so if we want to look_at each author at a time and within that author look_at the last_name and first name together So to modify our query to do that wer're going to use a condition within the condition Specifically within the author slash author we'll look_at the last_name and the first name This syntax error is temporary once we finish the query everything will look good So we put a second bracket there and let_me show again what I've done that said we're looking for books where the price is less_than and there_exists an author slash author sub_element where the last_name is Widom and the first name is Jeffrey Hopefully we'll get an empty answer here We execute the query and indeed we do Now our original goal was to have Jeffrey_Ullman so finally we'll change Jeffrey_Ullman run the query and now we get the correct answer Incidentally it's a very common mistake when we have a condition to put a slash before the condition If we did that we would get a syntax error When we write the square_bracket it essentially acts like a slash so when we reference a sub_element name within a square_bracket we're implicitly navigating that sub_element Next we're going to try a similar query with a twist We're going to try to find books where Ullman is an author and Widom is not an author So we we navigate the books as usual and we look for cases where there's an authors author last_name equals Ullman and there's an authors author last_name not equal to Widom Now you may already detect that the this is not the correct query but let's go_ahead and run And we see that we got three books but we know the first two books Widom is an author so as you may detected this is not correct What this asks for are books where there's an author whose last_name is Ullman and there's some author whose last_name is not Widom well in fact every book with Ullman as an author has some author whose last_name is not Widom That would be Ullman So even if I took away this condition and ran the query again I'll get exactly for the same results Well actually I got a syntax error I forgot to erase the and so let's get_rid of that run the query and now we do in fact get the exact same result So as a reminder we were trying to find books where the last where Ullman is an author and Widom is not in fact we do not have construct yet to write that query A little later in the demo we'll see how we can in a kind of tricky fashion but for what we've_seen so far with path_expressions and conditions we're unable to write that specific query So far we've_seen two types of conditions in brackets we saw comparisons and we saw existence constraints where we checked to see whether a particular sub_element existed As you might remember from the intro we can also put numbers inside in square_brackets and those numbers tell_us to return the F sub_element Specifically if we look_at this query we're using slash_slash to navigate directly to authors elements and then we want to return the second author sub_element of each author's element So we run the query and we'll see if we look that our data that Jennifer Widom Jeffrey_Ullman and Hector Garcia Melina each appear once as the second author of a book or a magazine if we changed this to three we'll be returning third authors only and we can see only Jennifer Widom as a third author If we change this to ten hopefully we'll get an empty result and in fact we do Now let's take a look_at some built in functions and predicates In this query we're going to find all books where there's a remark about the book that contains the word great So we're going to navigate using slash_slash directly to book elements and within the book element we'll have a condition that invokes the built in predicate contains which I_mentioned in the introductory_video which looks at two strings and checks whether the first string contains the second one So if we have a book where there's a remark which is a string that contains the word great then the book matches the condition and will return the title of the book We run the query and we see that we have one book that has the remark containing the word great Our_next query does something kind of new I like to call this query a self join but that's probably only because I'm a relationally biased person But what it's actually doing is querying sort of two_instances of our bookstore data at once and joining them together So we'll see that our Doc Bookstore appears twice in this expression Let_me explain what this expression is doing It's finding all magazines where there's a book that has the same title as the magazine and here's how it does it So our first path expression navigates two magazines and then it extracts in the condition the title of the magazines The magazine will match if the title equals some book title and so to find the book titles we need to go_back to the top of the document so we get a second incidence of the document and we find book titles Now when we have the equals here this equals is implicitly be existentially quantified Did you follow that Implicitly existentially quantified That_means that even_though we're doing equals on what's effectively a set the condition is satisfied if some element of the set is equal to the first title Okay There's a lot of implicit existential quantification going on in equality in XPath and in XQuery as_well as we'll see later on In any case let's run the query and we will get back the fact that the magazine called Hector and Jeff's_Database_Hints has the same title as a book and if you look back in the data you'll see we do have a book of the same name We saw one example of a built in predicate contains This example shows another built in function in this case the name function and it also shows our first example of a navigation axis We're going to use the parent axis What this query is going to find is all elements whose parent element tag is not bookstore or book Of_course this is just for demonstration purposes It's not really that useful of a query But let_me just walk through the construction of the query So we're starting with our bookstore and then we're using which finds all elements We saw earlier when we ran the query we saw that it matched every element in the book in the database Now since we've already put in bookstore We're not going to match the bookstore element itself but we'll match every child of the bookstore element So what the condition looks for is the tag of the parent of the current_element and it sees if it's book store or book and we return the element if it's neither book store or book at the parent tag Here's how we find the parent tag So name is a built in function name operates on an element and it returns the tag of that element The element we want to look_at is the parent of the current_element and the way we do that is with the parent navigation axis which is parent colon colon Finally the star is matching the tags of the parents Well here we say match any tag of the parent extract the tag and check if it's book store or book So when we run the query we'll see that we get that pack a lot of data but all of them are elements in the database whose parent is not the book store or book Here's another example of a navigation axis In this case we're using following_sibling Following sibling says if we are at a specific point in the tree you should match every sibling so every other element at the same level that's later in the document that follows the current sibling So let's walk through this expression and see what we're doing What this expression is looking for is all books and magazines that have a non_unique title In_other_words all books or magazines where some other book or magazine has this same title So we navigate down to books or magazine elements this is what we saw in one of our earlier path_expressions we'll match any book or magazine element and then we want to find one where the title is equal to some title of a later sibling Now our books and magazines are all at the same level in our data so when we do following_sibling we're going to be matching all other books and magazines that appear after the current one And again this star says that we can match on element of any type We could equivalently put book or magazine in here because we know they're all books or magazines and we'll do that in a moment but for now let's just focus_on running the query So we execute the query and we find two answers We find Hector And Jeff's_Database_Hints which is a book because we had a magazine of the same title and we find National_Geographic which is a magazine because there's another magazine of the same title So actually this query was somewhat incomplete And that was our fault The way we wrote the query we said that we want to return book or magazine elements when a later one has the same title So that doesn't actually return all of the ones with non_unique titles it only returns the first instance of each one with a non_unique title Let's modify the query to do the right thing What we need to do is not only check_whether the title equals the following_sibling title of some book or magazine but whether it might also equal a proceeding one So we add title equals the same construct using the proceeding sibling axis slash_slash title Here we go and now when we run the query we see that we get Hector and Jeff's_Database_Hints and National_Geographic but we also get another instance of National_Geographic and another instance of Hector and Jeff's_Database_Hints So now we have the correct answer We don't only get the first instance of duplicated titles but we get both of them Now to show the use of the star we were matching any book or magazine as the following_sibling What if all we were interested in is cases where there's a book that has the same title but not a magazine and we can do the same thing here In that case we shouldn't get National_Geographic anymore Let's run the query and indeed all we get in fact is Hector and Jeff's_Database_Hints as a magazine because that was the only instance where there was an actual book that had the same title as opposed to matching books or magazines with the star Don't take a look_at this query yet Let_me explain what I'm doing before you try to untangle the syntax to do it As I_mentioned earlier Xpath revolves around implicit existential quantification So when we are looking for example for an author whose name is Ullman implicitly we will match the path if any author has the last_name Ullman And in general most of XPath revolves around matching sets of values and then returning things if any element of that set matches the condition What if we want to do universal quantification in other_words for all That turns_out to be more_complicated but we can do it in a tricky fashion So what I'd like to do with this query is we're going to find books where every author's first name includes J If we wrote it in the fashion that we might be tempted to or we just say book author author first name includes J then we'll get books where some authors first name contains J To get books where all author's first names contains J is more difficult and the way we're going to do it is it's kind of a kluge we're going to use the built in function count So here's what we're doing in this query We're finding all books where the number of authors whose first name includes J is the same as the number of authors of the book without a condition okay So specifically under book we count the number of matches of an author's author sub_element where the built in function the built in predicate contains is true where the first name contains J And so we are counting the number of authors whose first name contains J and we're setting that equal to the count of the first name sub_elements We'll run the query and we will find indeed that there are two books where all of the authors' first name includes J We can use a related trick to write the query we tried to write earlier but failed to find books where Ullman is an author and Widom is not an author So with the implicit existential what_happened before is that we found books where there was an author whose name was Ullman and then there was an author whose last_name was not Widom And of course we still got everything back What we want to find is books where there's a last_name that's Ullman and where none of the authors have the last_name of Widom That's effectively again a universal quantification for all For all of the authors their last_name is not Widom Since we don't have a for all construct in XPath we're again going to use the count trick So in this query we're looking for books where one of the authors' last_name is Ullman and the number of authors using count again the number of authors whose last_name is Widom is zero So now we've expressed that query we run it and we get the correct answer That_concludes our demonstration of XPath We've shown a large number of constructs and we've written some fairly_complicated queries On the other_hand we certainly have not covered the entire XPath language If you're interested in our many online materials we'll also provide a data and we encourage_you to experiment on your_own This video gives an introduction to the XQuery query language for eXtensible_Markup_Language As a reminder querying eXtensible_Markup_Language is not nearly as mature as querying relational_databases It's much newer And there is no clean underlying algebra that is equivalent to the relational_algebra The sequence of development of eXtensible_Markup_Language querying which is started with XPath which we've already covered Then Access LT which we'll be covering later and in this video and the subsequent demo we'll be covering the XQuery language XQuery is an expression language also known_as a compositional language And we've_seen that already with relational_algebra What that means is when we run an expression of the language on a type of data the answer to that expression or query is the same type of data So let_me just draw the picture here We have some data We run an expression or query over that data and we get get a result back and that result is also going to be in the same type of data That_means we can run another query or expression over the same type of data that was the result of the previous one And again we'll again we get a result of the same type of data Now that also means that we can sort of put these together And when we put them together that will give_us additional expressions or queries that are valid in the language When we talked_about compositionality in the relational_model the types of data we were using were relations We would have maybe some relations here we run a query over those relations the result itself is a relation and we can run further queries over that relation that's the answer And then when we looked at relational_algebra we saw a composing the expressions of relational_algebra into more_complicated expressions and everything worked together In eXtensible_Markup_Language it's similar except the expressions operate_on and return what are known_as sequences of elements Now we're not going to go into a great detail of this model As we mentioned for XPath the sequences of elements can be generated from an eXtensible_Markup_Language document They can also be generated from an eXtensible_Markup_Language stream and the interpretation is quite natural Now let_me_mention that one of the basic types of expressions in XQuery is XPath So every XPath expression is in fact an XQuery expression or a valid query in XQuery Then we can use XPath and other constructs of XQuery to put together into more_complicated queries Now one of the most commonly used expressions in XQuery is what's_known_as the FLWOR expression and it looks closest to Structured_Query_Language of any of the constructs in XQuery Let_me explain how it works We have up to five clauses in that FLWOR expression and that's where the FLWOR comes from The first the for clause sets up what are known_as iterator variables So the idea here is that in the for clause this expression is evaluated and it will produce potentially a set result And then the variable will be backed Count to each element of the set one at a time and then the rest of the expression will be evaluated for each element So in other_words if this produces a set of end results Then we will effectively evaluate the rest of the queries N times We will see that much more clearly when we do the demonstration The let clause is more of a typical assignment So it's only run once each time the rest of the query is run And so this expression is evaluated and even if it's a set it's assigned once to this variable So it's not iterating now it's just doing an assignment Again it'll become quite clear when we do the demonstration The where clause specifies a condition and it's a filter very similar to the filters that we've_seen in Structured_Query_Language The order by is also sort of similar to Structured_Query_Language It sorts the results so that you can get the result in a particular order And finally the return clause says what we want to actually get in the result of our query And just as a reminder when we start the query with the for if we are effectively executing the query N times then each one of those N executions may result in one of the elements in the result The FLWOR expression has up to five clauses but actually all of the clauses except the return clause are optional The return is the only one that's required to have a well_formed query and to get a result I also want to mention that the for and let clause can be repeated multiple_times if we want to set_up multiple variables or multiple assignments and they can be interleaved with each other They don't need to be in a particular order A next query it's possible to mix query evaluation with simply writing down the eXtensible_Markup_Language data that we want in the result And here's an example In this case we're writing the opening and closing_tags that we want to be in the result of our query And then inside curly_braces we write the query itself And what these curly_braces effectively say are evaluate me The curly_braces are an indicator to the XQuery processor that what's inside them needs to be run as a query is replaced with the eXtensible_Markup_Language that's the query result and the final output is our opening and closing_tags with the eXtensible_Markup_Language query result inside At this point let's move_ahead to the demo We'll once again be using our bookstore data and we'll do a sequence of XQuery examples over that data Again we are not going to be comprehensive XQuery is a big language but will show the flavor of the language and a lot of its most useful constructs and we'll be able to write some quite_powerful queries In this demo we're going to run a number of queries in the XQuery language We'll be using the same eXtensible_Markup_Language data we've used for previous demos Here we can see the book store database with a number of books four books and a few magazines XQuery language is quite complex Certainly more complex than Structured_Query_Language We'll be covering a number of the constructs but we won't be able to cover everything and even the ones we cover will take a little getting used to I highly suggest that you take time to pause the video to take a look_at the queries or even better download the data and the queries and try them for yourself Our first query is a fairly simple one demonstrating a FLWOR or F L W O R expression That's the basic expression of the XQuery language Here we only have the F W and R part for where and return This query is going to return the titles of books that cost less_than ninety dollars where Ullman is an author Let's go_ahead and take a look_at the constructs of the query The four construct as we described has an expression in this case it's an XPath expression that returns the books in our document It binds the variable dollar B to each book one at a time and run the rest of the query for that finding the next thing it does is check_whether the price of the book is less_than ninety and whether there_exists an author slash author the last_name under the book element where the value of that is Ullman In the case where both of these conditions are true it will return the title of the book Let's run the query When we run the query we see that we get a result that's formatted as eXtensible_Markup_Language with two titles of the books that cost less_than ninety and Ullman is an author Our_next query is a binary_digit more_complicated It still consists of a for where and return clause but in the where clause we have existential quantification and then in the return clause we're constructing our result in eXtensible_Markup_Language What this query looks for is books where the title of the book contains one of the author's first names For each of those books it returns the book title and the first name of the author So we again in our for clause bind dollar B to each each book in the database Then in our where condition what we look for is a first name subelement of the book which gets bound to FN such that that the title of the book contains that first name So these as a existential quantified expression some in and satisfies our keywords So some binds to a variable which we bind in the set and then we check if any of those bindings satisfy this condition Again I urge you to take a close look_at this You may want to pause the video Once we've determined that our book satisfies the condition then in return clause we're constructing an eXtensible_Markup_Language element where the opening and closing_tags are book And then within that we return the title of the book and we return the first name of all authors of the book Whew So let's run the query We find that there are two books satisfying the conditions where there is a first name in the title Hector and Jeff's_Database_Hints and Jennifer's Economical Database_Hints Now there is tricky thing here which is that in Hector and Jeff's_Database_Hints that title does in fact contain a first name contains Hector but it does not contain Jeffrey Our query returned the title of books satisfying the condition with every first name of an author of that book What we're going to do next is restrict the result of the query to only return the first names that are actually part of the book title What we're going to do is modify the second portion of our return statement to be a little_more complicated In fact we're gonna put an entire for return a query right inside the braces here In XQuery we can mix and match queries and expressions as we wish So what I've done now is I'm again returning the title but in the second clause instead of returning all the first names I'm going to find the first names that are within the book and when the title contains the first name then I'll return that one So effectively I'm restricting the first names returned to just be the ones that appear in the title Let's run the query and we can see that correctly Jeffrey disappeared from the first book element returned Our_next query again demonstrates a few new constructs First of all we'll be using the let clause in this query rather_than the for clause Second of all we'll be showing some aggregation And finally we've embedded the full query inside eXtensible_Markup_Language So we've put averages opening and closing_tags for our result and within that we're putting our entire query Our query says that we're going to assign the variable key list to the result of this expression So remember the for clause is an iterator while the let clause is an assignment So this expression is going to find all of the price attributes in the database assign it the P List as a list and then it will return the average of the elements in that list Let's go_ahead and run the query and we see that our average book price is sixty five We can actually run this query in an even more compact fashion we can assign variable 'A' to be the average of this entire expression here and then we can just return 'A' Not much more compact but perhaps a little_binary_digit more intuitive Let's do that let's erase the answer to run the query again and again we get the same result Now let's see a query where we use both the let and the for expressions In this query we're going to start_by assigning dollar A to the average book price just like we did in the previous query and then we're going to find all books where the price of the book is below average So we'll again as in previous queries assign dollar B one at a time to the books in our database and then for each one we'll check if the price is less_than the average price that we had from our let clause If it is we'll return the book element and we'll return the title of the book and we'll put as a subelement in this book element the price And here we can see where we're taking an attribute and we're turning it into an element So we have our attribute the price we obtain the data as we saw we needed to do in the X demo and we place it inside the price element Let's run the query and we see indeed that we have two prices whose books are below average and here we've converted the price from an attribute to a sub_element As a reminder the FLWOR expression in XQuery has a For Let Wear Order by and Return We see in all of the clauses except the order by So let's see the order by in this example We're going to find the title and prices of books just as we found before We'll convert the price to a sub_element But in this case we want to order the result by the price of the book So we do so by adding this clause here between the for and return saying we want to order by the price easy enough We run the query and we see out result Actually it doesn't look quite right does it We have one hundred before and eighty five Well that's because price is actually a string and so it was doing a lexical graphic ordering of the string We can fix that easily by calling a built in function called XS INT that converts that value to an integer When we run the query now we get the correct ordering based_on the values of the price Now that we've_seen ordering let's take a look_at duplicate elimination Let's a query that finds all the last names in our database So we write a simple query that says for all names in the XPath expression that finds the last names and just as a reminder here we use the double_slash that looks at any depth in the eXtensible_Markup_Language tree and picks out the last names will return those last names and we know that these are our last names and we've_got many repeats of them because these last names appear several times in the database So let's see about getting rid of those repeats There is a built in function in XQuery called distinct values So what we can do is add distinct values here in our for clause We can apply it to the last_name and now our dollar N will be bound to each value only once and then we'll return the result We run the query and we find that we have our three last names appearing only once but it's probably not quite what we wanted This time when we run distinct values it just turns these three values into three separate strings and the returns one at a time rather_than embedding them with the last_name tag which was what we got when we didn't use distinct values So if we want the last_name tag then we can add the last_name here to our return clause let's just put in the opening_tag and the closing_tag was put in for us So let's put dollar N here So now we've added opening and closing_tags to our previous query we run it and whoops what do we get we got dollar N This is a reminder that when we write a return clause if we want actually have an expression evaluated in the curly brackets So by putting dollar N in curly brackets here now the execution engine will actually evaluate this query and put the result within the opening and closing_tags we run the query and now finally we got exactly what we were looking for The three last names that appear in the database with the duplicates eliminated In a previous query we saw existential quantification in the where clause We were using the some keyword S O M E Now we're going to see universal quantification in the where clause So for all and for that we use the every keyword What this query finds is books where every author's first name includes the letter J Once again we use the for clause to bind B to every book in the database and then for each book we check our where clause Our where clause requires that every first name that's a sub_element in that book satisfies the condition where the first name contains the letter J If we have such a book then we return it as a result of the query So now we can see that our result is A First Course in Database Systems and Jennifer's Economical Database_Hints because for both of those all of the first names include the letter J This query demonstrates what I like to call a self join because it's combining two_copies of the book data Specifically what the query is looking for are titles of books that share a common author and we'll assume we're just looking for common last names This is also the first query we're seeing where we have two_copies of the FOR clause So we're setting up two iterator variables essentially one called B and one called B that are both iterating over the books in the database If you'd like to think of this in relational terms it would be similar to having two_copies of the book table one of them bound to variable B and one to variable B Then what we look for in the WHERE clause is whether B and B have an author last_name in common And we do that by checking these two XPath expressions and seeing if they're equal Now we saw this behavior in XPath and we're seeing it again here where the WHERE clause has effectively an existential quantification occurring automatically What it's looking for is if there is some last_name under B that is equal to some last_name under B even if B or B have many last names In the case where that equality is satisfied then we will construct a book pair where we return the first title and the second title And here we're taking the titles which were originally attributes Oh no I'm_sorry those were originally sub_elements We're taking those sub_element titles and we're calling them title and title So let's go_ahead and run the query and we did get a bunch of book pairs probably more_than we expected Now all these books do in fact have a common common last_name in their authors but what we forgot to do was eliminate books with themselves Because of course every book has in common a last_name with itself So let's modify our query so that we return only books where they are two different books and we can do that fairly easily by just saying b title is not equal to b title Okay here we we go We run the query and we got an_error because I typed Bluetooth instead of B Let's run that again Alright And now we see that we now are not getting the pairs of books with themselves We're still perhaps getting more_than we expected however What's going on Well one thing we're seeing is that we're getting every pair twice in both orders if you take a look_at the data So this is a same old trick that you may remember back from the relational query examples instead of doing not equals let's try doing less_than In_other_words we're going to return each pair once and we're going to have the lexical graphically lesser book listed first Now we've run the query again and now we see we actually got what we were expecting As our grand finale we're going to invert our bookstore database The way we set_up our eXtensible_Markup_Language data we had our books and then underneath our books we had the authors of the books What if we want to instead our data constructed where we have the authors as the outer elements and within each author we list the books that they've written Now to understand this query you're absolutely going to need to pause the video Let_me just briefly show what's going on But again it would take quite some time to go through every detail The way we are going to construct our inverted bookstore is to find the authors by last_name as the outermost portion of our query For each author's last_name we're going to get their first names and then we're going to return the author with the first name the last_name and next find all of the books that were written by that author return the book with their ISBN price title Okay Again I urge you to pause the video and look very closely at what's going on Or even better download the data in the query run it yourself and play with it until you understand it Let's of the query and we'll see our result Just scroll up a little_binary_digit here And we'll see how the query did effectively invert the bookstore Now we have our authors as the outermost elements Within each author the first one Jeffrey_Ullman we have the books that were written by that author including their ISBN and price as attributes and the title as a sub_element Now in my original version of the query I didn't include the edition or the remark since those didn't appear in every book But in fact it doesn't cause any problem at all to add those in So let_me just add right here the edition So we're using dollar B as our variable for the book and the edition is an attribute Again not every book has an edition but we'll see that it's not going to cause a problem when it doesn't And similarly down here we'll include the remark again for those books that include them Let's run the query and let's take a look_at our results and see what_happened So for Jeffrey Ullman's first book we got the edition for the second book we got the remark and everything just worked out just fine So again there is our inverted bookstore That_concludes our demonstration of the XQuery language I'll again emphasize that it's a very_large language very_powerful We've seen a number of different constructs We've written some pretty interesting queries But to fully understand the language you'll need to run a bunch of queries for yourself and experiment with how it works In this video we'll introduce querying eXtensible_Markup_Language data using eXtensible_Stylesheet_Language_Transformations As a reminder querying eXtensible_Markup_Language data is not nearly as mature as querying relational data due to it being much newer and not having a nice underlying algebra like the relational_algebra We already talked_about XPath which was the first language developed for querying eXtensible_Markup_Language data And we've also talked_about XQuery which was actually developed after eXtensible_Stylesheet_Language_Transformations but it's similar to XPath in it's style where eXtensible_Stylesheet_Language_Transformations which we're going to cover in this video is actually quite different eXtensible_Stylesheet Language stands for the Extensible Stylesheet_Language and it was introduced originally but soon extended to included transformations and eXtensible_Stylesheet_Language_Transformations is currently much more widely_used than eXtensible_Stylesheet Language Here's how we can think of eXtensible_Stylesheet_Language_Transformations as a query language We have an eXtensible_Stylesheet_Language_Transformations processor and we feed to that processor our eXtensible_Markup_Language data in the form of a document or a stream And we also give the processor a specification in eXtensible_Stylesheet_Language_Transformations which by the way is expressed using the eXtensible_Markup_Language format The processor takes the data and the specification and it transforms the data into a result which is also expressed as an eXtensible_Markup_Language document or string Now if we think_about traditional database query processing there's actually a natural mapping If we think even about relational processing we have a query processor and a database We feed the data to the query processor we feed the query to the query processor as_well and out comes the answer So eXtensible_Stylesheet_Language_Transformations processing although it really is through transformations it can be thought of very much like querying a database So even_though eXtensible_Stylesheet_Language_Transformations be thought of as a query language the query paradigm itself is quite different from what we're used to with Structured_Query_Language or even with XPath or XQuery It's based fundamentally on the notion of transforming the data And that transformation occurs with rules To understand what the rules do and how the transformations work it's again very instructive to think of the eXtensible_Markup_Language as a tree So let's take our bookstore data and again make it a tree as we did before when we were first learning_about XPath So we have some books sub_elements and we have a magazine sub_element and I won't be elaborating all of these We'll just imagine sub trees here with our book we have a title and we have some authors The title might be our leaf so we'll have a first course in database_systems for example whereas our authors may have author sub_elements and within those author sub_elements we might have first name name and last_name abbreviated here with string values for those and of course more authors sub_elements as_well So that give the basic_idea of a tree structure of eXtensible_Markup_Language exactly as we've_seen before So now let's see what_happens with eXtensible_Stylesheet_Language_Transformations in light of this tree structure So the first thing that we have is the concept of matching a template and replacing it So the idea in eXtensible_Stylesheet_Language_Transformations is that we can write an expression that finds a template that finds portion of the eXtensible_Markup_Language tree based_on template matching For_example we might find books that have certain authors and once we find those will actually replace the entire subtree with the result of what we put in our template For_example we might decide that want to pick the title here and replace this entire subtree with the title Or we might match down to our authors and we might find our first name and last_name and say replace this entire author sub_element with the concatenation of the first and last_name Again the idea being that you write templates that match within the tree using in fact XPath as we'll see as one of the portions of writing those templates and then replace that portion of the tree We can also do that recursively So we can for example decide that we're going to replace this book with a different element and then recursively apply our templates to its children We'll see that in a demo It takes a little getting used to again The eXtensible_Stylesheet_Language_Transformations language has the ability to extract values and again it often uses XPath expressions in order to do that It also has some programming language like constructs It has a For Each so we can do iteration and it has conditionals so we can do if All of these will be much better seen in the demo Finally I'll have to mention that there's some somewhat strange behavior having to do with white space in eXtensible_Markup_Language data and some default behavior which we'll see in the demo And there's also an implicit priority scheme when we have multiple templates that can all match the same elements So let's move directly to the demo We're again going to be using our same bookstore data and we'll see a number of eXtensible_Stylesheet_Language_Transformations examples Even more_than XQuery or XPath our examples will not be exhaustive but they will give a flavor of the language and you'll be able to express some fairly powerful queries using just what we show in the videos Now let's see eXtensible_Stylesheet_Language_Transformations in action Let_me first explain what we have on the screen In the upper left window we have the document that we'll be querying over It's the exact same bookstore data that we've been using for all of our examples So I'm actually going to make that a lot smaller so that we can see our templates better In the upper right corner eXtensible_Stylesheet_Language_Transformations templates And every example we're going to do is going to have us opening and closing a style sheet with some parameters is to tell_us how we'd_like to display our results And then I'll be putting different templates between those opening and closing_tags Notice again that eXtensible_Stylesheet_Language_Transformations is expressed using eXtensible_Markup_Language once we have our data and our set of template matching rules we'll run our transformation and in the bottom we'll see our result So you can think of it as a query in the upper right the data in the upper left and the result displayed in the bottom Now even more_than XQuery it's not going to be possible to explain every single intricacy of the templates that we're going to write So I again encourage_you to pause the video to take a look as_well as download the data file and the transformation file so that you can experiment with them yourself Our first example is going to do some very_simple template matching It's going to look for book sub_elements and when it finds them it's going to replace those book sub_elements with a book title element the value of the title component of the book and a closing_tag book title And it's similarly going to match magazines of elements and replace those magazines of elements with an element that's an opening_tag of magazine title the value of the title sub_element of the magazine and the closing_tag So again the template will look through the eXtensible_Markup_Language tree They will match the sub_elements in the tree It'll match the book of elements and the magazine of elements And for each one it will replace those subelements with the expression in this case with our opening and closing_tags that have changed and the value of the title We run the transformation and we see indeed that the results are our four book titles now opening and closing_tags that are book titles and our four magazine titles For our next example we're going to only match books that satisfy a condition We do that by in our matching expression using XPath Now there's one small strange thing here which is we can't write the less_than symbol we actually have to use the escape symbol for less_than But otherwise this template finds books whose price attribute is less_than just like we do in XPath using the square_brackets for conditions and when it matches those books what it does here is it copies those books So this is an important construct that says if I match the book I'll copy the book I'll select dot which the current_element so in effect it's saying find the books and retain them exactly as they are Let's run the transformation and take a look_at what we get We can see that we got this book because it's price is and we have another book whose price is and another book whose price is But we do see something a little_binary_digit strange here We got our books so we also have these strings here These long bits of text that we well we don't really know where they come from Well this is one of the peculiarities of eXtensible_Stylesheet_Language_Transformations When you have elements in your database that aren't matched by any template what eXtensible_Stylesheet_Language_Transformations will do is actually return the concatenation of the string leaf or text leaf values of those elements I know it seems kind of strange There's actually a simple fix for that We're going to add a second template that matches those text elements and for those returns nothing So here we've added a template and let_me explain What we're matching here is elements that satisfy the text predicates so that will match those leaf text elements and when we write a template that has no body so we open the template and then we close the template with no body at all that says match the element and then don't replace with anything at all So this is very useful construct the templates that don't have a body for getting rid of portions of the data we're not interested in So let's run the transformation now and take a look_at the result and now when we scroll down we see that all of that extraneous text that we saw in the previous example is now gone So as we've_seen eXtensible_Stylesheet_Language_Transformations works by defining templates that are matched against the data When a portion of the data is matched by a template the template says what to do We might place that portion of data with something different and we might just remove that portion of the data from the answer or we might just copy it over into the answer Now let's explore what_happens when we have portions of the data that are matched by more_than one template in our eXtensible_Stylesheet_Language_Transformations specification So here we're going to have three templates The first two templates both match book elements The first template says when we match a book element just throw it away Again this is an example of the template when we don't have a body that says eliminate the matched elements from the answer The second template says to do exactly the opposite Says when we match a book sub_element keep that book sub_element exactly as it is As a reminder this body here says copy the current_element into the result Our third template matches magazines and this one we just have one and it says copy the magazine into the result So let's go_ahead and run this transformation and see what_happens Well first of all we got an ambiguous rule match so that's good The system recognized that we have two different rules that are matching the same element But then it did decide to give_us a result So let's take a look_at what_happened It did return in fact all of the books in the database as_well as all the magazines So we can see that it chose to use the second template instead of the first template when we had the ambiguity So let's try an experiment Let's take our two book templates and let's just reverse their order So now we have the one that copies first and the one that eliminates second Let's run the transformation and indeed something changed We no_longer got the books So what we can deduce from that is that when we have two templates that both match and we get this ambiguity warning it still does the transformation and it chooses the second of the matching transformations Actually it turns_out not to be quite that simple It doesn't always choose the second one In this example we're going to change our first template to match only books whose price is less_than So we'll use the same syntax we used before that before We have to escape that less_than character like this Less than Close our score bracket So now our first transformation says when we find books that are less_than let's return them and when we find any book let's not return it So again we're going to have some ambiguity let's run the transformation Well we actually didn't get an ambiguity error this time or warning and the reason is that eXtensible_Stylesheet_Language_Transformations actually has a built in notion of some templates being more specific than others and when a template is more specific it is considered the higher priority template So what_happened when we ran this particular transformation is the books that where the price was less_than were matched by the first template and because that one's considered more specific they were not matched by the second template So we can see below that we did get back all of the books that are less_than and none of the other books and again we got back all of our magazines So let's make one last change to experiment Let's take our second book and let's add to it a simple condition that's satisfied by every book which is the condition that the book has a title sub_element Again this is XPath Now perhaps our two rules have equivalent specificity in which we case we would again have ambiguity Let's just delete our result here and then let's run the transformation and see what_happens Indeed now we have an ambiguous rule match because both of these templates have a condition so they are considered equivalent again just when just like when neither of them had a condition And now that they're considered equivalent again the second one is going to take precedence because as you can see we didn't get any books in our result So even_though we have some books that are less_than those books also have a title so those books were matched by the second template and they were not returned So what you can see from these examples is that you do need to be very_careful when you write eXtensible_Stylesheet_Language_Transformations programs or queries where multiple templates will match the same data Now let's look_at a couple of different_ways of copying our entire input data as a result of our query Our first example is the simplest one We write a template that matches the root_element of the document As you may remember from XPath a single slash is the root_element And then as the body we have that copy of template that copies the entire current_element Let's run the transformation and we will see the we get our entire database as a result Incidentally we could change that slash to be bookstore It would do exactly the same thing since our bookstore is our root_element Okay delete this run the transform and once again we get the entire database as our result Now I'm going to show action with a much more_complicated way of copying the entire document but it uses an important kind of template that we'll see in other contexts This template is our first example of recursively applying templates to our result What we have here is a template that matches absolutely anything in eXtensible_Markup_Language data This is actually an ex path expression that says math an element with star that means any element tag any attribute at star or any text leaf of the eXtensible_Markup_Language data So again this or construct here is seen quite frequently in eXtensible_Stylesheet_Language_Transformations specifications to match just anything at all in the data When anything at all is matched that element of the data is copied and then the templates are applied recursively to everything below that's of any type So it may be best just to take my word for it or you can spend some time on your_own thinking about exactly why this works but again the idea that we match any type of element in our eXtensible_Markup_Language element attribute or text and we copy that object and then we apply the templates to all of its sub_elements recursively again copying them Now obviously this is not the best the easiest way to copy an entire document We saw the easiest way to do it with our previous example but we'll soon see why this particular template is valuable When we run it of course we get back the entire document Now the reason that this type of template is valuable is that we can use this as one of our templates and then add additional templates that give_us exceptions to copying the whole document And that will allow_us to copy the whole document except with changes in certain parts and what I'm adding here actually is a whole_bunch of additional templates So the first one says apply all templates recursively to link to the entire document The second says When you find while you're applying them recursively that you're at an attribute called ISBN we'll change that to a sub_element So we'll match the ISBN attribute We'll change it to a sub_element similarly to what we saw before by giving an open tag ISBN and the value of the current_element We'll similarly take our attributes our price attributes and change them to sub_elements and our editions our months and our years and our magazine And last of all we'll also make a change to our authors When we match an author instead of having sub_elements we'll convert those sub_elements to be attributes the last_name attribute and the first name attribute So let's run the transformation and we'll see our data is now significantly restructured We have our bookstore and we have our books but our ISBN numbers are now sub_elements and in our authors the last names and first names are attributes And all of the books are restructured in that fashion and our magazines again have attributes restructured as sub_elements Now let's see what would have happened if we ran this eXtensible_Stylesheet_Language_Transformations specification but we didn't have this mega template at the beginning that does the recursive application of templates to the entire database When we run the transformation now well we get a kind of surprising result We won't try to analyze it in its entirety It's a combination of only matching automatically of sub_elements and not attributes And furthermore dumping out all the text leaves like we saw in an earlier example So again presuming that we would not want this to be our result that shows the necessity of including the sort of generic template that matches every type of object in the database and recursively applies templates to its children Now let's switch gears entirely What we're going to do in this transformation is effectively write a program We're going to use the for each and sorting and an If statement and the program is furthermore going to take the eXtensible_Markup_Language data and it's going to transform it into Hypertext_Markup_Language which we can then render in a browser So it's just one template that matches the root_element of our document and once that root_element is matched it spits out the tag Hypertext_Markup_Language it sets up the table so again we're actually writing the result here and put some headers for the table And then we see a for each that says we're going to run the body of the for each for each book in the database We're gonna sort the result by its price If the price is less_than then we're going to generate a row in the table And that row is going to be set_up with italics for the title and it's going to give the value of the price it's going to close the row and we're going to close all the tags So again this is quite different in a couple of ways First of all that it's written more in a programmatic style and second of all that the result actually going to be Hypertext_Markup_Language Let's run the transformation and we can see the result here which is indeed Hypertext_Markup_Language In fact we can take this very Hypertext_Markup_Language and we can render it in a browser and see how nice it looks And here it is We can see very beautifully formatted the three books that cost less_than sorted by price with the title in italics all formatted in an Hypertext_Markup_Language table And that was with not a very complicated eXtensible_Stylesheet_Language_Transformations program So it's not surprising that eXtensible_Stylesheet_Language_Transformations is used frequently for for translating data expressed in eXtensible_Markup_Language to Hypertext_Markup_Language format for rendering as_well as being used as a query language Our last two examples are back to a more traditional template matching style Again we're going to start with this recursive template match that matches everything in the database That_means we're gonna copy everything over except we're gonna make one type of change Specifically we're going to change we're going to take Jennifer out of the database and then we're going to change Widom to Ms Widom So every place where we have Jennifer as the first name and Widom as the last_name we'll end up with just a name Ms Widom Specifically we do it with two templates The first template says when we find a first name where the data in that first name equals Jennifer okay so we're again are using the dot to refer to the current_element The data is a built in function So a first name that's equal to Jennifer When we match that we want to we'll actually return nothing There's no body in this template so that will remove that element Now you_might_wonder why we didn't just write a condition that said first name equals Jennifer The problem is to write that condition the current_element would be the parent and we don't want to remove the parent we actually want to remove the first name itself In addition to removing first names that are Jennifer we'll also match last_name templates where the value is Widom and we will replace those with an opening_tag name the string is Widom and a closing_tag name So let's run the transformation and let's take a look And we will see in the case where the author was Jennifer Widom it's now the single element name Ms Widom and we should see that occur a few other times in the database as_well As our very last example let's perform the same transformation but let's do it with just one template What we'll do is we'll look for office of elements where the first name equals Widom Now we don't need to use data So first name equals Widom And we'll take those entire author sub_elements and we'll replace them with an author sub_element where the name is Widom So we need to put author here Let's get_rid of this automatic simply generated closing tab we want it to be over here We'll get_rid of this first template So again we're going to make exactly the same change but we're gonna do it with a single template It's going to look for authors where the first name is whoops better make that Jennifer And it's going to replace them with the author sub_element with just Ms Widom We run the transformation and let's take a quick look_at what we got And we again see exactly the same result with a somewhat simpler program That_concludes our demonstration of eXtensible_Stylesheet_Language_Transformations Again we've shown only some of the constructs We haven't gone into great detail or walked through the syntax eXtensible_Stylesheet_Language_Transformations is very_powerful We've seen quite a few different things We've also seen a little_binary_digit of non intuitive behavior We have to be a little careful with white space We have to be a little careful when we have multiple templates that match the same data But once we get it all figured out it can be quite_powerful for transforming data and for querying data This video talks_about data modeling and User_Mode_Linux the Unified Modeling Language The area of data modeling consists of how we represent the data for an application We've talked a great length about the relational data_model Its widely_used and we have good_design principles for coming up with relational schemas We also talked_about eXtensible_Markup_Language as a data_model eXtensible_Markup_Language is quite a binary_digit newer and there are no design principles that are analogous to the ones for the relational_model But frequently when people are designing a database they'll actually use a higher_level model that's specifically for database design These models aren't implemented by the database system rather they're translated into the model of the database system So let's draw a picture of that Let's_suppose that we have a relational database_management system which is abbreviated Relational Database Management System often and I'll draw that as a disk just out of tradition So if we create a database in a relational system the database is going to consist of relations but instead of designing relations directly the database designer we'll draw that up here will use instead a higher_level design model That model will then go through a translator and this can often be an automatic process that will translate the higher_level model into the relations that are implemented by the database system So what are these higher_level models Historically for decades in fact the entity relationship model also known_as the ER model was a very popular one But more recently the unified modeling language has become popular for higher_level database design The unified modeling language is actually a very_large language not just for database designs but also for designing programs So what we're going to look_at is the data modeling subset of User_Mode_Linux Both of these design models are fundamentally graphical so in designing a database the user will draw boxes and arrows perhaps other shapes And also both of them can be translated generally automatically into relations Sometimes there may be little human intervention in the translation process but often that's not necessary So in the data modeling subset of User_Mode_Linux there are five basic concepts Classes associations association classes sub_classes and composition and aggregation We're just going to go through each one of those concepts in turn with examples So that class concept in User_Mode_Linux is not specific to data modeling It's also used for designing programs The class consists of a name for the class attributes of the class and methods in the class and that's probably familiar to you again from programming For data modeling specifically we add to the attributes the concept of a primary_key and we drop the methods that are associated since we're focusing really on the data modeling at this point So we'll be drawing our examples as usual from a imaginary college_admissions_database with students and colleges and students_applying to colleges and so_forth So one of our classes not surprisingly will be the student class And in User_Mode_Linux we'll draw a class as a box like this and at the top we put the name of the class and then we put the attributes of the class so let's_suppose that we'll just keep it simple We'll have a student_ID a student name and for now the student's GPA and down here in User_Mode_Linux would be the specification of the methods Again we're not going to be focusing on methods since we are looking_at data modeling and not the operations on the data And so one difference is that we'll have no methods Another is that we specify a primary_key if we wish and that's specified using the terminology PK So we'll say that the student_ID in this case is the primary_key And just as in keys in the relational_model that means that when we have a set of objects for the student class each object will have a unique student_ID There will be no student IDs repeated across objects in our college application database we're also likely to have a class for colleges so we'll have a class that we call college And for now we'll make the attributes of that class just the college name and the state And again in full User_Mode_Linux there might be some methods down here And we'll make the college name and this case be the primary_key So we're assuming now that college names themselves are unique So that's it for classes Pretty straightforward they look a lot like relations and of course they will translate directly to relations Next let's talk_about associations Associations capture relationships between objects of two different classes So lets suppose again that we have our student class and I won't write the attributes now I'll just write it like that and we have our college class in our User_Mode_Linux design If we want to have a relationship that students apply to colleges we write that just as a line between the students and the college classes and then we give it a name So we'll call it applied and that says that we have objects in the student class and objects that are in the college class that are associated_with each other through the applied association If we want to introduce a directionality to the relationship so to say that student are applying to colleges we can put in a arrow there that's part of the User_Mode_Linux language although we'll see that it doesn't really make much difference when we end up translating User_Mode_Linux designs to relations When we have associations between classes we can specify what we call the multiplicity of those and that talks_about how_many objects of one class can be related to an object of another class So we'll see that we can capture concepts like one one and many one and so_forth So let's look specifically at how we specify those in a User_Mode_Linux diagram and for now I'll just use two generic classes So let's say I have a class C and I have a class C and let's say that I have an association_between those two classes so that would be a line And I could give that a name let's call it A Let's say that I want to specify that each object in Class C well I'm just going to write those objects kind of as dots here below the class specification Let's say that I wanted to say that each one of those is going to be related to at_least M but at most N objects in class C so here are class C objects I'm going to have this kind of fan out in my relationship To specify that in the User_Mode_Linux diagram I write that as M and on the right side of the association line and again that's say each object then in C then will related to between M and N objects of C Now there are some special cases in this notation I can write M dot_dot star and star means any number of objects so what that would see is that each object in C is related to atleast M and as many as it wants elements of C I can also write zero to end and that will say that each object in C is related to possibly none for example here we have one that I haven't draw any relations tips Possibly none and up to N elements of C I can also write zero_dot_dot star and that's basic no restrictions on the multiplicity And just to mention the default actually is one_dot_dot one So if we don't write anything on our association we're assuming that each object is related to exactly one object of the other class and that's in both directions by the way so I can put a X Y here and now we'll restrict how_many objects of element of C is related to Incidentally User_Mode_Linux allow some abbreviations can be abbreviated as a just plain old one and can be abbreviated with just star So let's take a look_at our student and college example and what the multiplicity of the association of students_applying to colleges might be So let's_suppose that we insist that students must apply somewhere so they apply to at_least one college but they're not allow to apply to more_than and further more lets say that no college will take more_than applications so this example is contrived to allow me to put multiplicity specifications on both_sides So again we'll have our student class and we'll have our college class and we'll have our association_between the student and the college class and I'll just write the name underneath here Now applied So lets think_about how to specify our multiplicities for this So to specify that a student must apply somewhere but cannot apply to more_than colleges we put a one_dot_dot five on this side It really takes some thinking sometimes to remember which side to put the specification on But that's what gives_us the fan out from the objects on the left to the objects on the right So it says each student can apply to up to five colleges and must apply to at_least one so we won't have any who_haven't applied_anywhere On the other side we want to talk_about how_many students can have applied to a particular college and we said it can be no more_than We didn't put a lower restriction on that so we would specify that as to So I_mentioned earlier that multiplicity of associations captures some of these types of relationships you might have learned about somewhere else called one to one many to one and so on So let_me show the relationship between association multiplicity and this terminology So if we have a one to one relationship between C and C technically one to one doesn't mean everything has to be involved What it really means is that each object on each side is related to at most one on the other side So to say it's a one to one relationship we would put a zero_dot_dot one on both_sides Let's see if I can use some colors here So what about many to one Many to one says that we can have many elements of C related to an element of C but each element of C will be related to at most one element of C So in that case we still have a zero_dot_dot one on the right side indicating that each C object is related to at most one object of C but we have the star on the left_hand_side indicating that C objects can be related to any number of C objects and as a reminder star is an abbreviation for zero_dot_dot star Many to many has no restrictions on the relationships So that would be a star on both_sides Pretty simple and the last concept is the idea of complete relationships So a complete relationship is complementary to these others It says that every object must participate in the relationship So we can have a complete one to one and that would be one_dot_dot one on both_sides We could have a complete many to one and that would be on the left_side one_dot_dot star and on the right side one_dot_dot one and finally a complete many to many would be one_dot_dot star on each side As a reminder the default if we don't specify the multiplicity is a one_dot_dot one both_sides So that would be a complete one to one relationship Ok we've finished with classes and with associations Now let's talk_about association classes Association classes generalize the notion of associations by allowing us to put attributes on the association itself and again we'll use our example So we already knew how to specify that students apply to colleges but what if associated_with the application we wanted to have for example the date that they applied and maybe the decision of that application We don't really have a way to do that without adding a new construct and that construct is what's_known_as an association_class So we can make a class and we'll just call it App Info And it looks_like a class it's got the box with the name at the top and the attributes And then we just attach that box to the association and that tells_us that each instance of the association_between a student and a college has additional_information a date of that application and the decision of that application Now there's a couple of things I want to mention First of all in a number of examples I'll probably leave out the multiplicities on the ends of the associations That doesn't mean I'm assuming the default one one It's just when it's not relevant I'm not going to focus_on that aspect Now when we have students associated_with colleges So we have a student here we have a college Then we have an association_between those Now what we're saying is that association is going to have affiliated with it a date and a decision What we cannot describe in User_Mode_Linux is the possibility of having more_than one relationship or association_between the same student and the same college So when we have an association that assumes at most one relationship between two objects So for example if we wanted to add the possibility that students could apply to the same college multiple_times so maybe you know that want to apply for separate majors That would actually have to be captured quite differently We'd have to add a separate class that would for the application information with separate relationships to the students and colleges So this is a in my mind a slight deficiency of User_Mode_Linux Again that and it only captures at most one relationship between the two specific objects across the two classes Now sometimes we can make a design that has an association_class and it turns_out we didn't really need it and we're going to come_back to multiplicities to see how this can happen so again let's take a look_at just generic classes C and C Let's say that we have an association_between them and then we have an association_class We'll just call it Alternating Current And that's gonna have some attributes we can call them A and A for now And of course there's attributes in C and C as_well Let's_suppose that the multiplicity on let's say the left_side is star so anything goes and on the right side we have one to one So what that multiplicity says is that each object Of C is related to at most one object of C So actually exactly one object in this case So we know that there's going to be just one association for each object of C and if there's only going to be one association actually we could take these attributes and we could put those attributes as part of C instead of having a separate association_class so for example If this class happened to be the student class and this was the college class and we insisted that each student apply to exactly one college then the attributes we had down here the date and decision could be moved into the student class because we know they're only applying to one college so that would be the date and the decision for the one college they're_applying to Furthermore if we had zero_dot_dot one we can still move these attributes here and in that case if a student was not involved in a college had not applied to a college at all or more generally an object of C was not related to any object of C then those attributes would have the equivalent of null_values in them By the way it is possible for an association to be between a class and itself For_example we could have our student class and maybe we're going to have an association called sibling a student being associated_with another student because they're siblings an association_between a class in itself is written with a line tgat just goes between the class and itself And then we could label that sibling And for multiplicities we can assume that every student has between and an arbitrary number of siblings lets say so we can put a star on both ends of that association A more interesting association might involve colleges where say we have for every college a flagship main campus But then some colleges have separate branch or satellite campuses so that would be an association_between a college and itself saying that one college is a branch of another college Now let's think_about the multiplicities here First of all when we have a self association in User_Mode_Linux we're allowed to label the two ends of the association So I could for example say on one end we have the home_campus And on another end we have the satellite campus And now with those labels we can see the asymmetry and that lets us get our associations right So let's say that every satellite campus must have exactly one home_campus so that would be a one_dot_dot here and every home_campus can have any number of satellite campuses Or actually let's say something else Let's say every home_campus can have between zero and ten satellite campuses be a zero_dot_dot ten on that side of the self association Ok we're finished with the first three let's move on to sub_classes For sub_classes we're gonna do a fairly large example that involves students that we're gonna separate into foreign students and domestic students We're also going to separately specify students_who have taken AP classes and those will be our AP students So we're going to have the student class as the top of our hierarchy and the student class will again have the student_ID let's say the student name and GPA and we'll say the the student_ID is the primary_key for objects in that class we're going to have three sub_classes one is going to be the foreign students we'll call it foreign S one is going to be the domestic students and then we're also going to have a sub_class for AP students and I'm going to assume that you already know a little_binary_digit about sub classing from programming So the idea is that when we have a sub_class there are attributes that are specific to the objects that are in that sub_class and they'll inherit the attributes from their super_class So we're gonna make student be a super_class here And this is how we draw it with three sub_classes here for foreign student domestic student and AP student And we'll say that foreign students have in addition to a student_ID a student name and GPA a country that they come from We'll say that Domestic students are going to have a state that they come from and we'll also say that they have a Social_Security_number which we don't know that foreign students would necessarily have AP students interestingly is going to be empty It's not going to have any additional attributes but the AP students are the students that are going to be allowed to have a relationship with AP courses We'll say that the AP course has a course number and that's probably the primary_key And maybe a title for the course and some units for the course And then when one of our AP students takes the course We'll call this Association took We're going to have an association_class that goes along with that that's going to have the information let's called it AP info about them taking that particular AP class and we'll say that association_class has for example the year that they took the class and maybe the grade that they got in the class And lastly let's add some multiplicities Let's say that AP students can take between one and ten AP classes but they taken at_least one to be an AP student and let's say that every course has taken by at_least one student and arbitrary number of students So this is one of the biggest User_Mode_Linux diagrams we've_seen so far Again this is a superclass up here And we have our subclasses down here And then we also have an association and an association_class and some multiplicities And again notice that is ok that there are no attributes in the AP student sub_class that sub_classes define as those student who have taken AP course Here are some terminology and properties associated_with sub_class relationships a super classes and User_Mode_Linux are sometimes called generalization with sub_classes called specialization and some sub_class relationship is said to be complete if every object in the super_class is in at_least one sub_class and it's incomplete if that's not the case and incomplete is also sometimes known_as partial a sub_class relationship is known_as disjoint if every object is in at most one subclass In_other_words we don't have any objects that are in more_than one subclass and that's sometimes called exclusive And if it's not disjoint then it's overlapping meaning that objects can be in multiple sub_classes We can have any combination of these pairs so we can have incomplete overlapping or incomplete disjoint a complete disjoint that are complete overlapping lets take a look back at our example for this example we will probably have the case that it's a complete subclass relationship In_other_words every student is in at_least one subclass presumably every student is either a foreign student or a domestic student and further more we're going to say that it's overlapping because we will have students_who for example are both a domestic student and an AP student And in User_Mode_Linux the actual notation is to put little curly_braces here to specify that that subclass relationship is complete and overlapping To illustrate some of the other cases let's_suppose that we didn't have this whole section here with the AP students We only had foreign and domestic students In that case we would say that the subclass relationship is complete But in that case it would not be overlapping It would be disjoint Or suppose we didn't have this whole left_side here so all we had was the AP student subclass In that case it would probably be an incomplete complete subclass relationship because not everybody is an AP student and they wouldn't make any difference between overlapping and disjoints since there would be only one subclass in that case Okay we've now made it to our last concept which is composition and aggregation Let_me start_by clarifying right off that aggregation here has nothing to do with aggregation in Structured_Query_Language Well it's a completely different concept So let's first talk_about composition Composition is used when we have a database structure where objects of one class kind of belong to the objects of another class and the example I_am going to use is colleges and departments So I've drawn the two classes here And let's say for the department we have the department name and we have say the building that the department is in And so we're assuming that each college has a whole_bunch of departments now we can make a relationship an association_between colleges and departments to say that the department is in a college but when we have the idea that the departments belong to a specific college then that's when this composition construct is used And the way the composition is written is by putting a diamond over here on the end of the association So composition is really a special type association And we'll fill in that diamond here to indicate composition Aggregation happens to have an empty diamond which we'll see in a moment so when we have the diamond and we're creating one of these composition relationships there's implicitly a one_dot_dot one on the left_side so each department belongs to one college but what's kind of interesting here what's little different from the normal relationship is that we're not assuming that this department name is a primary_key exactly We could have this same department in fact even in the same building in different colleges and that would be okay because a department is through this relationship associated_with it's college So that was composition objects of one class belonging to objects of another Let_me give an example of aggregation This is a slight stretch but what I'm going to make is a class of apartments Not departments but apartments So we're going to imagine that there are apartment buildings represented in our database maybe they have an address that the primary_key and something like the number of units and what we're going to imagine is that some apartment buildings are owned by or associated_with the college but not all of them are And that's what aggregation does So for aggragation we again have a relationship here but in this case we make a diamond on this side that is open and what that says is that each apartment each object in the apartment class is belonging to a college either at most one college or no college at all So we can have apartments that belong to a college we can have kind of free floating apartments and that's what the open diamond which is aggregation is about So in conclusion the data modeling portion of the Unified Modeling Language can be used to perform database design at a higher_level It's a graphical language We went through the five main concepts of the language and also very importantly User_Mode_Linux designs can be translated to relations automatically And that is the topic of the next_video In this video we're going to learn_about translating designs in the User_Mode_Linux modelling inter relational schemas As a reminder if the application developer is using a high_level design model then they will create their design usually using a graphical language that's sent to a translator and the translator will then produce a schema using the data_model of the underline database_management system which frequently is relational So in our case we're looking_at high_level design in the User_Mode_Linux data modeling subset which is then translated to relations and installed in a relational database system It's this translation process that's the topic of this video In the data_model of subset of User_Mode_Linux we have five concepts that we learned in the previous_video and we'll see now that designs using these concepts can be translated to relations automatically with one provision which is that every regular class has a key And we'll see what we mean by a regular class in a moment So what we're going to do is go through each one of these concepts one at a time and show show how the concepts are translated to relations For our examples we'll use primarily the same examples that we used in the previous_video where we introduced User_Mode_Linux For classes will have a student class and a college class Our students have a student_ID which is the primary_key a student name and a GPA and our colleges will have a college name which we'll assume is unique so that's the primary_key the state and the enrollment The translation from classes to relations is absolutely direct we take the student class and we make a relation called student we put the three attributes down for that relation and then the primary_key from the student class becomes the key attribute in that relation And similarly our college relation has the college name as a key the state and the enrollment So all we're doing to turn classes into relation is basically turning them side ways So now we're done with classes and let's move on to associations We'll see that each association is going to be translated to its_own relation that grabs a key from each side of the association so let's go to our example the same one we used earlier that has students_applying to colleges So here's our diagram with students_applying to colleges and we'll already have generated the two relations for our classes the student and the college relation and for the association applied between students and colleges we create a relation we call it applied and that relation has attributes the keys from both_sides so that would be the student_ID from the student class and the college name from the college class And if you think_about it it makes a lot of sense Applied is capturing relationships between students and colleges so we'll capture each of those relationships with one tuple in the applied relation identifying the student with the key and the college with the key Now one question you might have is what is the key for a relation that we generate from an association it turns_out that depends on the concept of multiplicity that we learned about in the previous_video so let_me setup a generic example of two classes with an association and then and we'll talk_about multiplicity and keys Ok so here we have both classes We'll call them C and C And each one has a key attribute we called it K and K over here and then one other attribute and and then we have an association A between the two classes So in terms of their relations that will be generated we'll have for C K with K as a key for C we'll have K with K as key and then for our association A we'll grab the key from each side K and K and the question we have now is what is the key for relation A Well as a default the key can be both attributes together but in certain cases we can be more specific and again that's going to depend on the multiplicity on our association Let's_suppose that our multiplicity is on the left_hand_side and a star on the right_hand_side and let's look_at what that represents in terms of the type of relationship we're capturing And I'll make_sure get it right this time so what this is saying here is that if we have objects of C on the left_hand_side each one can be related to many objects of C on the right_hand_side but each object of C can be only be related to at most one object of C and some of them might not be related to any So remember now it's these edges that are being captured by the tuples in our relation for the association and we can see that each element on the right_hand_side can only be involved in at most one tuple of A so that tells_us that K is actually a key for A So when we have zero_dot_dot one or one_dot_dot one for that matter On the left_side or on one side of an association then the key attribute from the other side is a key for the association So lets test out that rule on our student and college association and see if it makes_sense So I've drawn the association here and we're interested in the relation that's going to be generated for the association itself which will be the applied relation with the student_ID on one side from one side and the college name from the other so that's the relation we're going to generate for the association and the question is what's key for that relation Well let's add the constraint that every student applies to exactly one college So that would be a one_dot_dot one on the right and then a star on the left if a college can have any number of applicants So our rule of thumb it's actually a rule said that if we have one_dot_dot one or zero_dot one_dot_dot one on one side then the other side would be the key So that would tell_us that if we have this one_dot_dot one on the right that student_ID would be a key for applied and indeed that makes complete sense if each student can only apply to one college then they will indeed be a in the applied relation Now there's actually a related concept here where we might need a relation for associations at all and again that depends on multiplicity and again let's start with a generic example so here's our generic example and from this our standard translation would give_us three relations one for C one for C and then one capturing the association A which would have a key from each side Now what we're going to discover is that in come cases it's actually possible to fold relations A into either C or C will end up with just two relation instead of three and will be capturing the same information So let's_suppose we have a and on our left_hand_side so again we're going to have the situation where from the left_side to the right we have a fan out so each one on the right is related to exactly one on the left If that's the case then instead of having a separate relation for the associations basically for capturing these edges here we can simply take the related element up from the left the key from the left and add it to the element for the right Let_me show how that's done So what we'll do is we'll change this C to have K and O go to as before but also to have the key from the left_hand_side of class or relation so the key from C and then we don't need A any longer And we can see why that works because every element in C is related to exactly one element in C so we just add the key for that single element that is related to that relation Now what if the left_hand_side were zero_dot_dot one instead of one_dot_dot one In that case it would be possible for there to be some elements in C that aren't related to any elements in C and that would still be okay with this design as long as null_values would be allowed in the third attribute of the relation for C And finally what's the key to this relation Well we knew before that the key for C was K just by definition of it being the primary_key for the class and that's still the case in the expanded C K will still be a key because we'll only have one instance of each object and the one K one that it would be related to through the association So what we saw to summarize is that when we have zero_dot_dot one or one_dot_dot one on one aside and then we have an association and instead of making a relation for the association we can add the key from the side with the zero_dot_dot one or one_dot_dot one to the relation that we generated for the class on the right_hand_side So let's take a look again with students and colleges just to confirm this intuition So let's again suppose that a student applies to exactly one college and our rule of thumb says that we have a one_dot_dot one on one side then we can take the key from that side and add it to the relation for the other side so we would be adding to the student relation the college name attribute and we'd be getting rid of applied entirely Let's just think_about if that makes_sense if every student is applying to exactly one college it makes perfectly good sense to just add that college name to the student relation and furthermore if we had students just applying to either zero or one college that would still be an acceptable design provided that null_values are allowed for the college name attribute Ok we've finished with classes and with associations now let's talk_about associations classes Association classes are pretty straight_forward We're not going to generate new relations more then we're just going to add attributes to relations that we're generating anyway so let_me use our usual example with the students and colleges and we'll add some information to their application Ok so here's our User_Mode_Linux diagram We still have students_applying to colleges and then we've added an association_class which as you remember is attached to an association and it gives extra attributes for that association So here we're saying when a student applies to a college we'll have a date of that application and a decision of that application The first scheme that I've shown down here is what we would get if we use the techniques we've already learned without the association_class so the student class generates the student relation college generates college and then the applied association generates a relation that has a key from each side And by the way right now let's say that there's_no multiplicities or other information that's going to give_us special keys so in that case the applied relation doesn't have any keys other than the two attributes together And further more because we have no special multiplicities we won't be folding that relation into others as we showed in the previous example So now what do we do with our association_class Well as you can probably guess it's quite simple All we do is extend our applied relation to include the attributes in the association_class So we just add date and decision right here and then we're all set Now there's actually a few things I want to mention at this point First of all I discussed in the video on UMO modeling The fact that UMO assumes that when you have a relationship or an association_between two classes there's at most one instance of that association_between any two elements of the class And that is can be seen quite well in this relational scheme up because we're assuming we have a most one relationship between any student and any college and then we have associated_with that one relationship the date and the decision A second thing I wanted to talk_about at this point in time is the fact that we for an automatic translation require that we have a key for every regular class When I_mentioned that at the beginning of the video I wasn't able to motivate it very well but this example explains it pretty well When we want automatic translation the translation for an association requires a key from each side and each side is going to be a regular class in that case So we needed to have a student_ID or some key for the student class and we needed a key for the college class in order to capture the association On the other_hand here the application info is not a regular class this is an association_class and we're just defining that as not regular and we don't need to have a key for this one because we're just adding their attributes to the relation for the association itself and the last thing that I wanted to mention is that the rules we saw for determining keys for the relation associated_with associations and for folding in the class and relations for associations also work when we have an association_class and it just goes naturally You just bring the attributes with the association_class along with the attributes that you have for the association You_might want to give that a try on an example just to see how it works but again it's pretty straight_forward Now let's take a look_at how self associations are translated to relations It actually follows exactly the same rules but it's worth while looking_at to see exactly what_happens So here's our first example of association which was sibling relationships between students So in this case we generate the relation for the class as usual student_ID name and GPA and then we generate a relation for our association just following the same that we already had when we had an association_between two different classes so we need to grab one key from each side well it's the same side that we're grabbing from so the one thing we need to do is just change the attribute names so that we have two_instances of the key attribute from the class but they have different names So you can see what we're doing here We're just saying that we have sibling relationship We take the student_ID from the two students_who are siblings and that pair becomes a tuple in our sibling relation and because so we have stars on both_sides so we're not making any assumptions about multiplicities We don't have any zero_dot_dot ones or one_dot_dot ones Then we don't have any key for the sibling relation except the two attributes together Now the other example we had in self association was colleges being branches of other colleges So here's our college relation that we generate with the usual attributes and then we have our that's the key there We have our association which we're calling branch which takes a home_campus and a satellite campus and since we've labeled those two sides we can actually use those labels as the attributes for the relation we generate So we'll call it branch and we'll have home and satellite So that's saying that the satellite the campus on the the college on the right_hand_side is a satellite of the one on the left_hand_side Now these values here will be college names that will be the domain that we're drawing from but we'll calling home and satellite to distinguish their roles in the association Now what about keys We said before that every campus every satellite campus has exactly one home_campus so we write it that way And that a home_campus could have I think we said something like zero to ten satellite campuses but in terms of generating our relations the only really important thing is this one to one and if you remember what our rules said it said that if we have one one to one on one side then the other side is a key in the association relation So what that would tell_us is that satellite is a key here and that does make_sense so we'll only have each satellites home_campus listed once and but a home might have many satellites so the left_hand would not be a key Ok we're getting there We finished the first three and now it's time for sub_classes This is a pretty big one First of all it turns_out that there's well atleast three and three commonly use translations from a sub_class relationship into relations so let_me setup a generic sub classing setup and then we'll talk_about the three different translation and when we might use which one So here's our generic set_up I had to draw it a little funny to fit it in the space S is our super_class and then we have two sub_classes S and S and just as a reminder what this says is that we have object of type S and they have a key attribute call K and another attribute call A and then we have sub_classes of those one called S which will have attributes K and A and will also have an attribute B and the difference to the sub_class as two that will have attributes K and A inherited from its parent and then an additional attribute C Now there are three different_ways as I said atleast three that we could translate this User_Mode_Linux diagram to relations In the first way we have a relation for each of our classes including the sub_classes and the sub_class relations will contain their own attributes plus the key of the super_class In the second case we still have one relation for each of our three classes but in this case the sub_class relation contains all of the attributes from the super_class and then the third case we just use one mega relation for the whole thing that contains all of the super_class and sub_class attributes So let's take a look_at what the three different translations would produce for this particular setup For the first one we'll have our relation for S that's going to have K and A then we'll have one relation for each of the two sub_classes S that will contain the key for the super_class so it will be K and that will still be a key here for the sub_class and B So to find all components of an object of S will get it's A competent from the super_class following that key and then we'll get the B component from this relation and then finally say S will be similar it will have the key for the super_class and attribute C again what this will require is some assembly to get all attributes of the sub_classes Now let's look_at the second translation We still have the super_class and our sub_classes now are going to contain all of the attributes that are relevant so they'll have the key attribute from the super_class They'll also have attribute A and attribute B and then S will similarly have K and then A and then C So in this case what we can see is that when we have an object that's in a sub_class all of it's information will be in it's sub_class relation and the super_class will only be used for objects that are in the super_class but not in any of the sub_classes And the third translation says let's just make a mega relation that contains everything So it will have the key attribute K and A and B and C And in that case we can see that we may have some null_values so if we have an object that's only in sub_class as one for example it would then have a null attribute C If we add an object that was only in the super_class and not in any of the sub_classes it would have a null for both B and C Now as you may have noticed what has already been on the slide the entire time is that the best translation may depend on the properties of the sub_class relationship and if you remember we had the properties overlapping versus disjoint and complete versus incomplete So overlapping meant that we might have objects that were in more_than one sub_class disjoint meant that's not possible complete said that every object that's in the super_class and the super_class is also in at_least one of the sub_classes where incomplete said that there would be some objects or they could be they're in the super_class and not in any sub_classes Sort of alluded to how some of those properties affect the different translations But let's look_at it very specifically Let's_suppose for example that we had a heavily overlapping sub_class relationship In_other_words we have many objects that are in multiple subclasses if that's the case then we might prefer design three because design three captures all of the different attributes in one place so if many objects have all those attributes we might like to just have them together rather_than needing to assemble them from the different pieces On the other_hand let's say that we have have a disjoint and furthermore let's say it's complete So we have every element being in exactly one subclass and there are no elements elements that are in just the super_class So in that case we might like to use design two because design two puts each object in individual subclass relation and furthermore since it's complete we could actually in this particular case get_rid of the first relation We wouldn't need that and so the best design would be number modified to only have the subclass relations So now let's revisit our gigantic example from the User_Mode_Linux modeling video and see how that would be translated to relations So just to remind_you what's going on in this big diagram we have students and students have a student_ID and a name and then some of our students might be foreign students and they have a country Some of our students may be domestic students and that they would have a state and a social_security_number We'll assume that every student is either foreign or domestic and then some of our students are AP students and interestingly they have no attributes but those are the ones who have an association called took with AP courses AP courses have a course number and a title and then when a student takes an AP course there's an association_class that says the year they took it and the grade they got So let's translate this to relations the whole diagram And we're going to use the first translation from our three schemes So that's the one where the sub_class relations contain the super_class keys and then all the specialized attributes So let's start_by generating the relation for the student class and that's straightforward that's just the student_ID and the student name with the student_ID being the key And then we're going to generate one relation for each of our three sub_classes So that would be the foreign students which will take the key then from the student relations so their student_ID and the country that they're from They'll be the domestic students and there we'll have again the student_ID inherited from the super_class and the state they're from and their social_security_number And then we'll also have the AP students and those have none of their own attributes but we can get the key and we do from the super_class So this is just going to be a list of the student ID's who are AP students Now let's keep going So let's take a look_at AP students taking courses So we'll have from the AP class a straight_forward translation to the course number and the title of the course and then finally we're going to have a relation for the fact that a AP student took classes We had some multiplicities on that the first time but neither of them were a zero one or one one so they're not going to be relevant in changing our design So took is going to have the key from both_sides Well we don't see any key in here do we But subclasses always inherit the key from their parents so the key here is implicitly the student_ID and that's what we'll be using here Student ID the key from the right_hand_side the course number that they've taken and then we'll have the two attributes from the association_class the year and the grade So this actually looks_like a pretty good_design Let_me just make a few comments The first one has to do with the AP student relation So if every AP student does have to does have to take at_least one course So if we have for example a one_dot_dot something over there then we could actually eliminate it's relation because every student_ID that appears in AP student will also appear in the Took relation so this one will be redundant in that case So again that could be eliminated based_on the multiplicity though I wouldn't really expect an automatic translator to necessarily figure that one out Another possibility I should mention is that we could have a primary_key specified here for domestic students for social_security numbers since we would expect that to be unique and that would translate to a key here but let_me be clear this would be a separate key It wouldn't be social_security_number and student_ID together but these would be two different keys and in Structured_Query_Language you can actually distinguish between those two but we can't do it the way we've written the relations here And finally let_me_mention again that requirement that we have keys for regular classes if we want to do translation automatically and you might have noticed that for example we certainly didn't have a key here for a foreign student with country being the only attribute but subclasses are also not considered regular classes So subclasses and association classes don't need to have keys and we can still have an automatic translation Logically subclasses are inheriting their keys from their super_class and that one does need to have a key in order to have automatic translation So that was a big one But you'll be glad to know that composition and aggregation is going to be quite quick So here is our example where we have our college class as usual but colleges contain departments and this solid diamond here is the composition operator that says that we have objects from the right_hand_side class that are components of the left_hand_side objects So let's look_at the translation to relations We'll translate the college side as usual with a key attribute college name and the other attributes And then although this is an association we're not going to have a separate class for it we're going to have the right_hand_side class have both the values of the object in that class and the association captured together So we'll have a relation called department and it will have the department name and the building and then it will also include the key of the object it belongs to So in this case that would be a college name Now if you think carefully about it this translation actually makes a lot of sense and is consistent with what we've already done We said when we introduced the notion of composition that we have Effectively have by definition a one_dot_dot one on the left_hand_side of the composition So if we treated this as a regular association used our regular translation and then used our rule that allowed us to get_rid of association relations we'd actually end up at exactly the same design By the way this is a last example where we have a class that's not one of those regular classes that require a key We don't have to have a key for the right_hand_side of a composition in order to have an automatic translation Aggregation by the way was the case where we have the MC diamond and that's implicitly a zero_dot_dot one instead of one_dot_dot one We had an example with apartment buildings I'm not going to bother to give that one again but in that case where we have an aggregation should have in an association All we need is the same design but the ability for that key we're grabbing from the left_hand_side to be null and then everything works out fine So to conclude the data modeling portion of User_Mode_Linux is a popular high_level language for database designs It's graphical and it can be translated automatically to relations as long as every regular class has a key and that typically is the case This video talks_about indexes It's actually a relatively short video about a very_important topic Before I get started however let_me_mention that indexes are also sometimes prefer two as indices those are equivalent I personally prefer using the term indexes The reason indexes are so important is that they are the primary way of getting improved performance out of a database Indexes are a persistent data structure They're stored together with the database itself Now there are many very interesting implementation issues in indexes but in this video and in this course in general we're focusing on the perspective of the user and application So we'll talk_about how applications will use indexes to speed_up performance So let's_suppose we have a very_simple table T that has three columns but we're going to focus_on columns A and columns B And we can see that Column A is a string valued column with animal names and Column B is an integer column Now we're gonna be asking queries that involve conditions over these two columns In order to speed_up those queries if we're concerned about evaluating conditions on column A then we can build an index on that column So we call that an index on column T A What that index allows_us to do and us in this case is actually the query processor is ask questions for example let's ask what tuples have cow in the value of T A If we ask that question of our index that that index will quickly tell_us that tuple and tuple have a value cow without scanning the entire table We could also ask the index what tuples have say value cat And if we ask the index that question it will tell_us tuple and tuple and tuple have the value cat If we're interested in evaluating conditions in column B then we can also build an index on column B For_example now we could ask questions like when is T B equal to the value two We asked the index and the index will tell_us that tuple and tuple have the value two We could also ask for example when the value in T B is less_than six And the index in that case would tell_us that tuple is less_than six two wow most of them three five and seven We could ask an even more_complicated question We could ask when the value for T B is say greater_than four and less_than or equal to eight Again we ask the index and in this case the index would tell_us that it is tuple two and tuple seven in that case Lastly suppose we're interested in having conditions that are on both columns A and B Then we can build an index that is on both columns together Now we could ask questions for example like when is T A equal to cat and T B say greater_than five Do we have any of those Well we have just one of them there That's tuple six We could also have inequalities by the way on the first column So we might ask when is T A less_than say the value D and T B equal to say the value and in that case we'll get the tuple as a result So I think this gives an idea with a simple table of how indexes are used to go directly to the tuples that satisfy conditions rather_than scanning the entire table So that's the main utility of an index Again the tables can be absolutely gigantic in databases and the difference between scanning an entire table to find that tuples that match a condition and locating the tuples more or less immediately using an index can be orders of magnitude in performance difference So really it's quite important to take a look_at the database and build indexes on those attributes that are going to be used frequently in conditions especially conditions that are very selective Now I_mentioned that we're not covering the implementation of indexes in this video but it is important to understand the basic data structures that are used Specifically there are two different structures One of them is balance trees and substantiation of that is typically what's_called a B tree or a B tree And the other is hash tables Now balance trees indexes can be used to help with conditions of the form attribute equals value They can also be used for attribute less_than value for attribute between two values and so on as we have shown earlier Hash tables on the other_hand can only be used for equality conditions So only attribute equal value And if you're familiar with these structures then you'll know why there's the limit on hash tables So balanced tree indexes are certainly more flexible Now there is one small downside For those of you who are familiar with the structures and with the running time the operations on the balance trees tend to be logarithmic in their running time while well designed hash tables can have more or less constant running time Even in large databases logarithmic is okay although when only equality conditions are being used then a hash table index might be preferred Now let's take a look_at a few Structured_Query_Language queries and see how indexes might allow the query execution engine to speed_up processing We'll use our usual student and college database The first one is a very_simple query it's looking for the student with a particular student_ID So if we have an index on the student_ID then again the index will allow the query execution engine to go pretty_much straight to that tuple whereas without an index the entire student table would have to be scanned Now let_me_mention that many database_systems do automatically build indexes on primary keys So it's likely that in an application the student_ID would be declared as a primary_key and there would be an index automatically But it's a good thing to check if this type of query is common And some systems even also build indexes automatically on attributes that are declared as unique As a reminder from the constraint video every table can have one primary_key and then any number of additional keys labeled as unique Now let's take a look_at a slightly more_complicated example Here we're looking for students_whose name is Mary and whose_GPA is greater then and there may be a few of those students So one possibility is that we have an index on the student name and if that is the case expand the query processing can find quickly the tuples whose student name is Mary and then check each one of those to see if the GPA is greater_than Alternatively we might have an index on the GPA In that case the system will use the index to find the students_whose GPA is greater_than and then look to see if their name is Mary Finally it is possible we can have an index on the two attributes together so we can have S name and GPA together and then this index can be used to simultaneously find students that have the name Mary and the GPA greater_than Now I should mention that because this is an inequality condition it is important that the GPA is a tree based index in order to support that evaluation of this condition where the student name is an equality condition so that could be a hash based index or a tree based index Now let's look_at a query that involves a joint We're joining the student and apply tables in order to find the names of the colleges that each student has applied to And we're returning the student name and the college name So let's_suppose for starters that we had an index on the student_ID attribute of the apply relation If that's the case then the query execution engine can scan the student relation and for each student use that SID and quickly find the matching SIDs in the apply relation Alternatively let's_suppose we had an index on the SID attribute of the student relation In that case the system could scan the apply relation and for each student_ID and each apply tuple find the matching student IDs in the student tuple using the index that we have there In some cases it's actually possible to use the two indexes together and make the query run even faster I'm not going to go into detail but indexes often allow relations to be accessed in sorted order of the indexed attributes So suppose we can get the student relation in sorted order and the apply relation in sorted order Then we can kind of do a merge like operation of the two indexes to get the matching student and apply records those whose SIDs are equal If we had additional conditions in the query there might be even more choices of how to use indexes and that gets into the entire area of what is known_as query planning and query optimization And this is actually one of the most exciting and interesting areas of the implementation of database_systems and is what allows_us to have of a declarative query language that's implemented efficiently So indexes seem like great things We just throw some indexes onto our data and all of a sudden our queries run much much faster So there must be some downsides and of course there are Let_me list three of them from sort of least severe to most severe So the first one is that indexes do take up extra space As I_mentioned they are persistent data structures that resides with the data I consider this sort of a marginal downside especially with the cost of disk these days its really not that big of deal to use additional space even to potentially double the size of your database The second downside is the overhead involved in index creation So when a database is loaded if we're going to have indexes those indexes need to be created over the data Or if we add indexes later on they need to be created Index creation can actually be a fairly time consuming operation so I'm going to make this as a medium downside On the other_hand once the index is created all the queries run faster so it's usually worthwhile to do it The last one is the most significant one and that's the issue of index maintenance So the index is a data structure that sits to the side of the database and helps answer conditions When the values in the database change then the index has to be modified to reflect those changes So if the database is modified frequently each of those modifications is going to be significantly slower than if we didn't have indexes So in fact in a database that's modified a whole_bunch and not queried all that often the cost of index maintenance can actually offset the benefits of having the index So it really is a cost benefit trade off to decide when to build indexes So given that we have this cost benefit trade off How do we figure_out which indexes to create when we have a database an applications on that database The benefit of an index first of all on how big the table is since the index helps us find specific portions of the table quickly It depends on the data distributions again because the index helps us find specific data values quickly And finally how often we're going to query the database First of all it's how we're going to update it As I_mentioned every time the database is updated indexes needed to be maintained and that's costly Every time we query the indexes may help us answer our queries more quickly Fortunately over the last decade or so many database system vendors have introduced what's_called a physical design_advisor In this case physical design means determining which indexes to build on a database The input to the design_advisor is the database itself and the workload The workload consists of the sets of queries and updates that are expected to be performed on the database as_well as their frequency Now actually the design_advisor doesn't usually look_at the entire database but rather looks at statistics on the database that describe how large the tables are and their data distributions The output of the design_advisor is a recommended set of indexes to build that will speed_up the overall workload Interestingly physical design advisors rely very heavily on a component of database_systems that already existed actually one of the most_important components of database_systems which is the query optimizer That's the component that takes a query and figures out how to execute it Specifically it'll take statistics on the database the query to be executed or the update_command and the set of indexes that currently exist and it will explore the various ways of actually executing the query which indexes will be used which order things will be done in It estimates the cost of each one and it spits out the estimated best execution plan with the estimated cost So now let's look_at how this component can be used to build a design_advisor Let's just draw the design_advisor around the whole thing here and the input to the design_advisor again are the statistics and the workload and the output is supposed to be the indexes So what the design adviser actually does is it experiments with different set ups of indexes For each set_up of indexes it takes the workload it issues the queries and updates to the query optimizer It doesn't actually run them against the database and see's what cost the query optimizer produces It tries this with different configurations of indexes and then in the end determines those indexes that bring down the cost the most In_other_words it will give you back those indexes where the benefits of having the index outweigh the drawbacks of having that index in terms of the workload and using the costs that were estimated by the query optimizer If you're using a system that doesn't have a design adviser then you'll have to kind of go through this process yourself You'll have to take a look_at the queries and updates that you expect how often you expect them to happen and which indexes will benefit those queries and hopefully won't incur too much overhead when there are updates Just quickly here's the Structured_Query_Language standard for creating indexes All indexes are given names We can create an index on a single attribute We can create an index on several attributes together We can also say that we want our index to enforce a uniqueness constraint so when we add the word unique it sort of adds constraint enforcement It says we're going to check that all values for A are unique using our index and will generate an_error if there are two values that have the same two tuples that have the same value for A and finally we have a command for dropping indexes In summary indexes are really important They're the primary way to get improved performance on a database By building the right indexes over a database for its work flow we can get orders of magnitude performance improvement Although we do have to be careful because there are trade offs in building indexes especially for databases that are modified frequently There are persistent data structure that are stored together with the database and there are many interesting implementation issues but in this video and course we're focusing specifically on the user and application perspective determining which indexes to build and how they will gain performance improvement for us This video introduces the concepts of transactions and interact actions with database_systems Transactions are a very_important concept The concept of transactions is actually motivated by two completely independent concerns One has to do with concurrent access to the database by multiple clients In this video we'll delve into more_detail about the properties of transactions As a reminder transactions are a concept that's been introduced as a solution to both the concurrency control problem and the system failure problem in databases Specifically a transaction is a sequence of operations that are treated as a unit Transactions appear to run in isolation even if many clients are operating_on a database at the same time And further more if there is a system failure in unexpected software hardware or power failure every transactions changes that were issued to the database are either reflected entirely or not at all Every database connoisseur knows that transaction support what are known_as the Atomicity Consistency Isolation Durability properties Although not everybody always remembers what A stands for atomicity C stands for consistency I stands for isolation and D stands for for durability And we're going to talk_about each of these four properties in turn We're going to talk_about isolation first We're going to talk_about durability second then we'll next talk_about atomicity and we'll conclude talking_about consistency So here's the deal with isolation We'll have a whole_bunch of clients operating_on our database and we'd kind of like each client to imagine that they're operating_on their own So as we discussed in the previous_video each client issues to the database system a sequence of transactions So this first client might be issuing first transaction T then T T and so on Over here we have T T T and as a reminder each transaction itself can be a sequence of statements So this might be statement one statement two statement three and so on and then those statements will be treated as a unit So the isolation property is implemented by a very specific formal notion called serializability What serializability says is that the operations within transactions may be interleaved across clients but the execution of those operations must be equivalent to some sequential serial orderOf all the transactions So for our example over here the system itself may execute all of the statements within each transaction and over here concurrently but it has to guarantee that the behavior against the database is equivalent to some sequence in order again So perhaps the equivalent sequential order will be as if the system first did transaction T then may T T and T maybe back to T and So on And again the system has to guarantee that the state of the database at this point in time even if its internally the statements within any of these transactions looks as if these transactions executed in order Now you_might_wonder how the database system could possibly guarantee this level of consistency while still inter leading operation It uses protocols that are based_on locking portions of the database Now we're not going to describe the implementation because implementation aspects are not the focus of this course What you need to know from a user's application perspective is really the properties that are being guaranteed Now with the formal notion of a let's go_back and look_at the examples from the previous_video that motivated the problems we could get into with concurrent access The first one was the example where two separate clients were updating Standford's enrollment Let's just call one of them T It's not a transaction And the other T So when we run thing is against the system and serializability is guaranteed then we will have a behavior that is at_least equivalent to either T_followed_by T or T_followed_by T So in this case when we start with our enrollment of either execution will correctly have a final enrollment of solving our concurrency problems Here's our second example In this case the first client was modifying the major of student in the apply table and the second was modifying the decision And we saw that if we allowed these to run in an interleaved fashion it would be possible that only one of the two changes would be made Again with serializability we're going to get behavior that guarantees it is equivalent to either T and then T or T and then T And in both cases both changes will be reflected in the database which is what we would like The next example was the one where we were looking_at the Apply and the Student table and we were modifying the Apply table based_on the GPA in the Student table and simultaneously modifying that GPA So again if these are issued as two transactions we'll have either T_followed_by T or T_followed_by T Or at_least we will have behavior that is equivalent to that Now this case is a binary_digit interesting because either of these does result in a consistent state of the database In the first case we'll update all the decision records before the GPAs are modified for anyone and in the second case will update the apply records after the GPAs have been modified The interesting thing here is that the order does matter in this case Now the database_systems only guarantees serializability They guarantee that the behavior will be equivalent to some sequential order but they don't guarantee the exact sequential order if the transactions are being issued at the same time So if it's important to get say T before T that would actually have to be coded as part of the application And our last example was the case where we had the Apply table the Archive table and we were moving records from one table to another in one of our clients and the other client was counting the tuples And again so T and T they're issued as transactions The system guarantees that we'll either move all the tuples first and then count them or will count the tuples and then move them Now again here's a case where the order makes a difference but if we care specifically about the order that would have to be coded as part as the application OK so we've finished our first of the four Atomicity Consistency Isolation Durability properties The other three will actually be quite a binary_digit quicker to talk_about Let's talk now about durability And we only need to look_at one client to understand what's going on So let's say that we have our client and the client has issuing a sequence of transactions to the database And each transaction again is a sequence of statements And finally at the end of the transaction there is a commit So what durability guarantees for us is that if there is a system crash after a transaction commits then all effects of that transaction will remain in the database So specifically if at a later point in time after this occurs there's a failure for whatever_reason the client can rest assured that the effects of this transaction are in the database and when the system comes back up they will still be in there So you may be wondering how it's possible to make this guarantee since database_systems move data between disc and memory and a crash could occur at anytime They're actually fairly_complicated protocols that are used and they're based_on the concept of logging But once again we're not gonna talk_about the implementation details What's important from the user or application perspective is the properties that are guaranteed properties down Now let's talk_about atomicity again we'll only look_at one client who's issuing a sequence of transactions to the database And let's look_at transaction T which itself is a sequence of statements followed_by commit The case that atomicity deals with is the case where there's actually a crash or a failure during the execution of the transaction before it's been committed What the property tells_us is that even in the presence of system crashes every transaction is always executed either all or nothing on the database So in other_words if we have each of our transactions running it's not possible in a system crash to say have executed on the database a couple of statements but not the rest of the transaction Now once again you might be wondering how this is implemented It also uses a log in mechanism and specifically when the system recovers from a crash there is a process by which partial effects of transactions that were underway at the time of the crash are undone Now applications do need to be somewhat aware of this process So when an application submits a transaction to the database it's possible that it will get back an_error because there was in fact a crash during the execution of the transaction and then the system is restarted In that case the application does have the guarantee that none of the effects of the transaction were reflected in the database but it will need to restart the transaction Now let's come_back to the fact that the system will undo partial effects of a transaction to guarantee the atomicity property that each transaction is executed in an all or nothing fashion So this concept of undoing partial Full effects of the transaction is known_as transaction roll back or transaction abort And the reason I'm mentioning it here is that although it is the implementation mechanisms for Atima City it's also an operation that's exposed by the database in an application would like to use it Specifically a transaction rollback can be initiated by the system in the case of an_error or a crash recovery But it also can be client initiated And let_me give a little example where a client might write code that takes advantage of the operation So here is some toy application code In this code the client begins a transaction it asks the Database user for some input It performs some Structured_Query_Language commands Maybe some modifications to the database based_on the input from the user It confirms that the user likes the results of those modifications And if the user says okay then the transaction is committed and we get an atomic execution of this behavior But if the user doesn't say okay then the transaction is rolled back and automatically these Structured_Query_Language commands that were executed are undone and that frees the application from writing the code that undoes those commands explicitly So it can actually be quite a useful feature for clients to use But clients do need to be very_careful because this rollback command only undoes effects on the data itself in the database So if perhaps in this code the system was also modifying some variables or even worse say delivering cash out of an Asynchronous Transfer Mode machine the rollback command is not going to undo those It's not gonna modify variables and it's certainly not going to pull that cash back into the Asynchronous Transfer Mode So there actually is another issue with this particular client interaction that I_am going to put a frownie face here It was a nice simple example of how rollback can be helpful But one thing that happens in this example is that we begin a transaction and then we wait for the user to do something And we actually wait for the user back here So experienced database application developers will tell you to never hold open a transaction and then wait for arbitrary amounts of time The reason is that transactions do use this locking mechanism I alluded to earlier so when a transaction is running it may be blocking other portions blocking other clients from portions of the database If the user happened to go out for a cup of coffee or is going to come_back in a week we certainly don't want to leave the database locked up for an entire week So again and a general rule of thumb is that transactions should be constructed in a fashion that we know they are going to run to completion fairly quickly Finally let's talk_about consistency The consistency property talks_about how transactions interact with the integrity_constraints that may hold on a database As a reminder and integrity constraint is a specification of which database states are legal Transactions are actually very helpful in the management of constraints Specifically when we have multiple clients interacting with the database in an interleaved fashion we can have a setup where each client can assume that when it begins it operates on a database that satisfies all integrity_constraints Then each transaction then sorry each client must guarantee that all constraints hold when the transaction ends and that's typically guaranteed by the constraint enforcement sub system Now with that guarantee since we have serialized ability of transactions that guaranteesthat constraints always hold Specifically the behavior of the database is some sequential order of the transactions We know and we can assume at the start of the transaction the constraints hold And then we guarantee they hold at the end And since the behaviors equivalent to a sequential order then the next transaction can assume the constraints hold and so on In conclusion transaction are a very_powerful concept They give a solution for both concurrency control and system failure management and databases They provide formally understood properties of atomicity consistency isolation and durability In the next_video we are going to focus more on the isolation property We're going to see that in some cases we may want to relax the notion of isolation while still providing properties that are sufficient for applications in certain circumstances In this final video about transactions we'll focus_on the concept of isolation_levels As a reminder transactions are a solution for both the concurrency control and system failure problem in databases A transaction is a sequence of one or more operations that's treated as a unit Transactions appear to run in isolation and if the system fails each transaction changes are reflected either entirely or not at all On this video we are going to focus_on the isolation portion of transactions As a reminder we can have multiple clients operating_on the same database And each client will submit a sequence of transactions So we have this client with T T Here we have T T T and so_forth And each transaction itself is a sequence of statements Serializeability says that it's okay for the system to inter leave the execution of these statements with the statements that are being performed by other clients however the behavior against the database must be equivalent to the transactions themselves executing in some serial order For_example in this case the system may enforce behavior that's equivalent to say doing transaction T first Then maybe T T and then T and so on Serializability give_us understandable behavior and consistency but it does have some overhead involved in the locking protocols that are used and it does reduce concurrency As a result of the overhead and reduced concurrency systems do offer weaker isolation_levels In the Structured_Query_Language standard there are three levels read_uncommitted read committed and repeatable_read And these isolation_levels have lower overhead and allow higher concurrency but of course at a cost which is lower consistency guarantees I've listed the three alternative isolation_levels from the weaker to the stronger and to complete the picture at the bottom we have a fourth one which is serializable which is what we've been talking_about already Before we proceed to learn_about the different isolation let_me_mention a couple of things First of all the Isolation level is per a transaction So each client could set different isolation_levels for each of its transactions if it wishes Second of all isolation_levels are in the eye of the beholder and let_me show you what I_mean by that So each client submits transaction to the database and might set the isolation_level for that transaction That isolation_level only affects that transaction itself It does not affect the behavior of any other transactions that are running concurrently So for example our client on the left might set its transaction to be a repeatable_read while our client on the right will set it's transaction to read_uncommitted and those properties will be guaranteed for each of those transactions and won't affect the other By the way the isolation_levels really are specific to reads They specify what values might be seen in the transaction as we'll see when we get into the details So let's start_by defining a concept called dirty_reads A data item in the database is dirty if it's been written by a transaction that has not yet committed So for example here are two transactions I'll call them T and T and by the way throughout the rest of this video I'm going to put transactions in boxes and you can assume implicitly that there is a commit at the end of each box I'm not going to write it each time So our first transaction is updating Standford's enrollment adding to it and our second transaction is reading the average enrollment in the college table We're using our usual database of students_applying to colleges So after this enrollment Standford's enrollment has added to it but before the transaction commits at that point in time the value is what's_known_as dirty If our second transaction here reads this value then it might be reading a value that never actually exists in the database And why is that because before this transaction commits there could be a system failure and the transaction could be rolled back as we described before and all of it's changes undone Meanwhile however the second transaction may have read that value before it was undone Here's another example now we have three transactions T T T our first transaction is modifying the GPA of student's who's high_school_size is sufficiently large Our second transaction is finding the GPA of student number And our third transaction is modifying the high_school_size of student So if this GPA here in transaction T is read before the commit of transaction T then that would be a dirty value for the GPA Again because of this first transaction doesn't commit then that value will be rolled back There's a second case where we might have dirty data read in this trio of transactions and that's the size_high_school here Because notice that here we're modifying a high_school_size so if this size of high_school is read before the commit point of the third transaction that would also be a dirty data item One clarification about dirty_reads is that there is no such thing as a dirty read within the same transaction In T for example after we've modified the size_high_school we might read the size_high_school later in the same transaction and that's not considered a dirty read So a read is only dirty when it reads a uncommitted value that was modified by a different transaction So here's our first isolation_level and it's our weakest one It's called Read I'm Committed and what is says is that a transaction that has this isolation_level may perform dirty_reads It may read values that have been modified by a different transaction and not yet committed So lets take a look_at an example It's our same example We've dropped the third transaction so our first transaction is modifying GPAs in the student table and our second transaction is reading average of those GPAs So if these transactions are serializable then it'll be the behavior's guaranteed to be equivalent to either T_followed_by T or T_followed_by T So either the second transaction will see all the GPAs before they were updated or it will see all the GPAs after they were updated As a reminder we don't know which order these will occur in Only that the behavior will be equivalent to one of those orders Now let's_suppose we add to our second transaction a specification that it has isolation_level read_uncommitted And by the way this very long sentence is how we specify the isolation_level in the Structured_Query_Language standard Now when we don't specify an isolation_level as we haven't here the default is serializable Although in most of our examples it won't actually matter what the first transaction's isolation_level is as we'll see We're going to be focusing on the data that's read in the second transaction and typically written in the first transaction Okay so let's see what's going on here Again this is T and T and our first transaction is updating the GPAs And now we've said in our second that it's okay for this average to read dirty values in other_words to see uncommitted GPA modifications In that case as the average is computed it could be computed right in the middle of the set of modifications being performed by T In that case we certainly don't have serializable behavior We don't have T_followed_by T since T is reading some values that are in the middle of T and similarly we don't have T_followed_by T It might be that for our particular application we just don't care that much about having exact consistency It may be that we don't mind if our average is computed with some old values and some new values we might not even mind if we compute in our average an increased GPA that ends up being undone when a transaction rolls back So if we're just looking for a rough approximate GPA we can use this isolation_level and we'll have increased concurrency decreased overhead better performance overall with the understanding that it will have reduced consistency guarantees Let's go one step up to the next isolation_level which is called read committed As you can probably guess this one specifies that transactions may not perform dirty_reads They may only read data values whose updates by other transactions have been committed to the database Now this isolation_level is stronger but it still doesn't guarantee global serializability Let's take a look through an example Our first transaction T is the same one modifying the GPA for students from large high_schools Our second transaction is the one where we are going reset the isolation_level In this case to read committed And it is going to perform two statements One of them is going to read the average_GPA from the student table and the other is going to read the maximum GPA from the student table So let's look_at one behavior that's consistent with isolation_level but we will see is not serializable Let's_suppose that this average_GPA is read before transaction T but the max GPA is computed after transaction T So the average will take will not take into account the increases but the max will take account will take into account the increases So let's see if this is equivalent to any serial order Is it equivalent to T_followed_by T Well it's certainly not because T 's first statement is reading the state of the table before T and not the state of the table afterward although C second statement is reading the state of the table afterward Similarly it's not equivalent to T_followed_by T because T is reading in its second statement the state of the database after T So there's_no equivalent serial order But again perhaps that's not needed for the particular application And by using Read Committed we do get somewhat more performance then we would have if we were serializable Our_next isolation_level is called Repeatable Read And it's our strongest one before we get to Serializable In Repeatable Read a transaction may not perform dirty_reads just like in read committed And furthermore there is an additional constraint that if an item is read multiple_times it can't change value You_might remember in our previous example we read the GPA multiple_times and it did change value So if we were using Repeatable Read for the consistency level there then the behavior that I described couldn't occur So even with this stronger condition we still don't have a guarantee of global serializability and we'll again see that through an example Our examples are getting a little_more complicated here So we have our two transactions T T our first transaction is still modifying the GPA I took away the condition about the high_school_size just to keep things simple and our second statement in our first transaction is modifying the high_school_size of the student with ID So we first modified GPA's and then a high_school_size In our second transaction and that's the one we're setting as Repeatable read we are going to read the average_GPA as we usually do and this time we are going to read the average of the high_school sizes Incidentally our first transaction is serializable as they always are by default Let's look_at a behavior where the first statement reading the average_GPA is executed before transaction T or sees the values before T while our second statement the high_school_size sees the values after transaction T So let's check our conditions We are not performing dirty_reads because the first read here is of the committed value before T and the second read is the committed value after T and furthermore any items that are read multiple_times have not had their value changed because we are actually not reading any values multiple_times So the execution of the first statement here before T and the second one after is legal in the repeatable_read isolation_level Yet we're still not serializable We're not equivalent to T before T because again this Statement of T is going sorry the first statement of T is going before T or seeing the state before T and we're not equivalent to T_followed_by T because the second statement of T is seeing the state after T Now there is another situation with repeatable_read that's quite important to understand We said that a transaction can't perform dirty_reads and it can't We also said that when an item that's read multiple_times can't change value But the fact is that Repeatable Read does allow a relation to change value if it's read multiple_times through what's_known_as phantom_tuples Let_me explain through an example Let's_suppose our first transaction inserts a hundred new students into the database And that's run concurrently with our second transaction which is right at the repeatable_read isolation_level and now we're just going to read the average_GPA and we're going to follow that with the max GPA similar to one of our earlier examples Now repeatable_read actually does allow behavior where this average is computed before T and this max is computed at after T So the justification behind that is pretty_much that when we do the second read of the GPA the tuples that we're reading for a second time do still have the same value So we are reading those some new tuples that were inserted and in fact if this max were an average instead of max we might get different answers for the average even with Repeatable Read at the isolation_level But that's what it allows and these hundred tuples here are what are know as the phantom_tuples They sort of emerged during execution out of nowhere Now I would have to say that my opinion is that this behavior within the repeatable_read isolation_level although it's part of the standard is really in effect of the way repeatable_read is implemented using Locks When a value is read once it's locked and can't be modified but when we insert new tuples they aren't inserted with locks so they can read in a second read of the same relation Don't worry_about the implementation details but do worry_about phantom_tuples because if you're using the repeatable_read isolation_level you do need to know that insertions can be made by another transaction even between two entire readings of a table Now on the other_hand if what we do in our first transaction is delete the hundred tuples instead of insert them in that case we actually can not get the behavior where the first statement is before and the second statement is after Because once these the average value has been read of this GPA this deletion will not be allowed because again kind of an implementation but those values are locked And so in this case the second read of that same relation wouldn't be allowed So in summary we may have phantom_tuples up here between two reads of the same relation in a repeatable_read transaction but we won't have tuples disappear from the relation in between two reads of it So that completes our three isolations levels in addition to serializable we had at the weakest read_uncommitted then read committed and then repeatable_read I did want to mention that we can also set transactions to be read only That's sort of orthogonal to setting the isolation_level what it does is it helps the system optimize performance So for example in our transaction where we were just reading the average_GPA and the max GPA we can set an isolation_level and then we can also tell the system that it's going to be a read only transaction That_means that we are not going to perform any modifications to the database within the transaction The system can use that as a hint to figure_out his protocols to guarantee the right isolation_level but it might not have as much overhead as if the transaction had the possibility of performing modifications as_well as performing reads OK so the behavior of transactions can be rather confusing and it's very_important to get it right or surprising things might happen But I think we can summarize it pretty well with this table here We're going here from the weakest to the strongest and we can classify the behavior of transactions based_on again what_happens with reading Can they read_uncommitted values Can they have non repeatable reads where we read a value and then read a different one later in the same transaction and can there be phantom_tuples inserted during the transaction If we set our isolation_level to serializable then we cannot have dirty_reads we cannot have non repeatable reads and we cannot have phantoms If we go one step weaker for a little_more performance and use repeatable_read then we still won't have dirty_reads we still won't have non repeatable reads but we might have phantom_tuples Moving up with Read Committed we still won't have dirty_reads but we might have non repeatable reads So we might read a value that's committed both times we read it however a transaction wrote the value in between those two reads so it's different each time and we may have phantoms as_well Finally read_uncommitted is the absolute weakest not many guarantees at all We might have dirty_reads we might have not repeatable reads and we might have phantoms So to wrap_up transactions completely the standard default is serializable behavior and we specified exactly what that means Weaker isolation_levels allow_us to increase concurrency decrease overhead so overhaul will get an increased performance but we have weaker consistency guarantees I should mention that some prominent database_systems have actually chosen to have repeatable_read as their default Oracle and MySQL are examples of that So in those systems it's assumed that most applications will be willing to sacrifice a little_binary_digit inconsistency in order to get higher performance And finally the isolation_level is set for each transaction and is in the eye of the beholder meaning that that transaction's reads must conform to its_own isolation_level but won't affect any concurrent transaction's isolation_level I_hope I've gotten the point across that transactions are one of the most_important concepts in database_systems They are what allow multiple clients maybe thousands tens of thousands of clients to operate_on a data base all at the same time without concern that the actions they perform on the data will affect each other in unpredictable ways And furthermore transactions are what allow database_systems to recover when there's an unexpected crash into a consistent space In this video we'll be introducing the concepts of constraints and triggers Later videos will have demos We're considering constraints and triggers in the context of relational_databases The Structured_Query_Language standard does include standardized constructs for constraints and triggers although this is an area where deployed systems do very considerably in how much they cover and how they cover it Constraints are also known_as an integrity_constraints and they constrain the allowable states of the database Triggers are a little different Triggers monitor changes to the database and when they're activated they check conditions over the data and possibly automatically initiate actions So we can think of constraints as a sort of static concept over the allowable states where triggers are more dynamic Let's talk a little_more about integrity_constraints and then we'll move to triggers So the idea of integrity_constraints is that we want to impose restrictions on the allowed data of the data base Now when we create a schema we say the types of the attributes we're already imposing structural and type restrictions But integrity_constraints tend to be more semantic they capture restrictions that have to do with the application So let's look_at a bunch of examples And these are in the context of our students and college database So a simple example might say that when we have a GPA value the GPA must be say greater_than zero and less_than or equal to Another example might say that when we have the enrollment for our colleges the enrollment must be less_than say fifty thousand Well actually for some universities it might be more like We might have the decision attribute in our application table is either the value yes or Y for yes or the value N for no or maybe null is allowed that could be a constraint Again each of these are constraining the data that could be in the database beyond the type structure that's already been defined Here's a little_more complicated one Maybe we'll want a constraint that says No decisions have been made on Computer_Science applications So if our major equals Computer_Science then this is a sort of logical implication then our decision is still equal to null Or here's an even more_complicated one Let's say that we want to enforce that students_who come from small high_schools are not admitted to super large colleges because it just wouldn't be a good idea for them We might say if the size of the high_school is less_than two hundred then if they're admitted well let's say they're not admitted to a college where the enrollment is greater_than let's say Of_course we wouldn't do that in reality but this just gives you an idea of the fairly_complicated expressions we can write as constraints to limit what the allowable data is in the database So why do we want to use constraints Well there's several reasons Actually one very practical reason is just to catch data entry errors So if we have constraints that just say that the values of the data are in the reasonable range for example our GPA's or our enrollments then if somebody tries to enter data that violates the constraints they probably were just making a mistake it was probably an_error and that can be caught automatically by the constraint enforcement system in the database system So a similar example is correctness criteria So data entry errors would be typically for inserts where correctness criteria might be for updates So if we're modifying the database for example we updated GPA or an enrollment if we're checking our constraints that will make_sure that our updates are correct they don't have errors Another use of constraints is to enforce consistency So we might have copies of data in the database in different places or some data that relies on other data and so when we have of that situation consistency we could have constraints that specify the consistency requirements and are checked automatically and finally a very different use of constraint is to tell the system about the data So specifically we might have key constraints that say values are unique or we might again have consistency restraints that the system can use to both store the data in a certain fashion that made it more efficient and also for how it processes queries query processing So let_me give a broad classification of the types of integrity_constraints that are supported in database_systems And this roughly from sort of simplest to the most complicated So a common type of constraint is simply a non null constraint The values cannot take on null that values cannot take on null A second type is key constraints we've_seen those already So a column or set of columns must have unique values in each tuple A very_important type of constraint is called referential_integrity and we're actually going to treat that in it's own video It's often some times known_as foreign_key constraints because it is a very frequently used and important type Next we have what are known_as attribute based constraints and these are constraints that are specified along with an attribute constraining the value of the particular attribute A similar type of constraint is a tuple based constraint but it's associated_with each tuple so it can constrain how the values in a tuple but in different attributes relate to each other And finally there's a notion called general assertions where we pretty_much use the entire Structured_Query_Language query language to specify constraints over the database across tables and within tables Now let's talk_about how constraints are declared and enforced There's two different times that we can declare constraints One is with the original schema So at the time we create tables we can associate constraints with those tables or with the entire database If we do it this way then the constraints are typically checked after bulk loading So as we discussed in previous_videos the way a database is often set_up is the scheme as declared and the initial set of data may be enough file and then it's both loaded in the database so we did clear constraints then after the data is loaded the system will check the constraints and if they don't hold an_error will be raised Now another possibility is that we decided once a database is already in operation that we have some constraints we'd_like to enforce Maybe the application is change or maybe we just realize that there is certain constraints on the data in that case what_happens is the constraint is checked on the current state of the database at the time it's declared Now this talks_about checking constraints on a single state of the database but of course if the database is modified we have to continue to check constraints So the idea is that once a constraint is in place and if the holds on the database then every time the database is modified the constraints to be checked Now of course what we really only want to check is dangerous modifications so we have a constraint on the GPA we don't need to check changes to the enrollments If we have constraints on one table we certainly don't need to check updates on another table or modifications So part of a good constraint_checking system will only check constraints after those modifications that can possibly cause the constraint to become violated On the other_hand the system does have to insure that after every modification the constraint holds There's also another concept known_as deferred constraint_checking And deferred constraint_checking says that and we might want to do a whole_bunch of modifications that during the modifications violate the constraint but once we are done with all of them then the constraints will hold again And in that case instead of checking after every modification what we actually check is after every transaction So we'll talk_about transactions in a separate video But the concept of transactions is that you can group a bunch of modifications together and they'll be executed as a unit And that unit is used for other purposes as_well for managing concurrency and for recovery but in terms of constraints it can also be the unit of modification that's used for constraints Check it Again if we perform a modification that violates the constraint typically the system will raise an_error and will undo the modification that violated the constraint so that the data base stays in a state that's consistent with respect to its constraints Now let's introduce triggers As I_mentioned earlier triggers are a more dynamic concept than constraints Constraints talk_about each state of the database where triggers talk_about how the database evolves And they can in fact themselves trigger action that cause the database to further evolve Triggers are sometimes known_as event condition action roles because the basic structure of a trigger says when some event occurs and that's typically a modification to the database of some type check a condition over the database Sometimes this condition will be checking the violation of a constraint but it can be more general than that And if the condition if the condition is true then perform an action So let's look_at some examples of what we might use triggers for in our college application So we might have a trigger that says if the enrollment is modified to exceed say thirty five thousand then let's initiate an action and now this is not a logical implication but a triggering of an action that rejects all applications So we can code that in a trigger and it's a little different and a constraint We might write another trigger that says if we insert an application that has a with a GPA say greater_than then again this is not implication but triggering an action we might accept automatically that applicant As another example let's say that we insert a or let's say we update the size_high_school to be greater_than say seven thousand seems pretty unlikely then that's probably an_error and we could change the value say to 'Wrong' Actually one thing that trigger can often doing this action is simply raise an_error and we can see this last one is effectively enforcing a constraint that the size school should be less_than or equal to We saw a number of reasons that we might want to use constraints now let's talk_about why we might want to use triggers Actually the original motivation for triggers was to move logic that was appearing in applications into the database system itself For_example if our application is doing all of the work to monitor each change to the database and that make additional changes based_on that monitoring why not put that functionality inside the database system in the form of triggers That makes it more modular and it insures that all the monitoring automatically occurs no matter which application is running on the database In addition to moving monitoring logic inside the database system a very common use of triggers is simply to enforce constraints Now you_might_wonder why would people not simply use the constraint system instead of writing triggers The reality is that even_though the Structured_Query_Language standard is very expressive in terms of constraints especially when you consider the general assertion feature no database system implements the entire standard Most of the constraint_checking features are somewhat limited On the other_hand the trigger features are quite expressive So there's a number of constraints a large class of constraints that can't be expressed using the constraint feature but can be expressed using triggers So expressiveness is one of the reasons The other is that using triggers you can not only monitor constraints but you can actually have constraint repair logic So when you use constraint systems except for one specific case having to do with referential_integrity that we'll see When you use constraint systems typically if the constraint is violated an_error is raised on the other_hand if you use a trigger trigger can detect the constraint is violated and it can launch an action that fixes the constraint so that's a good use of triggers Here's a quick preview of what triggers look like in Structured_Query_Language we'll go into much more_detail in a later video as_well as have demonstrations of triggers in a running system Again triggers are known_as unconditional action rules and we can see here the specification of events which are modifications to the database We can see here the condition that's written in a Structured_Query_Language like language and finally if the condition is true the action is executed To conclude constraints and triggers are about monitoring the state of the database Constraints specify allowable database states while triggers can check conditions and automatically initiate actions In later_videos we'll go into substantially more_detail and we'll have some demonstrations of the constraint and trigger features in deployed database_systems in this video we'll be giving a demo of constraint of several types as a reminder constraints also known_as integrity_constraints impose restrictions on the allowable states of a database beyond those that are imposed by the schema that's been defined and the types of the attributes We have a number of different types of constraints we have non null constraints which specified that a particular attribute cannot have no values we have key constraints that talk_about uniqueness in columns or sets of columns we have attribute base and tuple base constraints which specify a restrictions on the values and attributes or the values across attributes in particular tuples and finally we have general insertions which are quite_powerful they allow you to specify constraints across an entire database As we'll see in the demo not all of these constraint types are fully implemented There are some limits on the attribute base and tuple base constraints in systems as compared to the Structured_Query_Language standard and general assertions have not been implemented yet in any database system but we will give examples what they look like had they been implemented A very_important type of constraint is referential_integrity or foreign_key constraints and those will be covered in the next_video For a demonstration of constraints we'll be returning to the same simple_college_admissions database that we use for our Structured_Query_Language demos we have three tables one with a few colleges one with a number of students and finally a table that has information_about students_applying to colleges Let's start_by creating a table with a non null constraint So non null is a pretty_simple type of constraint If we decide that our GPA values in our database must not take on the null value when we create the table we just add the key words not null in the declaration with that attribute Let's run the creation of the table let_me_mention right up_front we're going to be seeing a lot of this word affected this misspelling here which gets on my nerve but I'm not going to mention it again Okay so let's do some insertions and updates just to experiment with a not null constraint We'll start_by asserting three tuples the first one has no null_values at all the second one has a null value for the high_school_size which should be allowed and the third one has a null value for the GPA which should not be allowed Let's run these three insert commands together and we see in fact the first two succeeded where the third one generated an_error If we go and look_at the table we'll see that indeed we got our first two tuples including the null for the high_school_size but there was no third tuple inserted Now we'll try a couple of update commands Both of them are going to set the GPA to null the first one for the student with ID and the second for the student with ID If we look_at our data we see that we do have a student with ID so when we try to update that GPA to null we should get an_error But we don't have a student whose ID is so even_though we're going to run a command that tries to set GPAs to null because there's_no matching data no data will attempt to be updated and we won't get an_error Let's run the query the two updates and we see indeed that the first one caused the constraint violation and the second one did not Now let's take a look_at key constraints I've dropped the previous version of the student table and now we will create a new one where we're declaring the student_ID to be what's_called a primary_key As you may remember a key constraint specifies that the values in the column that's declared as a key must be unique So let's go_ahead and create the table and now let's experiment with inserting and updating some data We'll attempt to insert three students first one Amy second Bob and third one Craig Since the third insert will generate a key violation because there will be two_copies of in the ID column that one should generate an_error We run the queries the inserts and indeed the first two are fine and the third one has a key error If we go and look_at the data itself we'll see that the first two are inserted and the third one wasn't Now let's take a look_at updates The first update is very_simple It tries to set Bob's ID to Since Amy already has ID three that should generate and error and when we run the update_command indeed it does Now we're going to do something a little_binary_digit trickier We're gonna run an update_command that subtracts from each student_ID now you_might_wonder why did I choose let's take a look if we subtract from Bob's ID two three four will turn into one two three and will have a key violation on the other_hand if the command first updates Amy's student_ID to then we won't have a key violation when Bob's in turned into two three into one two three So whether we get a key violation in this case could depend on what order the system chooses to execute the update So let's just run it and let's see what_happens Well things look good We didn't get an_error Let's go look back and refresh the table and we see indeed that both of the update succeeded without a violation so now let's set the as the student_ID back to what they were by adding let's run it see what_happens well this time we got an_error So we got a constraint violation error a key violation and nothing was updated That's presumably because the system again updated Amy's ID first and that generated an_error with the one two three for Amy So this sort of demonstrates it one it can be pretty tricky when key violations or other types of constraint violations are detected and when they aren't now we did mention earlier that there's a notion of for constraint_checking so if an application have knowledge that it would rather have constraints checked after a bunch of changes rather_than in the middle the for constraint_checking can be use for that purpose and this demo we're doing immediate constraint_checking You_might have noticed in the previous example that I use the term primary_key when I declared the student_ID as a key In the Structured_Query_Language standard and in every database system only one primary_key is allowed per table that's why it's called primary and often the table will be organized based_on that key making it efficient to do look ups on that for values for that particular key So if we decided we wanted to declare two primary keys in our table the student_ID and the student name we would get an_error now that's not to say we're not allow to have multiple keys in a table in fact we can have as many as we want only one of them can be declared as primary but we can declare any number of attributes or sets of attributes to be unique and that's again declaring a key constraint it says we can only have one we must have unique values in that column so let's create our table with the student name now also a key along with the student_ID and we'll do a few updates just to check that So we'll attempt to insert five students Amy Bob so far so good When we try Amy we should get an_error because we have now declared that the name must be a key as_well as the student_ID so we won't be allowed to have Amy door should be good Amy should again generate an_error we ran the query and indeed we get two errors So far we seen only keys that are one attribute but as you know we can have keys that spans several attributes that's not to say that each attribute is the key individually but rather the combination of values for all of the attributes must be unique in each tuple So let's_suppose that our college name is not unique on its_own but college name and state together are expected to be unique now syntactically we can't put the primary_key statement with the attribute anymore because it involves multiple attributes so the syntax is to list the attributes first in the Create Table command then use the keywords primary_key and put the list of attributes constituting the key in parentheses So let's create the table Now let's insert some data I've tried to pick a college name that's kind of generic Mason I don't know if I've succeeded but we'll try to answer the Mason college in California a Mason college in New York those should succeed because the two columns together need to be unique but not the individual column and then we should get an_error when we try to generate a third tuple with Mason California We run the query we run the inserts and indeed we do Now lets use multi attribute keys to declare some interesting constraints We're going to create our apply table and we're going to have two key constraints The first one says that the combination of student_ID and college name must be unique in each tuple What that's really saying is that each student can apply to each college only one time We're also going to say that the combination of student_ID and major must be unique in each tuple That_means that each student can apply to each major only once Now a student can still apply to several colleges and several majors but only one time for each So let's create the table and then let's try inserting some data We'll insert quite a number of tuples and lets take a look_at what we expect to happen Our first tuple says applies to Stanford and Computer_Science and then also applies to Berkeley and EE no problem tries to apply again to Stanford and that should be an_error because that's the second instance of the combination of Stanford On the other_hand should be able to apply to Stanford comes back and wants to go to Massachusetts_Institute of Technology but tries once again to major in EE That should generate an_error because the combination of and EE already appears in our second tuple And finally applies to Massachusetts_Institute of Technology but in biology and that should work just fine So we'll run the query and we'll find indeed the first two tuples and the fourth and the sixth were fine but the third tuple generated an_error because of the second application to Stanford and the fifth because of the second application to EE Let's go take a look_at the data And here we see in the apply relation that we did indeed insert the four tuples but not the two tuples that generated the key error Now we'll try a sneaky update_command We'll try to take our fourth tuple and we'll identify it by having the college name equal to Massachusetts_Institute of Technology and we'll try to be sneaky and change the the biology major to Computer_Science That will then violate the constraint of the uniqueness of Computer_Science so if all goes well that update will be disallow here is the update_command setting the major to Computer_Science with the college name is Massachusetts_Institute of Technology rerun the command and indeed it generates an_error The last thing we'll show in this example is how NULL values work with keys so we'll try to insert two tuples again using where both the college name and the major are null So as a reminder the first and second attributes need to be unique in the first and third attributes need to be unique so if NULLs counts for keys so it will generate an_error what we'll see is that we actually don't get an_error and we in fact do have the data in the table with the NULL values so the Structured_Query_Language standard and most database_systems do allow repeated NULL values even in column that are declared as unique for primary_key declared columns most systems though not all do not permit repeated NULL values in them That completes our demonstration of key constraints now let's look_at attribute base check constraints Lets create our table again with four students and this time we'll add two constraints to two of the attributes For the GPA we're going to add the keyword check and a condition that looks kinda like the where clause in the Structured_Query_Language query This condition specifies that GPAs must be less_than or equal to and greater_than zero We'll also put a check constraint on the high_school_size saying that the size of the high_school must be less_than five thousand So these are examples of sort of sanity checks that are mostly use for catching data entry errors saying that the attribute values must be within the expected range Lets create the table and now we'll take a look_at some data This time we'll insert two tuples It will be pretty easy to see how these constraints work The first one inserts Amy with a reasonable GPA and a reasonable high_school_size the second one inserts Bob with a reasonable high_school_size but his GPA looks a little out of whack We run the query and the first row is inserted but the second one isn't We take a look_at the data and we see that Amy has been inserted Now to test the constraints on the size of high_school we'll try to run an update_command that multiplies all high_school sizes by six Here's the command and when we run it we get an_error So attribute based constraints allow_us to associate a condition with a specific attribute and that condition is checked whenever we insert a tuple or update a tuple to make_sure that all of the values in that attribute satisfy the constraint A slightly more general notion is tuple based constraints Tuple based constraints are also checked when ever a tuple is inserted or updated but they're allowed to talk_about relationships between different values in each tuple And because we don't associate them with a specific attribute the check itself is put at the end of the declaration of of the table So we start_by declaring all of the attributes and then afterwards we put the keyword check again and then the condition inside parentheses Now this condition may look_at first a little_binary_digit odd to you It says that for each apply tuple either the decision is null or the college name is not Stanford or the major is not Computer_Science Why don't you think_about that for a second and think_about what it might be saying Now if you're good in Boolean Logic you might have written this down using logical expressions and use some of De Morgan's laws and turned your or's and not's into implications If not I'll just tell you that what this is saying is that there are no people who have applied to Stanford and been admitted to Computer_Science at Stanford Specifically either they haven't been admitted or the college is not Stanford or the major is not Computer_Science We'll create the table and then we'll experiment with some data First we'll try to insert three tuples The first one has a student applying to Stanford Computer_Science but not being admitted second they apply to Computer_Science but it says Massachusetts_Institute of Technology and they are admitted and then finally will generate a constraint violation by having the student apply to Stanford Computer_Science and be admitted We run the query and as expected the first two tuples are inserted and the third generates a violation Now let's try some update statements So we have a student who applied to Stanford Computer_Science and was not admitted And with a student who applied to MITCS and was admitted So first we'll try to take that Standford student and change the decision to yes that's not going to work So then we'll try taking the students admission to Massachusetts_Institute of Technology and converting that to be an admission to Stanford and that shouldn't work either We try all of those and neither of them succeed and both cases are tuple based constraintless check and the check condition was violated Before I do my last set of examples I did want to explain one thing in case you're trying these constraints at home The constraints that I've shown so far were perfectly well in SQLite and in post risks In my Structured_Query_Language as of the time of this video the check constraints both the attribute based and tuple based check constraints are accepted syntactically sp by the MySQL system but they're not enforced So it can be a binary_digit deceptive because you may create the tables exactly as I've done in my Structured_Query_Language but then you will be allowed to insert enough data and violate the constraints So again I recommend for trying check constraints for now SQLite or Postgres If you've been a shrewd observer of what we've done so far it might have occurred to you that we had some redundancy Specifically the attribute base check constraints that we' ve showed can be used to enforce some other types of constraints Very_specifically if we want to have a not null constraint we can just write not null That's a built in type of constraint But that's equivalent to adding an attribute based check constraint that for the GPA for example checks that the GPA is not null As a reminder is not null is a key word in the Structured_Query_Language language Let's create this table and let's try to insert a tuple with a null value We have student Amy again with a null GPA and that generates an_error A little_more challenging and interesting is to try to implement key constraints using attribute based check constraints So here's an attempt at doing so Let's just consider a very_simple table We'll call it T and it will have one attribute A And we'll try to write a check constraint that specifies that A is a key for T So here is my attempt at doing so I declare the attribute and then in at my check I say that the value of A is not in select A from T In_other_words the value of that or say attempting to insert or update is unique in table T Well first I'm gonna tell you that I'm not allowed to execute that there's various reasons that I can't execute it One simple one is that I'm trying to declare a table T and refer to it before it has been declared Another issue with declaring it is the sub query in the check constraint we'll talk_about that in a moment There's actually a third problem with this constraint which is we need to think_about when it's being checked If we say first attempt to insert the value A and then check the constraint then the constraint will be violated based_on the existence of itself So this is clearly not going to work There is in fact a different expression that might work if it weren't for a couple of other obstacles Here's an expression that doesn't have the problem of whether we check it before or after we insert A This is an expression of a key Constraint in a way you might not have thought of What this says is that the number of distinct values for an attribute A must be equal to the number of tuples in the table In_other_words every tuple has a distinct value for A Now there was one small issue here which is null_values because as we mentioned unique key constraints allow multiple instances of null But if we don't worry_about nulls then this is expression really is a different way of saying that A is a key We run the query and it doesn't allow it Again we have the same problem that we're referring to table T within the check constraint that we're putting in the definition of table T By the way that can be overcome Some systems do allow constraints to be declared or added to tables after the table has been specified So that would go away But no systems that I know of allow sub queries and especially not aggregation within check constraints Let's pursue a little further the question of subqueries and check constraints The key example's a little_binary_digit contrived because of course we can declare key constraints directly But in some cases are very natural constraint that we might want to express a check constraint using sub query And I've set_up a situation right here We create our student table as usual but when we create our apply table we want to have a constraint that says that any student_ID that appears in the apply table is a valid student In_other_words there is a student coupled with that student_ID Now we can write that as a check constraint This is syntactically valid in the Structured_Query_Language standard We specify that the student_ID here in the apply table is in the set of student IDs in the student table but currently no Structured_Query_Language system actually supports sub queries and check constraints Now for this the civic type of constraint it happens to fall into a class that is known_as referential_integrity where we say that this student_ID is referencing a student in the other is referencing the student id in the student table and therefore any student id and apply must also exist in the student and in another video we will referential_integrity in some detail But not every check constraint with a subquery falls in the class of referential_integrity_constraints The example I gave for keys doesn't and neither does the one here Now this is admittedly a little contrived But what this says is that every college's enrollment must be bigger than any high_school and so we write that by writing the check constraint in the college table that the enrollment is greater_than the maximum high_school_size from the student table Now again no system currently will support this However it is in the Structured_Query_Language standard Now one thing I want to mention about check constraints with subqueries is that they can be kind of deceptive And we can take a look_at the apply table again Supposing this was in fact supported by a system It would check whenever we inserted a tuple into apply or it updated a student_ID in apply that the constraint holds But what it will not check is when things change in student So we could write this constraint and every time we do an insert or update and apply it could be verified but somebody could go and change the student table and delete a student_ID and then what we feel as the constraint here is no_longer actually holding So it can be tricky to use those subqueries Again when we do referential_integrity as in this example the referential_integrity system will take care of making sure the constraint holds But when we have an example like the one with the enrollments if we say change the high_school_size in the student table it would not activate this constraint_checking that's specified with the college table The last type of constraint I_am going to show are general assertions General assertions are very_powerful and they are in the Structured_Query_Language standard but unfortunately they currently are not supported by any database system The first assertion I'm going to write is coming back to the issue of trying to declare or trying to enforce a key constraint without using the built in facilities Let_me just write the command here It says we're going to create a assertion called key Notice that this assertion is not associated_with a specific table Although this assertion only talks_about table T Assertions can refer to any number of tables in the database The way an assertion works is we create an assertion and we give it a name and the reason for this is so we can delete it later if we wish Then the keyword check appears And then we write a condition And the conditions can be quite complicated The assertion is saying that this condition written in SQL like language must always be true on the database So the particular condition that I've put in this first example is a condition we use to check_whether attribute A is a key in table T It says that the number of distinct values in attribute A must be equal to the number tuples in T Now I can try to run this but I guarantee you that it's not supported Let's look_at some other example assertions we might write if they were supported Here's an example that implements this referential_integrity that I was describing earlier This referential_integrity_constraint is saying that the student IDs in the apply table must also exist in the student table Now when we write an assertion of that form we tend to often write it in the negative form specifically we say that it's not the case that something bad happens It's not the case that there's some tuple in apply where the student_ID is not in the student table You can try on your_own to write this in a more positive fashion but you'll actually find that using Structured_Query_Language constructs it's not possible It's actually very common for assertions to specify the bad thing in a subquery and then write not exists As a final example let's_suppose that we require that the average_GPA of students_who are accepted to college is greater_than We can write that as an assertion pretty_much exactly as I just described it We take the average_GPA of students where their ID is among the IDs in the apply relation where the decision was yes and our assertion states that that average must be greater_than So so far I've described how assertions are created Let_me just briefly mention how they are checked or how they would be checked if they were implemented Any system that implements this very general form of assertion must determine every possible change to the database that could violate the assertion In this case modifying a GPA modifying a student_ID inserting or deleting from students or apply could all potentially violate the constraint And in that case after each of those types of modifications to the database the system would need to check the constraint make_sure that it's still satisfied and if not generate an_error and disallow the database change So I've only talked_about creating assertions Let_me just talk very briefly about how a system would enforce general assertions of this form if it supported them What the system needs to do is monitor every possible change to the database that could cause the assertion to become violated So we take a look_at this particular assertion it could become violated if we changed the student GPA If we inserted a student even if we deleted a student or if we inserted an application that was now having a decision of yes or updated the application status So all of those changes have to be monitored by the system the constraint has to be checked after the change and if the constraint is no_longer satisfied an_error is generated and the change is undone That_concludes our discussion of constraints with the exception of referential_integrity which is covered in a separate video and the related topic of triggers which is also covered in a separate video This video introduces a very_important type of constraint known_as referential_integrity As a reminder integrity_constraints in a database restrict the allowable data beyond what's already restricted by the structure and types of the database Now the term referential_integrity refers to integrity of references that appear in the database In a relational database a reference from one couple to another occurs through specifying values and integrity referential_integrity says that those values are valid It's sort of equivalent to having no dangling pointers if we used pointers in the database we'll see a number of examples So let's look_at our simple example database with the students colleges and students_applying to colleges that we've been using for all our demos Let's_suppose for example that we had a student who had applied to Stanford for some major let's say Computer_Science with a decision of yes When we have this tuple the value in the SID field here of presumably refers to a value here of an actual student So maybe is student Mary with some GPA in some high_school And furthermore the Stanford value in the C name or college name attribute is presumably referring to the college name in the college table So we would expect to have a Stanford value here Referential integrity talks_about these values here referencing the corresponding values in the other tables Specifically if we have referential_integrity from a attribute A say of a relation R to an attribute B of another relation S What that's saying is that every value that appears in the A column of relation R must have some corresponding value in the B column of relation S So if we take a look_at our example we would say then that we would have referential_integrity from the SID column of apply to the SID column of students We would expect every value that appears in this column to also have a value in the other column and similarly we all have referential_integrity from the college name attribute been applied to the college name attribute in college Again we want every value that appears in this column to also appear in this column Now we might have a violation for example if we had a say applying to Stanford for some major and some decision If we have no over in the student table then this here would be considered a referential_integrity violation Similarly we might have which is valid because we have a student here but if is applying to Yale and we don't have Yale over here then again we have a referential_integrity violation Now let_me_mention that referential_integrity is directional so we talk_about this SID here referencing the SID in the student table We could have referential_integrity in the other direction but that's saying something different That would be saying that every student must apply somewhere every value of her student_ID must appear in the apply table And this particular example we probably would not have that be the case we would probably be able to have students_who hadn't yet applied_anywhere or colleges where no one had applied yet And the most sensible direction for this referential_integrity in this case is from the apply to the student and the apply to the college But again we could have it in both directions if we so wished Now just a few more details of referential_integrity_constraints The referencing attribute in this case the referencing attribute is A is often called the foreign_key And in fact referential_integrity is often referred to as foreign_key constraints And even in the Structured_Query_Language syntax we'll see that the term foreign_key is used Second when we have a referenced attribute in this case now we're talking_about attribute B so we have R A to S B the referenced attribute is usually required to be the primary_key for the table for it's table or at_least specified as unique And that's more about efficient implementation than anything else But it is a requirement in the Structured_Query_Language standard and in most systems Third it is possible to have foreign keys that consist of multiple attributes just like keys themselves can be multiple attributes Let's say for example that in our college relation the college name together with the state form a key not the college name individually If that were the case then our apply table would presumably have one more column that specified the state so we knew which college a student was applying to And in this case we would have these two attributes together as the foreign_key referencing the college name and state together in the college table And we'll see an example of multikey multiattribute foreign_key constraints in our demo Now let's talk_about the enforcement of referential_integrity_constraints First let's think_about what operations can occur to the database to violate a constraint It's not every possible modification So again let's_suppose we have R A referencing S B So for example here we have apply referencing students and we have apply referencing college So certainly if we insert a tuple into the referencing relation so if we inserted a tuple into the apply relation that could potentially violate the referential_integrity if the value say an SID or C name didn't have matching values in the reference relations Sort of conversely if we delete from a reference relation say we delete a student then that could cause a violation because say we have the value here and the app apply tuple that was referring to after this is gone would then have effectively a dangling pointer And of course if we updated the referencing value either of these columns that could cause a violation if the new value didn't exist in the reference table or if we update the referenced values that could also cause a violation In the Structured_Query_Language standard and in all implementations if we have an insertion into the referencing table or an update to the referencing table that causes a violation to the integrity constraint the referential_integrity_constraint then an_error is generated and that modification is not allowed just like the violation of other types of constraints For the reference table however table S in our case table student and table college If there are modifications that violate the constraint if the referential_integrity was defined initially with some special options then it's possible for the database system to automatically modify the referencing table so that the constraint is not violated So let's talk_about that in a little_more detail Let's start_by talking_about deletions from the referenced table So let's says we have our student here maybe has applied a couple of places and then we have our student here in the student table So right now referential_integrity is good Everything's okay But let's_suppose that we delete the tuple with So there's actually three possible things that can happen depending on how we set_up the referential_integrity_constraint in the first place So the default behavior is what's_called restrict so restrict is actually a key word but that's the default and it says that if we do a deletion to the reference table and a constraint becomes violated then we generate a air just like I said before and the modification is disallowed The two other options are a little_binary_digit more interesting One of them is called set null And what set null says is if we delete a tuple in a reference table then we don't generate an_error Rather we take the referencing tuples in this case these two tuples with and we take their SIDs and we replace those SIDs with NULL And this is considered acceptable from a referential_integrity point of view to have nulls in the foreign_key column so that will occur automatically The third possibility is what's_called cascade so let's set_up a little_more data let's say we have who's applied to Stanford and we have Stanford over here and now let's say that we again delete this tuple so that would leave us with a referential_integrity_constraint violation here with the Stanford value So what cascade says for the on delete case is that if we delete this tuple then we'll simply delete any tuple that has a referencing value so this tuple will be deleted as_well Now the reason is called Cascade is because sometimes you can actually set_up a whole chain of referential_integrity_constraints so we have apply referencing college here but maybe we've had some other referencing ply and maybe even another table referencing that one and if we say deleted a tuple from college that caused us to delete a tuple from a ply if there was a tuple up here referencing that we might get a delete there and then a further delete and then so on Typically cascading will only go one step but we'll see an example in our demo where we'll set it up where a cascade will go some distance Now updates have a similar three options Let's erase all this stuff here Let's again set_up some example data So let's say our student was applied to Stanford and we have over here If we tried to update say this to be the value there strict command would say that's not allowed because that would leave us with a steg when pointer and will generate an_error The set null command will similar to the delete if this is changed to four five six Set any values to null So in this case we change to in the student And then we would change over here to null Probably the most interesting case is the cascade case for the update Cascade says that if we update a reference value then we'll make the same update to the referencing value So let's say we have Stanford over here in fact we have to if we have the value the referential_integrity_constraint being correct and now let's say that somebody comes along and says I think Stanford is spelled wrong It's actually Stanford well that's actually a common mispelling for Stanford The first one was correct But if someone makes this change if we have the cascade option for the referential_integrity_constraints between apply C name and college C name then that update will be propagated to any referencing values So in that case automatically this Stanford and any other Stanfords in the apply table will be updated automatically to Stanford Now let's take a look_at referential_integrity in action Let's create our three tables We create the college table with the college name as primary_key and the student table with the student_ID as primary_key That allows_us to have referential_integrity_constraints in the apply table that reference those two attributes When we create the table apply now we're going to specify the attributes and thhe key word references says that we're setting up a referential_integrity_constraint from attribute Student ID to the Student ID attribute of the student table And similarly we extend the declaration of the College Name attribute to have a referential_integrity_constraint to the College Name attribute of the college table We'll go_ahead and create those tables Now let's try to populate our tables If we make a mistake we try to put our apply values in first but there's_no data in the student table or the college table So when we try to insert for example student applying to Stanford we'll get a referential_integrity violation because there's_no student and there's_no college Standford and similarly for student applying to Berkeley So we see the errors and what we need to do is first insert the tuples for the students and the colleges and then insert the applied tuples afterwards So let's insert the two students and two three four and the two colleges Stanford and Berkeley no problem doing that And now we'll go_ahead and again insert the apply tuples and this time everything should work just fine and it does In addition to inserts into the referencing table we also have to worry_about updates So as a reminder let's take a look_at the students that we have We have students and and in the apply we have students and each applying to one college Now we're going to update our applied tuple that has student_ID Our first update tries to set the student_ID to but we'll get a referential_integrity violation because there's_no student with ID Our second update will be more successful It will update 's application to have student_ID We'll go_ahead and execute the update And we see that the second one did succeed So far we've looked at modifications to the referencing table but we also have to worry_about modifications for the referenced tables In our case that's the student table and the college table For_example let's_suppose we try to delete from the college table where the college name is Stanford If we try to delete that couple we'll get an_error because we do have an applied couple that has a value Stamford and is therefore referencing the couple we're trying to delete Similarly tried to delete some student couples Let's go_back and look_at our apply relation and we now see that both of the student IDs in apply are so it should be ok to delete the student couple with nothing is referencing it But it should not be okay to delete the couple with student_ID and indeed when we run the command we see that the first one generated an_error and the second one succeeded How about updating a referenced table Let's say that we decide we'd rather have Berkeley called Bezerkly So we try to update that college name but when we run the command we get an_error because we do have an apply tuple that's referencing the value as Berkeley And finally although we've been talking_about data level modifications referential_integrity_constraints also place restrictions on dropping tables For_example if we try tried to drop the student table we would again get a referential_integrity_constraint because that would leave data in the apply table referencing non existent data in what would now be a non existing table You can see that the errors says that you cannot drop a table student because other objects are currently depending on it So when we have referential_integrity_constraints if we wanted to drop the tables we'd have to first drop the applied table and then drop the table that it's referencing Now we're going to set_up the apply table to experiment with some of the automatic mechanisms for handling referential_integrity violations Specifically we still have the same referential_integrity_constraints from student id to the student table and from college names to the college table but for the student_ID referential_integrity_constraint we're and we're going to specify that if a student is deleted then we're going to set any referencing values to null and we do that with the keywords on delete which tells_us what to do when there's a delete to the referenced table we use the set null option For the college name reference we're going to specify that if the college is updated in the college table and that says on update we'll use the cascade option As a reminder what that does is that if we change the college name then we'll propagate that change to any college names that reference it in the apply table Now I could have specified two more options I could have specified an on update option for the student_ID and an on delete option for the college name so there could be four all together Because I left those out those both will use the default which is the restrict option which says if you perform a modification that generates a referential_integrity violation then an_error will be generated and the modification will not be performed just as we saw in the previous examples So let's go_ahead and let's create the table and then let's experiment with some modifications Let's start_by adding a couple more students to our student table So then if we take a look our applied table is currently empty because we just created it our college table has Stanford and Berkely and our student table now has three tuples student IDs and We'll insert five tuples into the apply table and all of them are going to be valid with respect to referential_integrity We're only going to insert students that are or and they're only going to apply to Stanford or Berkeley So we've inserted those values and now we'll perform some modifications to see what_happens We're going to delete from the student table all students_whose ID is greater_than Going back and looking_at that table we'll see that student and are going to be deleted Now remember we specified on delete set null for the apply referential_integrity_constraints Specifically when we look_at our apply table the references to the students that are about to be deleted should be automatically set to null when we run the delete command So we'll go_ahead We'll perform the deletion We'll take a look_at the apply table when we We see that those values have indeed been set to null and if we take a look_at the student table we'll see that we only have student left Now let's test our cascaded update As a reminder when we set_up the college name referential_integrity_constraint we said that if we update the college name in the college table we should propogate those updates using cascade to any references in the applied table So we're once again going to attempt to change the name of Berkely to Bezerkly This time it should allow_us to do it and it should change any applications to Berkely to now specify Berzerkly So we'll go_ahead and run the command and we'll look_at the apply and we will see once we refresh that indeed Berkeley has now been changed to Bezerkly This example is a doozy It's going to show a whole_bunch of features that we haven't seen in previous examples We're gonna use a simple table T with just three attributes A B and C and we're going to say that A and B together are a primary_key for the table In the example we're going to demonstrate referential_integrity within a single table so intra table referential_integrity We're going to demonstrate referential_integrity involving multiple we'll attribute foreign keys and primary keys and we're going to demonstrate a real cascading where we're going to have a cascaded delete that's going to propagate across numerous couples So typically one thinks of referential_integrity as having a referencing table and then the referenced value exists in a different table but there's nothing wrong with having referential_integrity within a single table For_example in the one attribute case I might have attribute B where every value in B must also appear in value A and that would be a referential_integrity_constraint within the table In this case things are slightly more_complicated because I'm using pairs of attributes for my referential_integrity_constraint Specifically I'm going to have attributes B and C together reference attributes A and B together Syntactically to declare a multi attribute referential_integrity_constraint in a table definition I have to put it at the end because I can't attach it to a single attribute Just like I do when I have say multi attribute keys which are also demonstrated here The syntax is at the end of the table definition I'd say that I'm going to have foreign_key constraint and attributes B and C together have a referential_integrity_constraint to attributes A and B of the same table Then in addition I'm going to specify on delete cascade That_means if I delete an attribute if I delete a tuple then any other tuple that's referencing the AB values of that tuple with it's BC values will automatically be deleted So let's run the table creation insert our data and then let's take a look_at the table and predict what's going to happen when we actually delete a tuple So here's the contents of table T after the insertions So we see that A and B together do form a key All of the AB pairs are unique and furthermore every BC pair has a corresponding AB value in the same table So every tuple except the first the BC pair references the AB pair of the preceding tuple So we have ' ' here referencing ' ' in tuple one our two one here references the two one in table two and so on So our referential_integrity_constraints are intact for this table What we're going to delete the first tupple by running a command that says delete the tupple whose A value is one When we delete tupple one because we have the cascaded delete set_up we will need to delete any tuple whose reference values are one one So that will be couple two So after deleting couple one the referential_integrity_constraint enforcement will delete couple two When couple two is deleted the two one value will be gone So tuple which references tuple will be deleted then tuple which references will be deleted and so on until the entire table is empty So here's our delete command to delete the first tuple We run the command We go_back and we look_at the table and when we refresh we see that the table is indeed empty That_concludes our demonstration of referential_integrity_constraints Referential integrity is actually extremely common in deployments of relational_databases The natural way to design a relational schema often has values in columns of one table referring to values of columns of another and by setting up referential_integrity_constraints the system itself will monitor the database and make_sure that it always remains consistent In this video we'll introduce the concept of triggers In a separate video we'll give an extensive demonstration of triggers in a running system As a reminder triggers are event condition action rules They specify that whenever a certain type of event occurs in the database check the condition over the database and if it's true execute an action automatically There are a couple of reasons that triggers are used fairly extensively actually in database applications One of them is to move logic that monitors the database from the applications into the database system itself That allows the monitoring to be done more efficiently and it's also more modular so it doesn't have to be repeated in every application A second and probably the most common use of triggers is simply to enforce integrity_constraints And you_might_wonder why are those constraints not enforced by the constraint system Well one reason is that some constraint systems are limited and we can enforce more expressive integrity_constraints using triggers and second of all triggers can do automatic repair of constraints when they are violated by specifying the repair as the action portion of the trigger I do want to mention that implementations of triggers vary significantly across the different database_systems In this introductory_video we'll be talking_about the Structured_Query_Language standard for triggers But in our demonstration we'll be using the triggers as supported by SQLite So here we have the syntax of creating a trigger using the Structured_Query_Language standard The second line is the event portion of the trigger It says that the trigger should be activated either before or after or instead of specific events and the specific events that can be specified are insert on a Table_T or delete on a table T or update two particular columns on a table T And actually the columns themselves are optional Update up columns on table T Let's skip the referencing variables clause for a moment and go on to the for each_row So for each_row is an optional clause that states that the trigger should be activated once for each modified tuple Let_me explain what the deal is here So when we run say a delete command on the database that delete command might delete say ten tuples If we specify for each_row in our trigger then we will run the trigger ten times once for each deleted tuple On the other_hand if for each_row is not present then we will execute the trigger once for the entire statement Now one tricky thing is that no matter what the trigger is activated at the end of the statement But its trigger it's activated either ten times for the ten deleted tuples or once if for each_row is not present Now let's talk_about the referencing variables I'm going to write them down here The idea of referencing variables is that they give_us a way to reference the data that was modified that caused the trigger to be activated So we can have in the referencing variables these are key words old_row as And then we can give a name to the old_row We can have new row as and again name and we can also have old table as a name and new table And there's a whole_bunch of things to explain here So it's possible to have up to all four of these on a single trigger but there are certain restrictions and let_me explain First of all if we have a trigger that is based_on insertions then we can only refer to new data That would be the new inserted data If we have a trigger activated by deletions then we can only refer to the old variables for the deleted data If we have the case of update then we can refer to both old and new and we will get the previous version of the updated values and the new version of those values So we can only have both old and new in the case when our trigger is activated by an update Now let's talk_about about row versus table So if we have a row_level trigger as a reminder that will be triggered once for each modified tuple but after the entire statement has run So lets take for example a row_level delete In the case of deletes we can only have old but we could have for a row_level trigger both the old_row and the old table The old_row would refer to the specific tuple that the trigger is activated for And again if we deleted ten rows and it will be activated ten times once for each deleted tupel While the old table will refer to all ten of the updated of the deleted tuples Now there's often a lot of confusion with the old table It's not referring to the old state of the database it's referring specifically to the set of tuples that were in this case deleted If our tuple if our trigger is not for each_row if it's a statement_level trigger then we cannot refer to the row_level variables but we only have the table level variables So to reiterate if we had say an insert that was row_level then we could have both new row and new table If we have a statement_level insert we can only have new table If we have a row_level delete then we can have both old_row and old table but if it's a statement_level delete then we can only have old table Finally if we have a row_level update then we can have all four of these But if we have a statement_level update then we would only have the old table and the new table Just to clarify when I say row_level I_mean that for each_row is present and when I say statement_level I_mean that for each_row is not present OK So now we covered the those clauses Fortunately the last two are a little_binary_digit easier The condition here is it's like a Structured_Query_Language wear condition It's going to test the condition on the database and if the condition is true then the action will be performed Actually what this is really like is like a general assertion We saw there were certain ways of describing conditions that are on entire databases and we will see a number of examples And finally last of all the action in the Structured_Query_Language standard the action is a Structured_Query_Language statement In systems some systems will have a set of simple statements and a begin end bracket Some will have stored procedures So this is a case where the systems do vary quite a binary_digit We'll be using Structured_Query_Language light which has as it's action begin and end with any number of Structured_Query_Language statements within it Well that all seems very complicated and there are quite a few complications with triggers but in many cases they're relatively straight_forward and I think the next thing we'll do just to relax for a moment is take a look_at a fairly simple example In this example we're going to implement referential_integrity as we discussed in the previous_video Let's say that we have a table R whose attribute A references attribute B of table S and we want to implement cascaded delete As a reminder what that means is if we delete from table 's' then any 'a' values that reference the deleted B values will themselves also be deleted Ok so let's specify that in a trigger it's really quite simple We give the trigger we say after we delete on 's' so this trigger will be activated whenever we delete from 's' We're going to make it a row_level trigger So that means we're going to activate the trigger once for each deleted row We're going to set_up that deleted row to be called O And finally there's_no conditions so whenever we have a delete from S then in our action we're going to delete from R all tuples where the A value equals the B value of the deleted couple from S So that should all be pretty easy to understand Just as one little change let's take a look_at writing the same trigger as a statement_level instead of a row row_level trigger So now I've taken away for each_row and let's look_at what changes we need to make Well first of all we don't have old_row anymore as I_mentioned for on statement_level triggers we only have old tables So now we're going to set_up a variable called OT that's referencing old tables and remember this is going to the set of deleted couples It's not the old value of the table but just the value of the tuples that have been deleted The other thing we need to change is the action of the trigger Instead of matching one tuple at a time we just look for tuples in R where the A value is among the B values that were deleted from S and we delete those couples from are And that works exactly the same as the row_level version of the trigger Now you_might_wonder which version you should use well it's turns_out some systems don't support both systems and you don't have a choice For this particular example probably the statement_level trigger would be more efficient Since it only activates the trigger once and takes care of all the referential_integrity cascaded deletes in one fell swoop So this example shows that triggers can be quite natural But there are a lot of complexities as I alluded to in the original slide that showed the full trigger syntax So just to go through some of the trickier issues we talked already a binary_digit about row_level versus statement_level And the use of the different new and old_row and new and old table With triggers that execute after the modification this is fairly understandable But things can get more_complicated when we have or instead of the modification that causes the trigger to be activated Secondly we can have multiple triggers activated at this Same time its pretty_simple What if I declared two separate triggers that are activated by deletes say on a particular table The we have to ask which one is going to go first and maybe the behavior will differ depending on which one goes first So that's something that needs to be thought about when one defines triggers and understands how they're going to behave when the database is modified Another possibility that we have to consider is not when triggers are activated at the same time But when triggers activate each other in a chaining effect so I might have a trigger that's activated and it performs an action that the database modification that activates another trigger which in turn can activate other triggers We can also have triggers that trigger themselves potentially We can have cycles well when a trigger triggers itself it's a cycle But we can also have t that triggers t that triggers t that triggers t again then we need to worry_about issues is like termination both for cycles and self triggering We can also have a case where a trigger has multiple actions and each one of those actions individually activates other triggers So we start getting a nested behavior in the trigger activation So again all of these need to be both defined carefully and understood carefully when one creates triggers so that one knows how one will behave in practice Another issue that's really more about trigger design is exactly how to write one's triggers when conditions are involved Sometimes it's possible to put a condition either as part of the action or as part of the when clause in a trigger Now certain trigger languages are limited in what they can say on the when and then again we wouldn't have a choice But sometimes we do have choice and it could effect actually the efficiency of trigger execution depending where we put the condition Finally I'll mention again that implementations do vary significantly especially across these tricky issues Now most of these issues are actually going to be discussed further in the demo But the demo that I'm going to give is going to use Sequel Lite which only has row_level triggers so let_me give one more example that talks_about the difference between row_level and statement_level triggers since I won't be able to show that in the demo This example is completely contrived to show a few issues Let's_suppose that we have a table T and it has two attributes Attribute K is a key for the table and attribute V is a value And we're going to have a trigger that's going to be activated when we perform insertions on T And let's_suppose that we perform a large number of insertions or at_least a few insertions and it's going to be a role level trigger So we're going to execute the trigger once for each_row that's inserted So as a reminder when we have a row_level trigger we can refer both to the specific tuple that's being processed for one activation of the trigger and we can also refer to the entire set of changes that were made by the modification command that caused the trigger to be activated So in this trigger we're going to use NR to refer to the current inserted row that we are processing and New Technology to refer to the entire set of inserted rows Okay so what is this trigger doing It's going to process one inserted row at a time And when it processes that row it's going to check_whether the current average value of 'v' in table 'T' so that's the current average value is less_than the average of the inserted rows Now one thing I want to say is that this value is stable so even if we modify T New Technology doesn't change New Technology is always set to the set of inserted tuples ok So we check_whether T's average is less_than the New Technology average and if it is then we're going to modify the update that modified the tuple that was inserted So we're going to update T and we're going to set the value to be V for the tuple that we're currently processing in other_words the tuple whose key is in new row Ok this is really really tricky I'm trying to demonstrate a bunch of things here So just let's back off and think again for a minute about what_happened We inserted a whole_bunch of tuples After we inserted those tuples we first determined what the average value of the inserted tuples That's this average value here Let's say it's Then we're going to for each inserted tuple check is the current average and T less_than If it is we're going to update that tuple that was inserted to be greater and that's going to be end of that activation So there's a couple of things that I wanted to point out specifically with this trigger One is that there is no statement_level equivalent of this trigger If we try to write this without the four each_row we can never get the same behavior because what we're doing is looking_at each_row one at a time and deciding whether to increase its value So we might increase the value for some subset of the rows but not all of them and it would not be possible to do that with a statement_level trigger The second thing that this triggers shows is the potential to have a non deterministic final state Because we're gonna again increase the value of the inserted tuples until the average exceeds a certain threshold and then we'll stop in updating those values or increasing those values So the subset of tuples whose values are increased is determined by the order in which the trigger processes the set of inserted couples Ok so this trigger is really really complicated Now of course nobody is going to ever write a trigger that looks exactly like this Part of the point was to show a bunch of different features and show a bunch of different subtleties Specifically in the context of row_level versus statement_level because we aren't able to make that contrast with the demonstration that we're going to give So to conclude triggers are event condition action rules that are used used to move monitoring logic from the application to the database and to enforce complex constraints potentially with automatic repair and the implementations vary significantly Now let's see triggers in action We're going to cover a number of features in the demonstration which is going to be in two parts We'll cover before and after triggers with both insert delete and update triggering events We will not be covering instead of triggers which are used primarily for views and so will be covered in the material on views We'll show access to new and old data for the modified data that causes a trigger to be activated We'll show trigger conditions and trigger actions We'll show some triggers that are being used to enforce constraints We'll show a trigger chaining where the execution of one trigger activates another trigger And we'll show triggers that activate themselves and cyclic behavior of triggers conflicts when multiple triggers are activated at the same time and finally nested trigger invocations when a trigger has several actions which themselves activate other triggers The video introducing triggers used the Structured_Query_Language standard It so happens that no database system implements the exact standard and in fact some systems deviate considerably from the standard not only in the syntax for specifying triggers but also the behavior meaning one has to be very_careful to understand the trigger system that one is using In terms of the primary open_source systems at the time of this video Postgres has the most expressive trigger system SQLite is a little_binary_digit less expressive And MySQL is considerably more restrictive than the other two Let's look_at some details In terms of expressiveness and behavior Postgres is the closest to the standard It really does implement the full standard It has row_level and statement_level triggers access to old and new rows and tables Unfortunately it uses its_own cumbersome and somewhat awkward syntax making it not very suitable for our demonstration SQLite implements row_level triggers only And it also has immediate activation which is a deviation in behavior from the standard As we discussed in the introduction the standard for triggers is that if we have a modification statement that makes many changes Regardless of whether we have a row_level or statement_level trigger we execute triggers at the end of the statement Whereas in SQLite we have row_level triggers only and there after each row_level change to the database As a result SQLite also does not have access to new table or old table Like SQLite MySQL also has only row_level triggers and they are activated immediately instead of at the end of the statement They also don't have access to old and new table There are two other significant limitations in MySQL One of them is that MySQL only permits one trigger per event type So for example only one trigger for any inserting into a particular table or deleting from a table Furthermore there's limited trigger training allowed in MySQL Now these features if we want to call them that Do you mean that the triggers in MySQL are quite understandable in how they behave because they're fairly restricted in what they can do and specifically in how they can interact For our demo we'll be using SQLite And let_me_mention a few things about the syntax Since there's row_level triggers only for each_row although it's a allowed to be specified in the trigger is implicit if it's not present As I_mentioned there's_no access to old table or new table and so as a result SQLite has no referencing clause instead prebuying these variables old and new two old_row and new row we'll see that clearly in the demo Finally the action in SQLite triggers is a begin end block with any number of Structured_Query_Language statements The demo is in two parts and in this part of the demo we'll be in the first five features the other three will be covered in the second part In the first part of the demo we'll be using our usual simple_college_admissions database with the college table student table and students_applying to colleges We will be starting with our usual database our four colleges our set of students and our set of couples of students_applying to colleges The idea is that we'll be creating several different triggers and then we'll be modifying the database and see how those triggers behave Here's our first trigger Let_me mention that the particular interface we're using does not allow_us to execute create trigger commands from this window so we'll actually be executing the demand separately Our first trigger will intercept insertions_into the student table and we'll check the GPA If the inserted GPA the GPA of the inserted student is greater_than or less_than or equal to that student will be automatically applying to Stanford for a geology major and applying to Massachusetts_Institute of Technology for a biology major Let's look a little closer at the tax we give the trigger a name We specify the triggering event In this case its insertions on student Next we have the option of specifying for each_row Now as a reminder in the SQLite system only the for each_row option is implemented So even if we actually left this clause out it would be a low level trigger Furthermore SQLite does not have a referencing clause So in the Structured_Query_Language standard there's the new row old_row new table and old table which can be bound to variables in the referencing clause In SQLite only new row and old_row are supported and therefore they're bound automatically to the predefined variables new and old Now remember new and old gives_us a way to access the data that was modified that caused the trigger to be activated In the case of an insertion there is only new data be the inserted row If we had an update we'd have both new and old available to us for the modified row And if we had a delete we'd have only old We'll see that in later examples In this particular example we're going to use new in our trigger condition to reference the insertion that caused the trigger to be activated We check if that Insert a tuple have a GPA within the range we're looking for If it does then we go_ahead and the trigger will go_ahead and execute the action The action will insert two new tuples into the apply table and it will again use the new variable to access the inserted tuple we're working on so it will get that SID for the inserted tuple Insert that together with Stanford Geology and a null decision into apply And similarly have that new student id applying to Massachusetts_Institute of Technology Biology with a null decision We've created the trigger So now let's try inserting some new students into our database Specifically we'll insert Kevin whose_GPA is in the range that we're looking for for automatic application and Laurie whose_GPA is outside of that range Let's go_ahead and insert those tuples and let's take a look_at our data When we look_at our student table we see that Kevin and Laurie have been inserted Now let's take a look_at apply Here we see that student and that was Kevin with the GPA has automatically been inserted to apply to Stanford in Geology Massachusetts_Institute of Technology in biology and As a reminder an empty cell indicates a null value So our trigger was activated by those insertions and it performed insertions_into apply for Kevin but again not for Laurie because her GPA did not satisfy the trigger condition Now let's make a more dramatic database modification to test out our trigger We 're going to essentially copy the entire student table into itself So we're going to double the size of the student table We're going to take each tuple and we're going to add a new tuple that's going to be identical except we're going to increment the student_ID in order to generate a unique ID So when we insert all of these tuples into the student table again all of those whose_GPA is in the range between and should have an automatic tuple into inserted into the apply actually two tuples inserted into the apply table having them apply to Stanford and Massachusetts_Institute of Technology So here's the insert command we're going to insert_into student the result of a query That's a reminder of how we do that from previous_videos Our query will select from the student all four attributes except that it will increment the ID So again this will double the size of the student table Copying every tuple but with a new student_ID So let's go_ahead and perform the insertions and now let's look_at the student table So we'll refresh and we see that the table has doubled in size and again we incremented the IDs What we're most interested in of course is what_happens in the apply table And now we should have a bunch of new applications to Stanford and Massachusetts_Institute of Technology and indeed we do and if we looked back we'd see that each of these IDs corresponds to a student whose GPI is in the range to satisfy the condition of the trigger We'll leave all these tuples in We're going to actually use them in later triggers that we're going to see This trigger is very similar maybe identical to one that we saw in the introductory_video This trigger simulates the behavior of cascade delete when we have a referential_integrity_constraint from the student_ID in the apply table to the student_ID in the student table Very_specifically it it's activated when we have the lesions from student and again I'm including for each_row and I'll do it in every trigger even_though if I left it out it the behavior would be the same So for each it's deleted from Student it has no condition so it always executes the action and the action says look and apply for any student_ID whose ID is equal to the deleted one And now and again we're using this reserved keyword old as a reminder that's automatically bound to old_row and so it will mind to the tuple that's being deleted that activated the trigger We'll find any apply records that refer to the deleted student_ID and we'll delete those apply records as_well So first let's take a look_at our student table because what we're going to do is delete from this table every student whose ID is greater_than so quite a number of these students are going to be deleted and then we're going to see that the applications of those students and there's quite a large applications greater_than will be deleted automatically by the trigger So here's the simple deletion command to remove all students_whose ID is greater_than We'll run the command and let's go take a look OK So we go to our student table and we see indeed that a lot of students have been deleted those with IDs greater that five hundred we go to our apply table and we see similarly but automatically all of the apply records with the student_ID greater_than have been deleted as_well Now let's create a trigger that simulates a cascaded update again for referential_integrity So let's_suppose we are trying to implement using triggers referential_integrity from the C name that appears in apply to the C name that appears in college And when we update college a name we're going to propagate those updates to any apply records that refer to that So let's take a look_at the trigger This is our first trigger that's activated by update commands So the event Triggering event there's an update on the college table and very specifically we've specified the attribute C name And if we left out C name then any update to college would activate this trigger But by putting in C name it's a little_more specific and more efficient Again for each_row which would be implicit if we left it out And then let's take a look So what our action does Similar to the other the previous trigger we don't have a condition We update we run the action automatically As a reminder we have now new and old variables since it is an update So each activation of this trigger will be for one row New will give_us the new value of that row Old will give_us the old value of that row So our action is going to update the apply table It's going to find college names that refer to the old college name and it's going to change them to refer to the new college name So I would say this is actually a very intuitive trigger easy to understand to perform cascaded update So let's modify two of the college names in our database We'll change Stanford to The Farm and for those of you who aren't aware The Farm is actually a common nickname for Stanford And we'll change Berkeley to Bezerkley and I'll let you draw your_own conclusions on that one So let's go_ahead with the update And let's take a look_at our data now And this time we'll just go straight to the apply relation We'll refresh and we'll see that our references to Berkeley have been changed automatically to Bezerkley and Stanford to The Farm And again I did not run an update_command on the apply table I only ran the update_command on the college table which has also changed And then the trigger was activated and propagated the update to the apply records So our previous two triggers simulated integrity_constraints The next two triggers we're going to write are going to simulate key constraints Specifically we are going to say that the college name needs to be unique within the college table So we're going to write triggers that intercept both inserts and updates for the college table And if they try to create or update a college name that creates a duplicate We're going to disallow that command by raising an_error One other difference between our previous triggers is that these are going to be activated before the insert rather_than after the insert So let's take a look We have a trigger that's activated before inserts on college and they do have a condition What it looks for is another college that has the same college name as the one we're trying to insert So that would be a key violation if we attempt to insert a college name that already exists In the action we're using a SQLite specific construct Raise Ignore so raise is an_error raise command and ignore says simply ignore the updates that's underway So when we try to insert a duplicate college the trigger will be activated and rays ignore will say disallow that insert Now we actually have a second trigger hidden underneath here This trigger handles updates to the college table When a college name is updated similar to the insertion it checks whether there's already a college with the proposed new college name And if it is it again uses the Raise Ignore command Now both of these triggers were specified before the modification and that's necessary in order to use this particular condition If we had specified after the modification as our type of triggering event then the condition would have to check something different it would have to check if we created duplicate college names but again we're using the before version here which is quite natural and the Raise Ignore command also works in tandem with the before Because what it says is to abandon the modification that's under way Now I will mention that the raising of errors in trigger action is quite system specific So what I'm showing you is simply the SQLite version that we would use in the case of a before trigger So now let's make some attempted modifications to College As a reminder we've already changed Stanford to The Farm and Berkeley to Bezerkeley So in our first two insertion attempts we'll try to insert Stanford into our college table and that should actually work because we changed the name Stanford to the farm We'll also try to insert another couple for Massachusetts_Institute of Technology and that should not work because that should activate the trigger and detect that Massachusetts_Institute of Technology would be a duplicate if it were inserted We'll run the commands we don't get any errors it's not raising an_error As a reminder the raise ignore command simply stops the command that's underway So we'll take a look_at our college table and we see that the new Stanford couple was inserted successfully but the attempt to insert a Massachusetts_Institute of Technology couple was rebuffed by our trigger Now lets try some updates as a reminder we had one trigger that intercepted inserts and another that intercepted updates Let's try to change Bezerkeley back to Berkeley We run that We go to college And we see that it did in fact change back no problem because it's not creating a key violation Now let's try changing the farm back to Stanford We'll run that Again we won't get an_error but when we go and look_at the college table we'll see that the farm is still there Because we've inserted the new Stanford tuple the trigger intercepted our update and didn't allow it to be changed because it would've caused a duplicate value So frustrated by that let's change the farm to Stanford again our favorite misspelling of the university We'll run the command We'll take a look And indeed the farm has now been changed to Stanford because it's not creating a duplicate value Incidentally there were a few things happening behind the scenes while we were experimenting with those key triggers Don't forget we define a trigger that intercepts updates on the college name and propagates them to the apply table So while we were changing those college names to experiment with the key triggers we were also modifying the apply table Let's go see what_happened to that table We go here and we look_at apply and we see our Berkeley students are still Berkeley While we weren't looking they change to Bezerkeley and back to Berkeley Our Standford students change to the farm and then they change to Standford So in order to not offend my sense abilities I'll be deleting those students those apply records before we proceed Now let's take a look_at a trigger that does a little_more than enforce a constraint What this trigger is going to do is monitor applications to colleges and when the number of applications to a college exceeds it's going to rename that college to have the college name and then dash done indicating that we're done with the college Specifically it will be triggered by insertions to apply since that's what going to increase the number of applications And when there's an insertion it 's going to count the number of apply records that have applied to that college Now this is an after trigger so it will include the one we've inserted and new is going to again refer to the inserted couple so we will count the number of applied records to that particular college and see if it exceeds ten If is does it's going to update the college corresponding to the one that's being applied to and we'll do that again by looking_at the new college name And it's going to take that college record And it's going to set its name to be the current name and this is a concatenation operator with the string done Once we run it you'll we'll see exactly how it works Another thing we'll be demonstrating in this example is trigger chaining where the activation of one trigger activates an action that activates another trigger And let_me remind_you of the first trigger we created where when we inserted students with a certain GPA we automatically inserted apply records for those students to Stanford and Massachusetts_Institute of Technology So what I'm going to actually do rather_than insert apply records directly to experiment with this trigger I'm going to insert student records Those student records if they have the right GPA will activate this trigger which will automatically insert_into apply And then those insertions will activate our apply trigger which will check to see if the college now has more_than ten applicants As a reminder our trigger that does automatic insertions of applications we'll be having students_applying to Stanford and Massachusetts_Institute of Technology So we have no students_who have currently applied to Stanford that's because I deleted those misspelling Stanford and that left us with none but that's fine that will Help us test our trigger Let's see how_many students we have who have applied to Massachusetts_Institute of Technology We currently have Five who have applied to Massachusetts_Institute of Technology Finally let's see how_many existing students we have with a GPA that's in range for automatic application and the reason I'm checking that is because I'm going to insert copies of those students that will activate my trigger and show the trigger chain So I currently have six students with GPAs in range so if I copy those six students as new insertions then we'll be adding six applications to Massachusetts_Institute of Technology That will bring us to a total of eleven and six applications to Stanford bringing us to a total of six So if all goes well with the trigger chaining and everything else when we after we we insert those six new students we should get new apply records that will cause Massachusetts_Institute of Technology to be done because it's exceeded the threshold of ten but Stanford not to be done So let's see if that all works out as we expect So my insertion command is going to completely replicate the student table It's going to make one additional copy of every student and as a reminder that will cop that will insert six new students_whose GPA is in range for automatic application So let's go_ahead and run the insert command and let's go straight to our colleges and see what_happens Indeed Massachusetts_Institute of Technology is now labeled as done but Stanford hasn't changed And let's look_at our apply table to understand what_happened so all of the couples actually starting here were inserted in this round of trigger activation so we added six students Six of them had GPA's in range That gave us twelve applications one each to Stanford and Massachusetts_Institute of Technology These other applications were left over from before Then the fact that Massachusetts_Institute of Technology went over ten applications meant Massachusetts_Institute of Technology was relabeled as done and there was a binary_digit more trigger chaining that happened Don't forget our update propagation trigger which is still there That one took Massachusetts_Institute of Technology done when it was modified and sent it back to modify the values of Massachusetts_Institute of Technology and the apply table This might be a good example for you to download and play with yourself or re run the video just to entirely understand what's going on Now let's see if we can coax Stanford to be done as_well So how_many students do we have who applied to Stanford Six Those are the six applications that occurred as a result of the most recent trigger firings And how_many students do we have with a GPA in range Well we have of them We have the that we had originally and then we copied each one of those over in our previous example So now if we once again copy our student table insert_into student an entire copy then we'll be inserting twelve students with GPA's in range Those students will automatically apply to Stanford and Berkeley That should put Stanford over the threshold of ten and change it's name to Stanford done Let's go_ahead and execute the command and then let's take a look_at the college table and see where things stand Indeed now Stanford is done Let's also take a look_at the apply table because there's some quite interesting things going on here So we see all of our tuples with Massachusetts_Institute of Technology done and Stanford done and let's just remember what_happened We insert tuples with a student Those tuples are causing automatic insertions to Stanford and Massachusetts_Institute of Technology However when Stanford or Massachusetts_Institute of Technology went over threshold over applications then the name was modified in the college table to be MIT done or Standford done That modification in the college table activated our update propagation trigger which then had the change in the college table propagate to the change in the apply table But then at some point along the way we see that we stop having MIT done and Stanford done and we revert back to Massachusetts_Institute of Technology and Stanford Well why is that That's because when we insert the MITs and Stanfords automatically and we go_back and count how_many there are for Massachusetts_Institute of Technology and Stanford after we've changed this to done we're back to zero If we kept going and then we hit the threshold again of then we would see that Standford done and MIT done change as_well Sorry That Standford and Massachusetts_Institute of Technology change back to Standford done and MIT done So this is very complicated Again what I'm really trying to demonstrate here is well of course just the basic behavior of triggers but also the complexity When triggers trigger other triggers that further trigger other triggers and back And we'll be seeing more of that in the second demonstration video But I did want to illustrate it here And there's one more point that I want to make which is that as I_mentioned in span class STtranscriptContent name id STtranscriptContent contenteditable true style margin top px margin right px margin bottom px margin left px padding top px padding right px padding bottom px padding left px border top width px border right width px border bottom width px border left width px border style initial border color initial font weight inherit font style normal font size px font family verdana vertical align baseline border top style none border right style none border bottom style none border left style none border width initial border color initial line height px text align left text transform none text decoration none webkit box shadow none box shadow none text rendering auto cursor pointer webkit transition property background webkit transition duration s webkit transition timing function linear webkit transition delay initial background image in triggers roll level triggers are activated immediately after each row_level change That is different from the Structured_Query_Language standard In the Structured_Query_Language standard even a row_level trigger will be activated after the entire statement And that's what for example Postgres does But in SQLite we're activating immediately And actually this particular behavior if you think_about it would be somewhat different if we were activating the triggers at the end of the entire command Even if we are doing it one row at a time Again very complicated stuff You_might want to pause and just think_about how this works or even better experiment yourself Let's go_back to a simpler trigger This one intercepts insertions on student before they occur and it checks if the high_school_size is in range If the high_school_size is below or greater_than we're going to assume that's an_error and we're going to raise our ignore Again that's a SQLite specific syntax that says Don't perform the modification that's underway Now that occurs before inserts on students We also have a trigger don't forget that incurs after inserts on students and that one checks whether the student's GPA is in a particular range and inserts the student into the apply table And we're fine to experiment a little_binary_digit with how these two triggers interact Now I've deleted all the data from the existing data from the student and the apply tables just to simplify looking_at what_happens with these triggers Now let's try inserting some students All three of the proposed insertions have GPAs that are in range for automatic application However the second and third students have high_school_size that are out of range So when we run the inserts hopefully the trigger will disallow the second and third insert for the high_school being out of range and the first trigger will activate automatic applications for Nancy Actually you know what There's a little hint down here what_happened We can see that the first insertion occurred Third And the second and third zero rows were affected So we can already see that it did what we expected but let's go take a look_at the data and make_sure that it did So if we refresh the apply we'll see that in fact there was one application to Stanford and one at Massachusetts_Institute of Technology for let's make_sure that's our student That was Nancy and the other two students that we attempted to insert were not inserted into student and they had no apply record inserted either Now let's change this one to an after trigger We're still going to enforce the same constraints that high_schools need to be between and but we're going to check the constraint after couples have been inserted In that case it's not sufficient to just ignore the operation that's under way because the couples been inserted already What we'll do instead is we'll manually delete the tuple that caused the violation we'll delete from student the student that was just inserted Now we have two triggers that are activated at exactly the same time The one that checks this constraint and the one that does automatic applications Let's see what_happens when we perform some insertions We'll insert two students Again both of them have GPAs that are in range for automatic application However the second student's size of high_school is out of range It's So both students will be inserted but then our trigger that checks the constraint on high_school_size will we hope delete Rita So let's go_ahead and execute and now let's take a look_at our data We'll refresh and we see that as expected Quincy was inserted with no problem Rita was in fact inserted but then our trigger was activated and she was deleted Now let's take a look_at the apply table A ha So Rita's applications are there And if we look closely at the triggers and we recommend you do that you will see that both of them are activated by the inserts at the same time Now one of them is going to delete Rita but the other one is going to process the insert and insert Rita into the apply table As a grand finale we'll just show two triggers that are a little_binary_digit more_complicated and more realistic And you may want to pause the video to look_at these closely because I'm not going to go into them in great detail The first trigger automatically accepts students to Berkeley if they have a high enough GPA and they came from a large enough high_school So it will intercept insertions_into the apply It will check if those conditions are satisfied and have to use sub queries here to find the student's GPA and to find the student's high_school_size Again I urge you to pause the video and take a look to make_sure you understand what's happening If this student satisfies the conditions then their apply record is set to a decision of Yes and we apply that apply record by matching the student_ID and the college name Once again I've deleted all existing students and apply records just to keep the demonstration simple We're going to insert two students Amy and Bob They both have high GPAs but only Bob's high_school_size is big enough to grant him automatic admission to Berkeley Then we're going to have our first student Amy applied to Berkeley and our second student Bob applied to both Berkeley and Stanford And we'll see that Bob is going to automatically be admitted to Berkeley So we've run the query and we take a look_at apply and we see that we have inserted the three apply records but only the second one and that's Bob's application to Berkeley had a decision that was modified And our last trigger is also more complex and I guess sort of realistic What this trigger monitors is enrollments of colleges and at the point that an enrollment of a college is increased past the threshold of then some action will be taken on its application Anybody who's applied to EE at that college will be deleted from the application table and all remaining applications where the decisions have been yes are going to have the decisions set to undecided One thing that is interesting about this trigger is it does monitor the dynamic behavior of the database and look for a threshold to be passed And that's something that triggers can do that simply cannot be done with constraints Some of our other triggers many of them as you saw were more effectively monitoring static constraints We set the threshold at We see that Standford has students and the others aren't very close to the threshold For this trigger I've also repopulated our database so we have a whole_bunch of apply records So let's go_ahead and make an update and see what_happens We're going to increase all college enrollments by That will cause Stanford to pass the threshold of and it should activate the trigger and change the apply table When we go look_at the apply table we see that indeed the electrical engineering majors have disappeared and all of the other applications to Stanford that were formerly yes have now been set to undecided That_concludes the first half of our demonstration of triggers In the next_video we'll be exploring even further the interesting interactions that triggers can have and how they behave See you then If you made it through that long first part of the demo of triggers here we are in part two As a reminder our introduction video about triggers used the Structured_Query_Language standard but no database system actually implements the standard and most systems deviate considerably Postgres is the closest and is the most expressive with triggers However it has a kind of cumbersome syntax so we're not using it for our demos SQLite follows posts risks and is also quite expressive MySQL is considerably less expressive Mostly with some limitations it has and how triggers can interact so again we are using SQLite for our demo and just a few of the differences from the standard It has role level triggers only no statement_level triggers and an immediate activation semantics For each_row can be specified in triggers but if it's not there it's still a row_level trigger It doesn't have old tables or new tables since it has immediate activation semantics only And it doesn't have a referencing clause the variables old and new are automatically bound in every trigger to what would be old_row and new row if it had a referencing clause And finally the action part of SQLite triggers are Structured_Query_Language statements embedded in begin and end blocks Overall here's the long list of features that are covered in our two part demo We covered the first five features in part one so in part two we'll be talking_about self triggering triggers we'll be talking_about triggers that have cyclic behavior conflicts that means when we have multiple triggers triggered at the same time and finally nested trigger invocations when the action part of a trigger triggers additional triggers And finally we'll also add to the a demo a demonstration of SQLite row_level immediate activation And again that doesn't follow the standard Its a binary_digit subtle but let_me review it In the Structured_Query_Language standard all triggers are activated at the end of the commands even if they modify multiple rows Whereas in SQLite and also in MySQL triggers are activated immediately after each row_level modification The tables that we use for this demo are very_simple because we want to focus specifically on trigger behavior so we'll just be using four tables each with a single attribute Let's go to the demo Our first trigger is a simple one that demonstrates triggers triggering themselves It's activated when we have an insertion on T It has no condition and after that insertion it inserts an additional row into T that contains the value that was inserted plus one So let's go_ahead and insert a row and see what_happened The first tuple in T the one is the tuple that we inserted with our command The second tuple the two was inserted automatically by the trigger However we expected additional trigger activations to insert additional tuples It turns_out that SQLite as a default doesn't allow a trigger to be activated more_than once in a trigger processing session presumably to prevent infinite behavior But if we'd_like we can toggle a variable in the SQLite session called recursive triggers If we turn recursive triggers on then that checking is turned off and triggers can be activated arbitrarily many_times during a trigger processing session Now with recursive triggers on this trigger would in fact activate itself indefinitely inserting more and more tuples into T Actually what would happen is eventually an_error is generated But we can modify the trigger in order to put in a limit Specifically we'll add a condition to the trigger that says we'll only perform the action when the number of tuples in T is less_than ten So what we expect now is when we start_by inserting a tuple we'll insert one two three four five and so on but when the size reaches ten the condition will not be satisfied The action won't be executed and trigger processing stops I deleted the two tuples from T so now let's once again start_by inserting a single tuple with a value one and let's see what_happened We take a look_at table T we refresh and we see that indeed ten tuples were inserted the first one we inserted all the rest by self triggering of our single trigger and when it got to the size of ten triggering terminated because the condition part of the trigger was not satisfied Now let's generalize the idea to three triggers that trigger each other in a cycle The first trigger is going to be activated by inserts on T and when there is an insert on T it will insert_into table T the same tuple except incremented by one That will activate trigger R which is triggered by insertions on T When T is when R is activated it will likewise insert_into table T Again the value that was inserted into T incremented by Then trigger T will be activated by those insertions onto T and it will go_back and insert back into table T again incrementing value So let's start as usual by inserting into the first table the value one and let's see what_happened Now let_me_mention that before I started this example I turned the recursive triggers flag off So here is our original Tuple That activated a trigger and it inserted two into Table_T We can go_ahead and see that That in turn activated a trigger that inserted a three into Table_T and then we go_back to Table_T and the four was inserted But because SQLite has as a default the limitation that each trigger is activated only once in a trigger processing session at that point trigger processing terminated Now we're going to do the same thing that we did last time We're going to turn recursive triggers on again using this command for our SQLite session and then we're going to modify our triggers to put in a termination condition So we only need to put the termination condition in one trigger and we'll put it in the third trigger and this time we'll allow it go a little farther so the third trigger will add a condition that when the size of the first table T is less_than then the trigger will go_ahead and execute its action but when the size of T exceeds then it won't and that will break the cycle and trigger processing will terminate As always lets start things off by inserting the tuple into table T So when we look_at table T we see our original insertion This inserted a into table T which then caused a to be inserted in table T and then back to a four being inserted into Table_T And that triggered a five being inserted into T and so on So we can see the trigger behavior Now we did put in a limitation so when we go look_at the size of T we'll see that it got to exactly and then when the size of T exceeded the third trigger's condition was not satisfied and that caused the action not to be executed which brought everything to a halt Okay that's enough of cycles Now let's take a look_at a situation where We have two triggers activated at exactly the same time and they perform updates to exactly the same portion of the database Both of our triggers are activated by insertions_into table T And we're gonna again start trigger processing by inserting one tuple with the value The first trigger has no condition It updates the contents of T to set all the values to The second trigger checks to see if there_exists a in table T and if so it sets the values to So these two triggers are designed specifically so we can see which one goes first If trigger R goes first it will set the value to The condition part of our will be true and it will then in turn then set the value to However if trigger R goes first it will not find a in table T so it's actually will not be executed and will then our trigger R will go next and we will set the value to Okay so let's go_ahead and do our insertion with these two triggers defined and see what_happens We insert the value we take a look_at T and we discover that we have the value of So what does that tell_us That actually tells_us that trigger R went first We performed our insertion It looked to see if there was a two in the table There wasn't because there was just a one It didn't execute its action Then trigger R went and it modified the value to be two So that's interesting the second trigger went first So as an experiment let's try reversing the order in which the triggers are defined We'll create trigger R first and R second I've deleted the tuple that was in T so once again we'll insert a T to get things started we go take a look and now we see indeed that we have the value three Going back to look_at our triggers a value three means that trigger R went first it set the value to two that made trigger R 's condition true and then it set the value to three So interestingly in SQLite when you have triggers that are activated at the same time it appears that the trigger that was created second is the one that is going to go first Now let's experiment with nested invocation of triggers To make things exciting this time I'm going start_by inserting a zero into table T instead of a one And we're gonna be using all four of our tables T through T with three triggers The first trigger is going to be activated by the insertion into T And it will simply insert a tuple into table T and a tuple into table T What we're going to see and what we're specifically experimenting with is that the first insertion will in fact activate triggers before the second insertion is performed The first insertion into T will activate our second trigger inserts on T and this trigger will in turn insert_into tables T and T and it will insert the values too So this will occur in a somewhat nested fashion and in fact in this action we'll see the first command will be executed and it will in a nested fashion activate our trigger R which will insert simply into table T the value three So let's get started and again as I_mentioned for excitement we'll be inserting a zero this time and let's go see what_happened to our tables So table T has just the zero that we inserted as expected Table_T has just a single tuple with the value one which is exactly what we expected this trigger is only activated once Now let's take a look_at table T Table_T has a and a and they're in that order And what that tells_us is that this insertion here activated by trigger r happened before this insertion here from trigger r and that this is what demonstrates the nested invocation of the triggers and just to look_at the last table we will see something similar in T We again can see nested invocation because the two occurs after the first insertion of three and then we have the final insertion of three So this is a binary_digit complicated You_might want to look_at the video again or or even better yet download the triggers and try them yourself Our last example is designed specifically to demonstrate the immediate activation of the low level triggers implemented in SQLite specifically I've populated table T already with four tuples with the value one and what my trigger is going to do when I insert additional values into T is this insert_into table T the average value in T What I'm going to do is insert a batch of tuples into T in fact I'm going to insert four tuples with a value of two So when I'm done I'll have four s and four s in table T If triggers followed the Structured_Query_Language semantics of being activated the very end of the entire statement So the entire batch of inserts then the average values that we insert_into T would reflect the average of the four s and the four s so it would be the value However what SQLite does is activates the trigger after each tuple level insertion So as we insert the s one at a time an average will be inserted into table T two and the first average will reflect have one in table T the second will reflect having two s in table T and so on I think you'll get the idea when I show what_happens So here is the command I'm going to use to insert the four s into table T I'm going to insert_into T the result of this subquery which takes every value and adds So let's go_ahead and do it And let's take a look_at table T And we see now that indeed we have the four ones and the four twos So the real story is told when we look_at table T And here we see indeed that we do not get four averages that are which is what we would have gotten with the Structured_Query_Language standard semantics Instead we saw that for each two that was inserted we inserted the average into table T reflecting the number of twos that were in the table so far Again fairly_complicated you might want to watch this example a second time or download it and try it at home So that concludes our rather long coverage of triggers We've seen that it can actually be quite complicated when triggers interact with updates on the database when they perform updates and especially when they interact with each other Now the reality is that a lot of times that triggers are used in applications is for very_simple purposes We might have a couple of triggers that are enforcing simple constraints They don't interact with each other and they don't interact with the database in complicated ways And I've shown a few of those triggers early on However I did want to make a point that when triggers are used for more_complicated purposes when they interact with each other and with the database in complicated ways it is important to be very_careful to fully understand how they behave This sequence of videos introduces the very importance concept called views In this video we'll talk_about defining and using views And then we'll give a demonstration The next sequence of videos we'll talk_about modifying views and then we'll also talk_about materialized_views Views are based_on a three level vision of databases which are known_as the physical layer the conceptual layer and the logical layer At the bottom level we have the actual data stored on a disk So here's my picture of a disk and then we have disk pages And on those pages is stored the actual data So that's the physical layer The next layer up known_as the conceptual layer is the abstraction of the data on these disk into relations And that's what we have been talking_about for a lot of our videos What we are introducing now is what's_known_as the logical layer And that's a further abstraction above relations into what are known_as views As we'll see a view is defined as a query over relation So it's still in the relational data_model So we showed one view that is query over two relations here we could have a view that's query over views and maybe views together with relations It's a quite general concept So until now we've_seen applications and users of those applications operating_on the relations at the conceptual level But users and applications can also operate_on views at the logical level So why do we introduce this additional level on top of this conceptual layer which is the layer of relation Certainly we understand why we want to use relations and not talk_about pages like this but what's the benefit of adding views on top of relations Well one benefit is that it allows_us to hide some data from some users when the granularity of which we want to hide data doesn't correspond exactly to relations Another is that it might make certain queries easier and more natural when we query over the views rather_than directly over the relations And the third reason is modularity of data base access because we can think of these views as modules giving_us access to portions of the database Again when that granularity that we want doesn't correspond precisely to relations One thing that I should mention is that real applications do tend to use lots of views lots and lots of views so the bigger the application is the more need there might be for modularity for ease of certain queries or for hiding data and views are the mechanism to achieve those goals So let's talk_about how views are defined and used It's actually a pretty_simple concept To define a view which we'll call V we specify a query of we'll just say View query in a query language typically in Structured_Query_Language over a set of existing tables As we'll see those could even be used The schema of the view then we can think of it like a table is the schema of the result of that query Now let's say we want to run a query Q Over our database This is not the view query This is just some ad hoc query over our database and we want to reference V in that query So we can once V is define reference it just like it's a table and conceptually what we can think of is V being say a temporary table that is assigned to the result of running the query over the current instances of R through_RN so we've now populated V like it is a table and then we evaluate Q and we can simply refer to V as we evaluate Q So that's what_happens conceptually Now in reality what_happens is the query Q that references V is actually rewritten to use the tables R through N that are in the definition of V Instead of referencing V and we'll see that in our demo and as I_mentioned the RIs these tables that are referenced in the view definition can themselves be views as we saw in our diagram earlier with those purple squares The syntax for Creating a view in sql is very_simple We use the keywords create view we give the view a name and then we specify the query for that view and that query would be in standard sql Now as I said the schema for the viewed that will become known_as the name is the schema of the result of this query If we want to actually name the schema so rename the attributes in the results of the query then there is an alternative syntax that actually lists the attribute names for the view Once this command has been executed from this point on we can use new name as it's a regular table in the database So let's move_ahead now to the demo In the demo we will be using our standard simple_college_admissions database As a reminder in case it's been a while or you're new to our videos it's a database with three tables One with information_about colleges that have a college name statement enrollment One with information_about students and finally records showing that students have applied to colleges As_usual we have our four colleges Stanford Berkeley Massachusetts_Institute of Technology and Cornell We have a bunch of students with their name GPA and the size of their high_school and finally we have students_who have applied to colleges for a specific major and there's a decision of their application So let's create our first view Our view is called Computer_Science_Accept and it contains the IDs and college names of students_who have applied to major in Computer_Science and the decision was yes So it's the IDs and names of students_who have been accepted to a Computer_Science major We execute the command that creates the view and now we can take a look_at the view We go to our view manager and we see Computer_Science_Accept and we double click and here are the contents of the view Now even_though it looks_like it this view is actually not stored When we ask to see the contents of the view or as we see momentarily run a query that uses the view that command or query is rewritten based_on the view definition that we gave So now let's run a query that uses the view as if it's a table Our query is going to find students_who were accepted to computer_science at Stan ford and have a GPA less_than So here's our query written in Structured_Query_Language and we can see here in the From clause we are referring to Computer_Science_Accept as if it's a table So we'll join the student relation with the Computer_Science_Accept relation We'll make_sure that the college is Stanford and the GPA is less_than three point eight We run the query and we see that Helen is the only student who satisfies the criteria So what actually happens when we run this query that refers to a view First now I'm going to talk_about what_happens conceptually and then I'm going to talk_about what the system actually does So conceptually we can think of the query referencing the view as triggering a process where we create an actual table with the contents of the view and then we run our query over that table So I've demonstrated that here We created temporary table T and that query contains exactly our view definition So this was the definition of our view Computer_Science_Accept Then we take the query that we want to run this was our query and we replace the reference to the view with the reference to the temporary table T otherwise everything is exactly the same We'll run that query and then we'll drop our temporary table So if we execute all of this again we will see our result is Helen what_happens underneath in most systems is that the query that we write is actually rewritten not to reference a view or to reference a temporary table but actually to reference the what are known_as base_tables the ones that are used in the view definition So let's take a look_at that process So here we've taken our reference to the view Computer_Science_Accept and we've created a sub query in the from clause I_hope you remember that from the Structured_Query_Language videos So we've just taken the definition of the view It's right here We've put it into a sub query we've named it Computer_Science_Accept and then the rest of the query can remain the same So this is actually convenient a very useful feature where we use the sub queries in the from clause So we'll run the query and once again we see Helen Well believe it or not this is not exactly what the system tends to do either This is a very easy and convenient rewrite but unfortunately underlying database_systems don't always execute queries that have sub queries in the from clause in the most efficient way So a sophisticated system will actually do a rewrite into a simpler query The rewriting_process is more_complicated The query ends up being simpler And here's that query It's actually a simple query over the student and the apply relations based_on the definition of our view where we're finding here from the first part of our wear comes from the view definition the major has to be Computer_Science and the decision is yes Then we have the joint condition that comes from the query and the last two conditions are also from our query So you might want to look back and think_about it but this is exactly what we were asking for when we were defining the view and then running a query over that view So when we run this we should once again get Helen and we do Of_course is that as a user you don't have to worry_about any of this You just define the views and you use them and it's up to the system to have an efficient and correct implementation As I_mentioned in the introduction we can define views that reference other views and that's what I've done in this example This example finds students_who are accepted to computer_science at Berkeley and they come from a high_school that's greater_than So I'm calling the view CS Berk and the view is a join of the student relation with our Computer_Science_accept view It's going to join based_on the IDs So the accept view is already going to be finding students_who are accepted out of college for computer_science and then we're going to constrain it to those who are accepted to Berkeley and from a high_school_size greater_than So we run this command but this doesn't show us the result it just creates the view Now we go to our view manager and we see that we have two views and the Berkeley view finds that there were two students_who were accepted to Berkeley They come from a large high_school or high_school greater_than although we're still returning their GPA in the view So now that the view CS Berk has been defined we can run a query on CS Berk So let's find those students_who have been accepted to Berkeley Or computer_science with a high_school_size greater_than five hundred and that have a GPA greater_than three point eight and we see that that's Amy So this is a beautifully simple query But what's happening underneath is a little_binary_digit complicated because this is defined over one view which itself is defined over other tables So the rewrite process in this case is a kind of recursive process of expansion Let's take a look_at what the query would look like rewritten So now that simple reference to the view Computer_Science Burke has It's been replaced by this whole section here lines through And you can see sort of the layer process At the outer layer we have The definition of Computer_Science Burke which itself referred to Computer_Science_accept So we had in of Computer_Science Burke not this expansion here but just Computer_Science_accept And then we had the additional joint condition and the reference to it being Berkeley and the size High_School and then this Computer_Science accepted self is expanded to its view definition So now when we run the query we should still get the same result and we do I'm not going to show it here but this query could similarly have a flattened rewrite into a join as we saw when we saw the expansion of Computer_Science_accept So now we have the view Computer_Science berk that's defined over the view Computer_Science_accept What happens if we try to drop the view Computer_Science_accept We get an_error here We can't drop the view because other objects depend on it So that's a nice error That's from the Post Grist system Unfortunately if we used one of the other two systems Structured_Query_Language Light or My Structured_Query_Language what would actually happen is it would allow_us to drop the view and then when we attempted to refer to Computer_Science berk at that point we would get an_error Because when it did the rewriting_process it would discover that Computer_Science_accept no_longer exists Now let's create what_happens to be one of my favorite types of views We're going to take our three tables in the database and we're going to join them together so that we have all of our information together in one table So we're going to apply our join conditions to get the colleges coordinated with the applications and the students and then we're going to take all of the attributes involved So let's run that view We'll call it mega and let's take a look_at what we have in mega Here it is So this is all of the information in our three tables joined together and we'll see that makes queries quite convenient Now let_me remind_you that this information the view contents are not actually stored So this is a toy database it wouldn't be a big deal to store it here But if we have a very_large database where it's crazy to think_about actually storing the join of everything together that doesn't mean we cannot create the view because the view is just a logical concept and again can ease querying as we'll see now Once we have our mega view we don't need to think_about joins We simply state the conditions that we want in the data in our query results So for example if we want to find high GPA applications to a Computer_Science major at large colleges We just use mega in our clause and give our conditions high GPA Computer_Science major high enrollment and we run the query and here's our result And incidentally if you're worried about the rewrite of that query it's actually no big deal That one's going to rewrite in its flattened version to adjoin of the three tables with the join conditions and then the conditions that we specified And again if we run this query we'll get the same result So that completes our demonstration of defining views and using views in queries As you can can see it's a pretty straight_forward and convenient concept for the application developer and are used very commonly for modularizing applications for making queries easier to to formulate and for authorization purposes This series of videos covers the topic of modifying views Now I have to admit that the amount of coverage is a binary_digit disproportionate compared to defining and using views Commonly people only define views and use them and don't try to modify them in applications but when views are modified the issues become a binary_digit tricky So it is important to cover the topic First a reminder of why people use views They use them to hide data from some users so for authorization purposes they make some queries easier they help you modularize your database applications and as I've said several times our real applications do use lots and lots of views Now querying views is relatively straight_forward as we've_seen Once a view has been defined and given a name say V then in our queries over the database we can reference V as if it were a regular table What happens underneath is that when we reference V in a query it's rewritten to use the tables over which V was defined and we'll call those the base_tables Those can actually be regular tables or they can be other views Now what about modifying views Once a view has been defined can we modify it just like it's any table in the database Well in one way that doesn't make_sense because V isn't stored so it's not a table that we can go in and update insert tupples modify tupples or delete tupples But in another way it absolutely has to make_sense because the idea of views is that some users or application developers are seeing those views as their only view of the database We saw that three level idea of databases We have the physical layer the disc the conceptual layer the relation and then the logical layer which again some applications see as their access to the database Well our solution is actually parallel to what we do with queries When we have a modification command that involves a view V we'll rewrite that modification command to modify instead the base_tables over which V is defined If those base_tables happen to be views themselves that we have a recursive process of rewriting those modifications to further go down to until we get to the base_tables that are actually the tables stored in the database So maybe this is no big deal We saw in our demo that modifying queries that reference views into queries that reference the base_tables is a relatively straight_forward process Well I_am going to say right up_front it's not quite as straight_forward when we are talking_about modifications So let's draw a picture to see what's going on Let's say we have our view V here and V is defined based_on a query over its base_tables And for now let's just assume that those are stored actual relations So it's defined over relations R to RN Now someone comes along and they want to modify V Now V is just a logical concept but the user thinks of V as a table so they write a modification command so that would be say insert delete or update_command using Structured_Query_Language language And they're imagining that V is a stored table so they're imagining that the result of that modification command is going to be a new table V prime What needs to happen is down here at the bottom that modification that imaginary modification to V has to be translated to actual modifications on the base_tables R through_RN So now we modify one or more of the base_tables to R prime through_RN prime And now the idea is that our imaginary V prime then is the same query It's the result of this same query V's definition over the new value the new R prime through_RN prime the updated base_tables So if we can always figure_out how to translate this modification up here into the modifications down here so that the square diagram holds so that the resulting modifications here give_us the effect we wanted upstairs then we're in good shape So the question is can we always perform this translation the modifications so the square diagram holds And the answer is actually usually yes Usually there is a translation that works for us and we'll see some examples of that in our demos The problem actually is that there's often many such translations and so we don't know actually which one the user intended So let_me give an extremely simple example Let's_suppose that our relation R has two attributes A and B and our view V is defined as the projection on A of R Let's say that our current contents of relation are just one tuple to tuple In that case the current contents of view V are just the tuple Now let's say the user comes along it's a user who is operating_on the database through view V and they say insert_into view V please the tuple three So we need to translate that insertion which is up here into insertion or some modification on the base_tables so that the view will when we're done contain the tuples one and three Well we can certainly do that The problem is what exactly do we insert We could insert for example We could insert and so on So there's actually an infinite number of translations that will create the tuple three in the view Here's an even more extreme example Let's_suppose we have a relation with one attribute and our view be the average value of that So if for example our relation has the values then the average at this point would be three Now let's say that the user comes along and says let_me update that average I'm gonna set that average to be seven Well how do we update the base data so that its average is now seven Well as you can imagine there are many many many actually an infinite number of ways to update the base data so that the view average would now be seven So that's the crux of the problem How do we decide which modifications to make to the base_tables so that we get the desired modification to the view Correctness is not so hard to achieve but resolving ambiguity can be Existing systems have actually taken a fairly different approach as to this problem Again to specify the problem we have modifications specified on a view V We need to rewrite those to modify the base_tables so that when we have our view of over new base_tables that reflects the desired new state of the view One approach is that the view creator actually specifies that rewriting_process So they will say you know when somebody tries to insert_into the view here's what we need to do on the base_tables if they try to delete here's what we do and update here's what we do So the positive of this approach is that all modifications can be handled because the view creator is going to specify exactly what_happens in the case of those modifications The downside is that there's_no guarantee of correctness meaning there's_no guarantee that that square diagram is adhered to and we'll see examples in our demo of where it's not or even that the translations are meaningful So we're relying on the view creator to create correct and meaningful translations explicitly The second approach is to restrict the views and modifications that are allowed so that the system can automatically perform the translation into modifications on the base_tables that are correct meaningful and and unambiguous So the plus now of course is there's_no user intervention and the result is going to be correct The downside is that the restrictions on the view and the modification combinations are fairly significant as we'll see So the first approach is actually enabled by a type of trigger that's known_as instead of I alluded to those triggers briefly in the trigger video but here is where we are actually going to see them in action In postgres there's a concept called Rules that's very similar The second approach is actually the one that's adopted by the Structured_Query_Language Standard and the Structured_Query_Language Standard gives a very rigorous limitations on what views can be modified Systems vary in what they implement Most of them are actually a little_binary_digit more flexible than the standard and we'll see that in upcoming demos as_well In this video which will mostly be live demo we'll talk_about modifying views through triggers As a reminder once we've defined a view say called V we'd_like to be able to run modification commands over V as if it were a regular table Now since V is not a regular table It's just a view definition what_happens is that the modification commands_against V are rewritten to modify the base_tables over which V is defined and as we talked_about previously unlike with queries overviews we can't in general automate the process of rewriting of modifications on views into modifications on the base_tables As we discussed in the introductory_video there are two basic approaches to dealing with the fact that the translation can't be automated One is to have intervention by the view creator where the rewriting_process is specified as part of creating a view if we want to allow the view to have modifications against it and the other is to restrict the allowed views and modifications so that the the translation can be automated In this video we're talking_about the first case and specifically we're going to see how instead of triggers A special type of trigger can be used to specify the rewriting_process to enable modifications against views We'll be using our standard simple_college_admissions database for the demonstration where we have our usual three tables the college table the student table and the table containing information_about students_applying to colleges So let's turn to the demo As_usual we'll start with our four colleges Stanford Berkeley Massachusetts_Institute of Technology and Cornell our bunch of students and finally the records showing our students_applying to colleges for a specific major and a decision for that application Some of the views we use in this demo are going to be the same views that we used in our demo showing how we define and query overviews and now we are going to focusing on the ability to run modification commands_against views Our first views is one of those it's called Computer_Science_accept and as before it contains the student_ID and the college name where the student has applied to major in Computer_Science at that college and the decision is yes We've now created the view and we can take a look_at its contents and we'll see that we have a few students_who have been accepted to Computer_Science at a few colleges Now what we'd_like to do is run a delete command against the view Let's say that we want to delete student the two records of student being accepted to major in Computer_Science So we would run that command like this very_simple Delete from Computer_Science_accept or student_ID equals one two three But when we run the command we get an_error because Structured_Query_Language light does not allow_us to modify views So here's the trigger that we're going to create to enable deletion commands to be run on the Computer_Science_accept view This trigger is going to intercept those deletions and it's going to translate them into deletions on the base data over which Computer_Science_accept is defined So let's take a look_at what we'd_like to happen If a user asks to delete some data from the view and again let's go_back and look_at our view If the user wants to delete one two three Stanford from the view then what we want to be deleting is the apply records that gave us these tupples in the view So let's go take a look_at the apply relation here and we'll see that this was the first record here where the student applied to C S at Stanford and the decision is yes and the rd record where this person applied to Berkeley and C S is yes We don't want to delete the other records for the student because those weren't contributing to our Computer_Science_accept view So we'd_like to translate that deletion on the view into a deletion over apply such that the new value of the view no_longer has those first two tupples for student So let's see how that's implemented by our trigger Now the one important thing to know that happens and this is the one real contribution the system makes to what we're doing here to enable view modifications is that we do have access two of the deleted tupples So this trigger it's actually a row_level trigger so it's going to be run once for each deleted row and when we run the trigger We will have this special variable old that contains the contents of the row that the user is asking to delete So in the action portion of the trigger that we can refer to the variable old to the student_ID and college name that are in the accept view as if we were really deleting that tupple from the view so again let_me reintegrate the system is going to bind to variable old the tupples to be deleted from the view using the scheme of the view even_though those tupples aren't physically in existence And we'll use the value from those to translate to the deletion we want to perform on the apply table Very_specifically for the Computer_Science_accept view if we have a student_ID college name combination to be deleted from the view then what we'd_like to delete from the apply table is the tupple that has that student_ID that collage name a major of Computer_Science and a decision yes So the person_who writes this trigger which is presumably the person_who created the view is going to use the combination of what was defined in the view major equals Computer_Science in decision equals yes combines with the value getting to be deleted from Computer_Science_accept I_hope that makes some amount of sense So now that we've created this trigger let's go_back and let's try to perform our deletion operation again this time it looks_like we had success so lets go_ahead and see what_happened We go to our view manager We see indeed those couple tupples are gone Now remember this table isn't actually stored this when we look_at the view its actually running the view query against the data So when we look_at the apply table we should also find that's the really important thing that the tuples corresponding to being accepted to Computer_Science are indeed gone And it's the deletion of these tuples from apply that causes the tuples to be deleted from the view and that was the deletion on apply was because we installed this trigger that intercepted the deletion on the view and Translated to the deletions on the base_tables So that worked out pretty well now suppose that we want to enable update commands on our view Let's say we want to change This student 's acceptance to Computer_Science at Cornell to be an acceptance to Computer_Science at Carnegie Melon University instead So here's the update_command we run it's very_simple we find student_ID and we change the college name to CMU and we're updating the view Computer_Science_accept Of_course when we run this we again an_error because SQLite won't allow_us to perform an update_command on a view except if we have a special instead of trigger that intercepts that command and translates it So that's what we'll do next is create a trigger So here's the trigger but what I'm going to demonstrate with this trigger is that nothing is forcing the trigger writer to actually do the right thing when a modification is translated to the base_tables So we are going to intercept updates to the Computer_Science_accept view and in fact you might remember from triggers that we can be even more specific with update triggers and say which column is updated So this particular trigger is only going to update is only going to intercept commands that are trying to update the college name column of the Computer_Science_accept view And then it's going to translate them to update the apply table Now what we have when this trigger is run and again it's run once for each roving updated is we have old and new that are bound to the old value of the updated view tuple and the new value of the updated view tuple And only the college name will have changed in this case So we're going to update the apply the relation We are going to set college name to be the new college name and that's going to be for the tuple where we have the old student_ID and the old college name that finds the tuple to update however whoever wrote this got a little wacko instead of writing the condition that it's applications to Computer_Science where the decision is yes they've made it applications to EE where the decision is no But nothing prevents us from writing this trigger which performs this incorrect translation So now that the trigger is installed we can run our update_command before we do that lets go look again and remind ourselves what we have we have has applied to Cornell and we're trying to change that to be Carnegie_Mellon And in our apply relation we have has applied to Cornell of Computer_Science yes that's why is in the view also by the way has also applied to Cornell offer EE and the decision on that one was no So back to our update_command Now we'll run the command and now that we have our trigger installed the command succeeds And let's go see what's happened to our database Here we are in Computer_Science_accept and nothing has changed has still applied to Cornell We wanted it to be CMU but we wrote the wrong trigger so it did the wrong thing And specifically if we go look_at apply now we see that this record here this apply record has actually been updated So our trigger went in and modified that Cornell to be CMU because it matched the major being EE and the decision being no So again what I'm emphasizing here is that it is important to write the correct translation and nothing in the system is checking that that is the correct translation Of_course the correct translation would be if we wrote Computer_Science here and decision equals yes That would be the correct oops decision equals yes Here would be the correct trigger that we'd_like to install to manage updates on the college name of the Computer_Science_accept view So far we have been looking_at the Computer_Science_accept view and we've looked at deletions and updates Now we are going to look_at insertions and we'll look_at a somewhat different view This is a view of the IDs college names and majors of students_who have applied to major in either Computer_Science or EE Now let's take a look_at the Contents of our view in CSEE and we see that we have quite a number of students_who have applied to major in EE or Computer_Science and the colleges to which they've_applied Now let's_suppose we want to insert a new tupple into this view So we want student to have applied to Berkeley in Computer_Science We run the command and of course we're going to get an_error because we haven't yet installed the trigger that intercepts insertions_into the view called CSEE So here's the trigger and this one's pretty straight_forward It says instead of inserting on the view and again the system will provide to us the user's intended insertion on the view in the variable called new as part of the trigger so instead of inserting into the view we'll insert_into the apply table the student_ID the college name the major and then we'll put in null for the decision With that trigger installed we can now go_back to and our insert and perform it and look_at the contents of our view So here's our new tupple that we wanted insert the view and again this is not a start table this is just a result of a query over applied but it's there as we wanted it to be And if we take a look_at the applied table we'll see that was inserted with Berkeley Computer_Science and a null value for the decision But now let's see where things can go wrong Our trigger was blissfully simple but what if we put here that is going to apply to Berkeley in Biology And we try to insert that tuple into our view called CSEE that is suppose to be Computer_Science and E majors So lets go_ahead and run that insertion And everything went fine let's take a look_at our data Well here's our view and nothing changed our attempted insertion apparently didn't happen Well here is our insertion we inserted a biology major so what in some sense that's good we don't want our biology major to show up in our view On the other_hand our insertion trigger was activated and if we go_ahead and look_at the data specifically the apply relation we'll see that we did get a tupple insert_into apply That tupple just didn't show up in our view because it didn't satisfy the condition So presumably we don't want users to be able to writ insertion commands_against a view that affect the underlying database but don't get reflected in the view because they shouldn't be in the view in the first place So we need to write a better trigger to handle insertions_into this particular view So we are going to drop the trigger that we wrote and create a new one and and this one is going to use the when clause of triggers it's the first time we used it in this demo and it's going to check that the tupple that the user is attempting to insert_into the view has a major that's Computer_Science or that's in EE If the attempted insert doesn't have Computer_Science or EE as a major then the trigger won't do anything But if does then it will insert_into apply the student_ID name and major Now that we've fixed up our trigger let's go_back to our insertion command We've already done damage with student So let's try student this time Let's attempt to insert the student and then let's see what_happened If we go to the view that student is not in the view which is a good thing because it's a biology major But most importantly we hope that nothing happened to the apply table and just to be sure we have refreshed and we see that indeed nothing got inserted So that's the case that we would like I_mean maybe we'd_like an_error message to be generated but certainly if the user attempts to insert_into the view a tuple that doesn't belong in the view wouldn't like the database to be altered Now let's_suppose instead that student asks to major in EE Now when we run the command we should see in our view that the student has shown up and indeed they have So that insertion was intercepted and the underlying modification was made to the apply table so that the view now has student and if we go look_at apply we'll see again that successfully was inserted to apply So now insertions_into Computer_Science EE are doing the right thing So we've_seen examples of triggers to intercept now updates and insertions to views and do the right thing of modifying the underlying base_tables so that the view gets the desired modification The next phase of the video is just going to show a few examples quickly where we'll define views that have ambiguous modifications in fact where we might not even want to allow users to modify the contents of the view at all so our first view demonstrates that when a view includes aggregation it's probably not sensible to allow users to modify it This view finds for each high_school_size the average_GPA of high_school students_who went to a high_school of that size So we'll create the view We'll go and take a look and let's see what it contains So here are the contents of that view It has the different sized high_schools represented in our database and the average_GPA of students_who went to a high_school of that size So does it make_sense for a user to modify this view I_mean maybe would a user want to write a command that says update the view and set the average_GPA to or the high_school_size as So going back and looking here we wanna change this average of GPA How would we have a rule for modifying the underlying data to change the average_GPA It really isn't very sensible and making insertions and deletions to this view well maybe we translate deletions to translate to delete every student who went to a high_school of that size but that's probably not what we want So fundamentally when a view includes aggregation it usually does not make_sense to allow users to perform modifications on that view Aggregation views are most useful just for running queries Here's an even simpler view where we may not want to make modifications This view just lists all the majors that are represented in our apply relation So we'll create the view and take a look_at it and we see that we have seven different majors in our relation Now would it make_sense for a user to say I want to add a new major to that view so I'd like to have students_who have applied to the chemistry major Well certainly looking_at the view it will be no big deal to put chemistry in here but would be the underlying change to the database What who you know who would be the student that's apply to that major and at what college So in this case what makes it not very sensible to update this view is mostly that we're just taking one column out of our table and also that we're eliminating duplicates from that column so to think how to translate this to the underlying base data we'd have to add a lot of of new values for columns just begin with and maybe there would be multiple tuples contributing to a new tuple in the apply relation Again you know we might allow deletions we could potentially say if delete a major then we delete every application to the major But that's not be what's intended by creating the view So when we have projections onto few columns or we have the distinct keyword That's another example where we may not want to allow users to perform modifications to the view And here's our third last example along these lines this one's actually the most complicated one The view that we're creating now called the non_unique is going to give_us all student tuples where there's some other student who had the same GPA and the same high_school and it's a different student So you may want to refresh your Structured_Query_Language to remind yourself how this works Let's go_ahead and create the view and take a look_at what's in it Here we find that there are three students_who were some other student has the same GPA and size_high_school In fact all three have the same GPA and size_high_school which is not surprising in our small data set So let's_suppose the user decides they don't want Amy in this view So they try to write run the command delete from the non_unique view where the student name is Amy So what deletions should be performed or what modification should be performed to the underlying data in order to get Amy out of the view Well of course we could delete Amy herself and that might be the most sensible But if we deleted Doris here and the other Amy I forgot to point out these are two different Amy's this one's if we deleted Dolores and the other Amy then this Amy would be deleted from the view too So there's something a little subtle going on on this particular example We are asking to delete from a view that just selects from students so you might think there is a one to one mapping so let's just delete the students that were asking to have taken out of the view But because the sub query references the same relation as the outer query the leading tupple reference in the outer query also affects the result sub query which can feedback into what's in the outer query So it's this interaction between the two references of the table that makes it pretty tricky to figure_out how a modification to the view should be mapped to the modification to the base table So we've_seen three examples of where it doesn't make a lot of since to allow users to write modifications commands_against the view And those are based_on constructs in the view definition And actually we're gonna see those same constructs pop up when we talk_about what the Structured_Query_Language standards says about views that are allowed to be modified but I do want to remind_you that in this video we're talking_about writing triggers that enable modifications to views And so really for all the views that I showed you we could choose to write triggers that intercept the modifications and perform some modifications on the base table but probably that translation would not make a lot of sense and that's the point I'm trying to make Not that you can't do it when you have the power of writing instead of triggers but you probably don't want to do it because it probably doesn't make_sense in the application Now let's return to views that we do want to allow it to be modifiable and so we're going to enable that through instead of triggers and the next example is more complex because it involves a join of two tables This view is called Berk and it returns student ID's and majors where the student has applied for that major at Berkeley We'll create the view we'll take a look_at its contents and we'll see that we have two students_who have applied to Berkeley one to major in Biology and one to major in Computer_Science Some of our students have gotten deleted from our database along the way through our examples by the way So let's say that we want to enable insertions_into this view So we want to be able to insert a pair that's a student_ID and a major and then insert_into our underlying tables in some fashion so that that tuple is now in the view So what we're going to do is assume that when someone wants to insert_into this view they are inserting for an existing student I'm going to actually check that's the case So if we have an existing student and we're inserting into the view then all we need to do is add a tuple to the apply relation that says that they are applying to Berkeley for the major that's specified in the insertion we've asked for on the view So we will write that as a trigger So here is a trigger it says instead of inserting on the Berk relation for each_row again we'll get one instance of the trigger for each inserted row We'll check first of all that the student_ID of the inserted tuple and again new is bound to the inserted tuple the student the tuple the user wants to insert_into view Berk We'll check that that student does exist in the student relation If they don't exist in the student relation we're just not going to do anything with the requested insert If they already exist in the student relation then we can insert_into apply the student_ID Berkeley because we know this is the view for students_who applied to for applications to Berkeley the major they're_applying for and null for the decision So instead of demonstrating just a single tuple being inserted I'm going to do something a little_more complicated and ask to insert a whole_bunch of tuples into Berk and we're going to see that each of those tuples is going to result in an insertion into the apply table underneath and of course the desired insertion into the view So the way that I'm going to generate a batch of tuples to be inserted is into the view is to run a sub query And a sub query's gonna take all student ID's that are not in the apply table for Berkeley so students_who have not yet applied to Berkeley and we're going to for each one ask that we insert_into the view that student and a psychology major application to Berkeley So lets first take a look_at our apply table to see who is going to be applying to Berkeley for psychology So we have two three four who has applied to Berkeley already and nine eight seven and their in our view By the way if your wondering why these three students down here didn't appear in our view it's because these were inserted as an artifact of our demo and we never inserted matching student tuples So these aren't going to participate in the joint So we just have two students in our joint All the rest of the students here who_haven't applied to Berkeley are now going to apply to Berkeley in psychology as a result of us running that insert command that's gonna be intercepted by our trigger So here's the trigger here's the insert command lets go_ahead and run the insert and now let's take a look and see what_happened We go to Berkeley and we do see indeed a whole_bunch of psychology majors now in our view and of course that's because we should have a whole_bunch of psychology majors now in our apply relation and here they are So back to our view what if we now want to enable users to delete from the view We're going to again assume that the intention is not to delete the student the student_ID table is gonna stay stable And what we're going to do is we're gonna delete the corresponding applications So here's the trigger to do that it's activated by deletions on the BERK view and the system again will bind in the desired deleted tupples from the view to the reserved variable old So the action the trigger will delete from apply where the student id matches the one to be deleted the college name is Berkeley because that's what we're looking for that doesn't come from the deleted tuple that's not present in the view but we know that's a condition from the view definition and the major has to match the desired deleted major So we'll ask to delete from Berkeley all the records were the major is Computer_Science So lets go_ahead first and take a look_at the view and we see that we have just one so we'll expect this tuple to be deleted from the view But that's going to happen because we are going to delete the corresponding apply record underneath So we go_ahead and execute the deletion We'll take a look first at the view Make sure that Computer_Science is gone and it is and then we'll go look_at it in apply We forgot to look_at it in first place but I think it was student And now has no_longer applied to Berkeley And finally let's enable updates to our Berk view update the student_ID But we'll allow updates to the major So we'll have a student that applied to Berkeley in say psychology decide they want to major on something else So here's the trigger And this one is going to intercept updates to the major on Berkeley so it won't intercept updates to the student_ID And again now we'll have the old and new values bound to the old and new imaginary tupples of Berk It's going to update the apply relation It's going to find the record where we have the student_ID and our update This could be old student id or new student id since it's value isn't changing So we find the student id we care about their application to Berkeley for the old major and we'll change that to be applying to the new major So let's take all those psychology majors and let's turn them into physics majors So with this one update_command we'll be asking to update most of the tupples in the Berk view each tupple one at a time will be intercepted by our trigger and it will modify the apply table for the corresponding record and turn the psychology major into physics We'll run the command and we'll take a look_at what_happened Here's our view and we see indeed that all the Psychology majors have turned into Physics majors And of course that's a result of the modifications to the underlined table apply where again we've_seen all the psychology is changed to physics Now back to our view I said we weren't going to allow modifications to the student_ID and the trigger that we wrote only intercepted modifications to the major So let's see what_happens if we try to modify the student_ID Here's a command that tries to set to in our Berk view And if we run the query then we do get an_error because we didn't implement any translation for modifications to that column The last portion of our demonstration is going to demonstrate the interaction of view modifications and constraints So I'm going to drop my apply table create a new version of it where we add to the decision column a constraint that the column is not allowed to be null Now let_me remind_you of a view we created a long time ago called CSEE that took the student's colleges and majors where the student had applied to major in Computer_Science or EE And lets look specifically at the trigger that we created to handle insertions_into this view So you might remember or you might not that we first created an incorrect trigger But here is the one that we have activated And the important thing to notice is that it intercepts insertions_into the view and then it inserts into apply a tuple that contains the student_ID the college name and the major and then puts null for the decision And this is where our troubles are going to occur because we've now created a version of apply that doesn't allow null for a decision So let's try inserting into our CSEE view The value is one to three Berkeley in Computer_Science and that will translate to an insertion to apply of the same values and null for a decision We run the insertion and we get the constraint violation So what_happened is the insertion into the view was intercepted by the trigger translated to an insertion to apply but that insertion generated a violation against our NOT NULL constraint on the decision column So it's doing the right thing but we'll never be able to insert_into the CSEE view now because we're always going to attempt to insert more values So if we want to have that not low constraint on the apply relation we'll have to modify modify our translation for CSEE so it doesn't generate the null_values And here is a second example along the same lines We're going to drop apply again and create a new version of apply This time we're going allow null_values and decisions So we've taken away that null constraint but we've added a key constraint to apply saying that the combination of student_ID college name and major must be unique So a student can't apply twice to the same college for the same major So because we've created a new apply relation it's starting out empty But let's insert some tuples but we're going to insert_into the apply relation via the CSEE view and its trigger So we'll insert student applying to Berkeley for Computer_Science and Berkeley for EE And again we're inserting these into the view We'll run that We'll take a look_at our view and we'll see that where is it here we go CSEE has those two tuples that we inserted Of_course those are intercepted by the trigger and created insertions_into apply And here's the apply relation which looks exactly the same but with a decision as null Now what we're going to do next is actually insert_into one of our other views I don't know if you remember the Berkeley view but that was the IDs and majors of students_who had applied to Berkeley and let's go take a look_at what_happens when we try to insert_into the Berkeley view Here's the trigger that we wrote to handle insertions_into Berkeley When the student that we're trying to insert does exist then we'll insert a record into apply having that student apply to Berkeley for the major that's specified in the insertion and a null decision which again is fine now because we removed that non null constraint So we're going to try to insert_into Berkeley Student majoring in EE and we're going to see that's actually going to violate our key constraint So let's go_ahead and we get a non_unique constraint because that attempted to insert_into the apply table Let's go look_at that table and attempted to insert in to apply Berkeley EE and that would have violated the constraint that we can only have one tuple for a student id card College name and major combination So again our attempt to insert_into the view was translated that's the earlier insert Sorry our attempt to insert_into the Berkeley view was translated into an insertion to apply that violated the constraint And as a grand finale let's try an update on our Berk view So let's_suppose that we want student one two three to major in Computer_Science in both applications to Berkeley and that will generate an update to the underlying apply table that should violate our constraint Just as a reminder let's take a look_at what_happens when we try to run an update_command on the major of the Berk view Here's our trigger that we wrote a while ago It intercepts updates to Berk and it translates them to update the apply relation find the appropriate record where it's an application to Berkeley matching the old major and the student_ID and it changes it to the new major So we're going to try to update a BERK We're going try to set let's find it here we're going to try set the major equals Computer_Science for student_ID So if we take a look_at what is going to try to the apply relation It's going to try to set these majors both to Computer_Science and that will give_us again a key constraint violation So let's go_ahead and try to run the command against Burk it'll translate to an update on apply and that update won't be allowed So that completes our demonstration of enabling view modifications using triggers so this demonstration was all run on the SQLite system The SQLite System does not allow any modifications commands on views directly so we saw every time we tried that we got an_error And the only way that a user can run modifications over views is if we have in place the special INSTEAD OF triggers that intercept the modifications that are attempted on the views and translates those to modifications on the base table Now the system when it does process the triggers generates for the modification on the views what the data would be in terms to binding those modification to the old and new variables that are available in triggers But the rest of the process is up to the user to write the actions that will take place when the user attempts to do a modification on a view and as we saw there are no checks in place to make_sure the user writes correct modifications that result in view updates However when the user does things correctly it can all work in a very_powerful and correct fashion This video will be a demo of automatic view modifications As a reminder when we've defined a view called V we'd_like to issue modification commands_against V as if it were any table Since V is just a logical concept and not a stored table the modifications have to be rewritten to modify the base_tables over which V is defined We saw in an earlier video that unlike queries overviews modifications overviews cannot be automated in the general case We've discussed that there are two different strategies to dealing with modifications to views and specifically how they're rewritten to modify base_tables One is to have the rewriting_process specified explicitly when a view is created The other possibility is to restrict the allowed view definitions and modifications over those views so that the translation can be automatic The second strategy is the topic of this video Now in the Structured_Query_Language standard there is a definition for restrictions to views and modifications for what are known_as updatable views The restrictions are on the definitions and they apply to inserts deletes and updates all together What I_mean by that is that a view is either updatable or it's not The Structured_Query_Language standard specifies four restrictions The first one says that the view must be defined as a select statement on a single table T That_means it cannot be a joining view Second of all when their attributes from T that are not in the view in other_words they're attributes that don't appear in the select_clause of the view those attributes of the table T must be allowed to be null or they must have a default value defined for them Third sub queries in the view must not refer to the table T but sub queries are allowed to refer to other tables and finally the view is not allowed to have group_by or aggregation In our demo we use our standard simple_college_admissions database with a college table student table and apply table We have as usual our four colleges our bunch of students and our students_applying to colleges for a particular major I wanted to mention that this demo is being run in the MySQL system MySQL among the three systems we're using is the only one that allows automatic view modifications SQLite and postgres both support view modifications through triggers or rules as we saw in our other video The views in this demo may look familiar there are more of the same views that we used in the original video on defining and using views For_example our first view is the Computer_Science_Accept view that finds students IDs and college names where the student has applied to major in Computer_Science at that college and the decision was yes We'll go_ahead and create the view and then take a look_at the contents of the view and we see that we have a few students here Now let's say we want to delete student_ID from the view So we would delete the first two tuples of the view So here's the command that says the delete from the view where the student_ID is Because our view definition satisfies the requirements for an updatable view in the MySQL system we can simply run the command and the system will take care of modifying the underlying base table so that we get the effect of this deletion So lets go take a look_at our view And we'll see that are indeed gone and if we look_at our apply table we see that tuples where has applied to Computer_Science are deleted as_well So the system automatically translated the delete command over the view into delete commands on the base table with the proper effect At this point let_me_mention that MySQL is actually a little_binary_digit more generous about the views and modifications that it allows than the Structured_Query_Language Standard requires and we'll see some examples later that make that very clear Now let's take a look_at insertions_into views We'll create our view called CSEE that contains the ID college name and major of students_who have applied in major in either Computer_Science or EE We'll create the view take a quick look_at it and we'll see that we have quite a few students Now let's_suppose that we want to insert a student into CSEE The student with ID applying to Berkeley for a Computer_Science So we'll go_ahead and execute the command and again because we have automatic view translation the system will translate that into an appropriate underlying insertion into a base table Let's first take a look_at the view and we'll see that we have indeed Berkeley Computer_Science and then we can take a look_at the apply table and we will see that the system apply inserted the corresponding tuple into apply with a null value Of_course let_me remind_you no insertions happening to the view The view is really just a logical concept So when we take a look_at the contents view we're actually running a query over the apply table We by the way being the system It takes care of that for us Now let's see a couple of examples where things seem to go wrong but don't worry we'll see a fix to them afterward Let's_suppose that we want to insert_into our CSEE view not AACS or EE major but a psychology major So we'll insert student to apply to Berkeley in psychology We'll run the insertion and it seems to have worked fine so let's see what_happens Well we look_at CSEE and obviously the student is not there because they're majoring in psychology If we take a look_at the apply relation we'll see that there was in fact an insertion So this doesn't look good because we don't want the system to automatically be inserting data into the base_tables that isn't appearing in the view and what we wanted was an insertion into the view Let's see a second example of this form We'll go_back to our accept view which is students and colleges where the student applied to a Computer_Science and the decision was yes And as a reminder here's what we have A few tuples of the student id and the college name So let's say we wanted to insert_into Computer_Science_accept the value Berkeley So we would want that tuple in Computer_Science_Accept based_on the definition of Computer_Science_Accept We ought to know that we could actually insert_into the apply relation the two values that were specified here along with Computer_Science and yes because this is the only two missing values in the apply relation Let's go_ahead and execute the insertion and see what_happened We go to our view manager we look_at Computer_Science_accept there's_no sign of but let's take a look_at apply and we see that the system actually did apply did insert Berkeley into the apply relation that's the translation but it wasn't smart enough to put Computer_Science and yes in here so again we have a kind of disappearing insertion Not really what we want to happen when we try to insert_into the view So I_mentioned that we do have a fix for that We can add to our view definitions something called with check_option And let's call this Computer_Science_Accept Two When we add with check_option when the system automatically preforms a translation it actually checks to make_sure that the translation properly inserted the data into the view So let's go_ahead and create this view and then let's perform our insertion again So let's try inserting a tuple into Computer_Science_Accept Two the version of the view with the check_option and we get an_error because the system will detect that the translated insertion into the apply relation would not produce a tuple that appears in the view We can similarly create a corrected version of our CSEE view where we add the check_option We'll call it it CSEE Now let's try two inserts into CSEE one where the student is majoring in psychology and that should generate an_error and one where the student is majoring in Computer_Science and that should be an okay insertion We'll go_ahead and execute there as we see where the first one generated an_error and the second didn't so to go_ahead now and take a look_at CSEE and we'll see that student is in that view and actually that student is also going to be in the CSEE view because the result of the correct insertion was the insertion of student into the apply table underneath Now let's take a look_at a few views that aren't allowed to be modified The first one finds the average_GPA of students based_on their high_school_size So let's go_ahead and create the view We'll take a look_at it And we'll see that it contains for each high_school_size the average_GPA of students_who attended this high_school of that size Now let's_suppose that we decide we want to delete from that view the tuples where the high_school_size is less_than five hundred We go_ahead and run that and we see that it's not updatable according to our system And if you think_about it it really doesn't make_sense to try to delete tuple from this view because what would you delete I suppose you could delete all the students_who went to a high_school of that size but that doesn't seem to be what the user would probably be intending to happen Similarly we could try to insert a tuple into high_school GPA and again we'll get an_error and again it just doesn't make since How would we insert a tuple with an average_GPA What students would we insert Some fabricated students from small high_schools Just again doesn't make a lot of sense so the system disallows those updates So the previous example wasn't updatable primarily due to the aggregation Here's another example that also is not updatable Here we're taking just the majors from the apply relation generating a list of them So let's take a look_at what the view would contain and we see it's again a list of majors We have a student here with a null major Now would it make_sense to insert or delete from this view Well inserting certainly not We'd have to fabricate the student who's applying to that major Deleting could make more sense if we wanted to delete all the apply tuples for a particular major but probably that's not what the user intended So if we try to for example add a chemistry major we would get an_error And if we decided for example to delete Computer_Science major we'd again get an_error So this view is considered non updatable by the underlying Structured_Query_Language system and by the this is my Structured_Query_Language that we're running but also by the Structured_Query_Language standard So one of the other conditions we saw for a view to be update able is that it doesn't have a use of the outer relation also in a sub query And so I've created a view here that violates that constraint This says let's find students where some other student has the same GPA and the same high_school_size and we'll call that view Non Unique So we've_got the student in the outer query the student in the inner query we'll go_ahead and create the view We can take a look_at the view and here we'll see that we have three students where some other student has the same GPA and high_school_size and they're all the same GPA and high_school_size it turns_out in our small database and let's think_about it Would it make_sense to be able to modify this view Well if we wanted to for example delete the two Amy tuples the underlying system could delete the Amys that would actually have the effect of deleting Doris if you think_about it or they could delete the first Amy and Doris and that would delete the other Amy So there's quite a few underlying modifications that could perform the modification we're trying to perform by the way here's that modification So if we try to run the delete command and it takes the Amy's out of the view Again it's not allowed that's because there's_no obvious translation There's many possible translations in the underlying base_tables and again the Structured_Query_Language standard disallows subqueries referencing the same table as the outer query because of the ambiguity of modifications on this type of view Now that's not to say that subqueries aren't allowed at all in views that can be modified and here's an example where we have a subquery and the view is allowed to be modified This is a view of all the information_about students when the student has applied somewhere for a major that's like the major that's a biology major so we're using a SQL like predicate here to match the major So let's go_ahead and create the view We'll take a look_at it and we'll see here that we have three students_who have applied to some biology type major Let's say that we wanted to delete the first tuple from the view the tuple with the student name Bob So we wanted to run this command here If we take a look_at the view definition we can see that what would make_sense here because we are selecting students is to actually delete the student tuple for Bob and that is what the system will do There would be other possibilities like deleting the apply tuple so there is ambiguity here but the decision in the MySQL system and in the Structured_Query_Language standard is that if you have an outer query that's referencing a single table which is a requirement in the Structured_Query_Language standard then a deletion from the view is going to result in a deletion from that one table So we'll go_ahead and we'll run the delete We see that it happened We'll take a look_at BIO we'll see that the first tuple is gone but what's most_important is to take a look_at the student table and we'll see that Bob is gone from the student table Now it is the fact that Bob's apply tupple actually is still present Bob was ID applying to Berkeley in biology If we have set_up our database properly we'd probably have a referential and integrity constraint from apply to student And then the deletion to student would have either generated an_error or generated a cascaded delete But we didn't set_up those referential_integrity_constraints in this demo so only the student tuple for Bob was deleted as a result of the view modification command Now coming back to our bio view What if we decided we actually wanted to perfrom an insertion into the view Let's take a look_at what we have and let's_suppose we want to insert another student who is applying to biology but remember the view is defined over the here we go I'm_sorry over the student table So that will result in an insertion into the student table just like the deletion resulted in a deletion from the student table So let's say we want to insert a new student Karen Here's her information And we want to insert her into the bio view So let's go_ahead and run the command And it seems to have been successful But let's take a look_at the view and there is no sign of Karen Well why is there no sign Because all it did was insert a tuple into the student table Here we go that's the basic translation and there's_no tuples for Karen in the apply table We certainly didn't fabricate some application to some major that matches biology so this is again an example where we can have the underlying modification not produce a tuple in the view But it will effect the actual database So again we can use that check_option to make_sure that doesn't happen We'll create a new view called Bio Two and this one has the check_option Let's go_ahead And now let's see what_happens when we try to insert_into bio so we'll insert another tuple and this time we get a failure because this can't translate automatically into an insertion that would actually have the effect that we want on the view So we saw that we could delete from the view with no problem but we can't insert_into the view So you_might_wonder why don't we always include the with check_option when we create views And certainly I would say that's a pretty good idea but it will have an effect on efficiency So if one can guarantee that all of the updates that are made to the view are going to have the desired effect when they are translated then the check_option can be left out but if there's any question it's a good idea to include it So now let's take a look_at a view that involves a join This view gathers information_about students_who have applied to Stanford We can also see that in the view definition we're giving names to attributes in the view If we don't specify these names explicitly as in all of our previous examples it just takes the names from the schema of the select statement defining the view Now that actually wouldn't be allowed in this case because we have two attributes called SID So what we're going to do is we're going to call the student_ID that comes from the student relation SID The one that comes from the apply relation we'll call AID Those are always going to be equal as we'll see but they are going to have some interesting effects when we modify the view There will be some meaning to having two different attributes also have the student name that comes from the student table and the major that comes from the apply table Let's go_ahead and create the view and we'll take a look_at it's contents We can see that we have a set of students_who have applied to Stanford Now lets talk_about burning modification commands_against this view Lets say that we wanted to change our two students_who have applied to major in Computer_Science They have their names not be Helen and Irene but be Computer_Science Major instead Of_course we wouldn't want to do that but it's good for illustrative purposes So here's the command and again we're updating through the view so we're saying update the STAN view to set the student name to be Computer_Science major if they've_applied to major in Computer_Science We'll go_ahead and run that And let's take a look_at our view Now we see that Helen and Irene are no_longer a Helen and Irene but they're rather both called Computer_Science Major Now we'll take a look_at what the update translated to So we'll go take a look_at our students and we'll see that what who are Helen and Irene are now changed to Computer_Science major So how did the system know to translate that modification to the student table given that the view is defined over two tables Well we're updating the student name and the student name comes from the Student table so the system will basically map the attributes that are being modified to the underlying table where the attribute came from and then it will perform the corresponding modification to that underlying table Now that approach can introduce some problems Let's say that we decide we're going the update the AID attribute in our view If we update the AID attribute the system will translate that into a modification to the Apply relation but then we'll see that the tuples will no_longer properly join So let's see that through an example So here's our view and let's decide we'll take our first tuple and we're going to update it's AID to not be anymore but to be So here's the command that does that We go_ahead and run it and let's see what_happened In our view we refresh and we find that that tuple is gone We did not get the updated to and that's because down in our Apply table we have that modification and we can see it's the first tuple to be modified to be but in our student table that student is still so it's another one of these examples whereas the underlying tables were updated as a result of asking for an update to the view but we didn't get the effect on the view we wanted By now you've probably figured out what we're going to do is add a check_option to our view to make_sure that bad behavior can't occur So here is the same view with the check_option We'll call this view Stan and then we'll try to do a similar update to Stan that we did before we'll try to update Stan to set the AID to we go_ahead and run that and as expected the check_option fails and it doesn't allow the update Now let's go_back to our original view without the check_option and let's see if we're allowed to perform insertions_into the view So first let's remind ourselves of the contents of the view Here's what we have now And I'm going to try and insert_into this view a tuple and I'm just going to specify the student_ID and the name and let's see what_happens In Structured_Query_Language when I insert a tuple if I only want to specify some of the attributes I can list them in the insertion command give volumes for those and then we will put null for the other values So lets see if the system allows_us to insert_into STAN Well it seemed to And let's see what_happened Here's our STAN view And we refresh and it looks_like nothing happened Let's look underneath at our apply tuple and we don't see any s happening there But if we take a look_at our student tuple in fact Lance did appear So lets try something similar on our view that has the check_option So now let's try inserting Mary into the view when we have the check_option and we see that it fails in that case which we would not want presumably But what if we first inserted into apply a tuple for Mary applying to Stanford and then we try to insert Mary into the view Now everything looks good and let's go take a look_at the view first Here's Mary and she was properly inserted into the view applying to History at Stanford and And if we take a look_at student Mary has been inserted there as_well And it's no problem having the null_values here because they weren't involved in our views and those attributes are allowed to be null in the underlying tables Now let's try something similar with Nancy Before we try to insert Nancy into our view and again this is STAN with the check_option we'll insert a tuple for Nancy into the apply relation But we are going to insert an application to Massachusetts_Institute of Technology not to Stanford So hopefully we'll get the first insertion but the check_option fails because when we try to insert Nancy into the student table as a result of the view rather_than the system tries to insert Nancy Nancy will join her but not applying to Stanford only applying to Massachusetts_Institute of Technology And the last thing that we'll do is attempt to perform a deletion from the Stanford view Let's say we want to delete the student with ID So the first tuple from the view So here's our command and when we run the command we see that we get an_error We cannot delete from the join view So the MySQL system has decided not to implement deletions from joined views and in that case it's because again it's quite ambiguous how that would be translated A deletion from the STAN view could be achieved by deleting from the student table or from the applied table or from both and because of that ambiguity the system doesn't allow it When we had a view that was defined over one table even with a sub query it was sort of more clear that a deletion from the view should delete from that outer reference table That completes our demonstration of automatic view modification We saw that in the MySQL system when we create views and asked to write modification commands on the views depending on the type of view and the type of modification that modification may be translated automatically into a modification on the underlying base_tables Now if we don't include the check_option sometimes that translation will result in modifications to the base_tables that don't have the proper effect of modifying the view If we include the check_option then the system will guarantee that the modifications it makes to the base_tables do result in a modification to the view MySQL is a little_more generous in what views it allows to be modified over what the Structured_Query_Language standard specifies For_example MySQL does allow joined views with certain modifications Also in the Structured_Query_Language standard every view is either considered updatable or not If a view is updatable any of the modifications can be performed on it insertions deletions or updates In MySQL since it's a binary_digit more generous about what views can be modified it's also a little_binary_digit more fine grained So we can have views that have certain types of updates allowed for example insertions and updates while other types might not be allowed for example deletions this video covers the topic of materialized_views As a reminder the reason that we use views in database_systems is to hide data from users to make some queries easier or more natural to express And to modularize our access to the database And real applications do tend to use lots and lots and lots of views So those views are for virtual_views Virtual views are what we've been talking_about in our previous_videos I'm not actually sure I used that terminology A virtual view is the usual type of view where we define it as a query of the database We don't actually create a table for the view Queries and modifications are rewritten based_on the view definition Now there's also a notion of a materialized_view obviously for this video and materialized_views give_us the same advantages of virtual_views but one additional advantage which is perhaps the most_important one which is to improve query performance over the database So again as a quick reminder about virtual_views we define a view V say by giving a query to specify the view over some relations or even other views The schema of the view is the schema of the result of the query When we have a query queue so this is a user query that references the view V then conceptually not actually we can imagine that there is a table called V We run the view query over the current state of the relations We put the result in V And then we can evaluate the user query queue which refers to V Now in reality what_happens as we already mentioned is that the user query queue is rewritten based_on the view definition to just use the base_tables Now let's talk_about what_happens to materialized_views Again exactly the same we define a view We give it a name say V And we define it as a query over a set of table or other views then the system actually creates a physical table V with the schema of the query result Next the view query is executed over the current state of the database and the results are put physically in that table V Now queries can refer to V as if its a table table because it actually is a table stored in a database This all sounds great of course there are some down sides The first down side is that V could be very_large When we talked_about virtual_views We showed some examples where we could create a view that was just enormous much larger than could ever be stored in the database but because the view was only a logical concept it wasn't a problem When users ran queries over the view they'd typically have selection conditions so you'd never be materializing that very_large view In materialized_views obviously you're creating the view and so it is a problem if the view is extremely large So that's one of the downsides The other downside is that we need to worry if the view is stored What happens when we have modifications to those tables over which V is defined We need to actually modify the stored table V either makes changes to it based_on the changes to the base_tables or completely recompute the view Let's move now to an example And we'll use our usual sample database shown here at the bottom of the slide Let's create a materialized_view We'll give it the name CACS It's for Computer_Science applicants to California colleges so this is a three way join over all of our relations and it's going to select the college name and student name when the student has applied to the college the college is in California and the student is applying to major in Computer_Science So once this command is issued the system will actually create a table called CACS and now the good news we can CACS in any query we want as if it's a table because it is Now the down side is that the base data over which the view is defined is modified we have to worry that our view is invalid that it's become out of sync with with the base data So let's think_about what modifications could occur to the database that would cause the view to become invalid Well we have to worry_about the three relations that are referenced in the view that is the college relation the student relation and the apply relation and for the college relation well inserts could change the results of the view We could have a new college It seems unlikely we could have a new college that the student will have already applied to in California for C S certainly deletes can affect the view and then updates to any of the attributes that are mentioned in the view and for the college that would be the college name and the state For the student table again inserts to student could affect the view if we already have an applied tuple and a college tuple that it matches Deletes would certainly affect the view and again updates and in this case the attributes that are referenced from the student table are the student name and the student_ID And finally apply again that is the most likely one that would have modification that would affect the view inserts deletes and again updates and here the set of attributes that are relevant are the the college name the student id and the major Now if there is certain constraints on the database referential_integrity_constraints for example it might be that some of these operations couldn't affect the view For_example we might not be able to insert a college where there's already an application for that college or insert a student likewise We might not be able to delete a college if there's applications referencing it So if there are additional constraints that the system is aware of it might be able to eliminate some of these modifications But regardless many modifications will have to be monitored to make_sure that the view is modified to stay in sync with the base data By the way if this feels a little_binary_digit familiar to you when we talked_about general assertions over the database that was one of the types of constraints that we could specify We went through a similar exercise where if assertions were defined as clearly as over the database and we looked at what operations could occur what modifications to the database needed to be monitored to see if an assertion might be invalidated Really an assertion can almost be thought of as a materialized_view over the database And if you look back at that video I think you'll see there really is a correspondence between those two concepts So just to reiterate the system xx materialized_views stored must monitor all the modifications that might invalidate the view When there is a modification either the view can be completely recomputed or sometimes there's clever algorithms called incremental maintenance algorithms that can just make small modifications to the view based_on the modifications that were made to the base data So we've talked_about queries over materialized_views Very simple because the views actually stored in the database Now what about modifications on materialized_views Well there's good news and bad news The good news is that since the table is stored if a user issues a modification command and insert delete or update_command the system can just perform that command directly on the table The bad news is the base table still need to stay in sync with the view So really the exact same issues that we talked_about with virtual_views about a modification that the user wishes to execute on the view being propagated to the base_tables occur here the only difference is that we're actually modifying the view as_well as modifying the base_tables so I'm going to draw that same square diagram that we saw for virtual_views to explain again the issue with modifications on views so we have our view V and based_on our view queries view query Q That view is defined over down here our set of relations that could be based table could be other views Now the only difference with virtual_views is that based_on the view query In this case V is actually stored in the database so it's there relations are also in the database of course Now the user comes along and the user says I'd like to perform a modification command on V could be an insert delete or update And as a result we can actually run that modification since V is stored so we get some new version of V prime Now what the system has to do if it can is perform modifications down here on the base_tables and that would be producing then R prime through_RN prime And what we want is these modifications down here to be such that the view query when executed on the our primes down here would produce also v prime It's probably better to make that arrow upwards instead of downwards In any case I_hope that you get the idea that we still need to stay in sync and the translation of these modifications here as we saw in virtual_views have various issues sometimes there's_no good meaningful translation Sometimes there are many translations and it's hard to know which one is the right one So again the exact same issues arise We're not going to talk_about these issues at length in this video I do want to mention actually that more often with materialized_views then with virtual_views sometime people just say I'm not going to allow the view to be updated Materialized views are often used specifically for performance on queries and so users will be allowed to query the view but will not be allowed to modify the view Now the next topic I want to address is how a database designer picks which materialized_views to create So for virtual_views were mostly used as extra layer of abstraction based_on modular access to the database or authorization concerns but as I_mentioned a couple of times already materialized_views are also used for increased performance and that makes the process of picking which ones to create fairly interesting So if we think_about the benefits of a materialized_view from an efficiency standpoint a number of factors play into whether a materialized_view is going to give_us increased performance better efficiency One of them is just the overall size of the database one is the complexity of the view If we have the view we don't have to re execute the query So if it's a complex query it might be helpful not to be re executing it over and over Then there's the question of how_many queries are going to be issued on the database that use the view if we're going to query the view only once or twice it's probably not worth storing it and keeping it up to date The other question then is how_many or how often there are going to be modifications to the base data that affect the view because whenever we modify the base data but this affecting of the view means we have to do extra work to keep the view up to date I also alluded to this notion of incremental maintenance Incremental maintenance says that we can take modifications to the base data and propagate them into the view without fully recomputing the view Full recomputation can be a very expensive process So if we have a workload where we occasionally could use the view for queries but we're constantly updating the database and having to do full recomputation clearly it's not going to be worthwhile to create the materialized_view Overall if we think_about the trade offs we're looking_at here at a high_level it's what's_known_as a query update trade off this actually occurs in various places in database design and applications So how often are we going to query the database where we get increased performance on our queries versus how often we're gonna get to update the database where updates are gonna cost us in performance So the idea is then to analyze the workload over the database also based_on these factors like the size of the data and the complexity of the view and decide whether we are going to get more advantage by increasing the queries and that's not offset by the disadvantages of the updates By the way does this sound familiar to you at all this query update trade off decision of whether to make this extra structure that speeds up queries but slows down updates Probably if you're thinking you'll realize that indexes or when we talked_about them have exactly the same trade offs to consider When we build an index are we going to speed_up queries but we are going to slow down updates And actually materialize views in a certain way generalize the concept of indexes And in fact that brings us to our next and last topic which is the topic of automatically rewriting user queries to use materialized_views And this again is similar to indexes So when we build an index in a database for a database when we run a query we don't actually see that the query is deciding to use the index We build the index and it will speed_up the queries because the system itself will make that decision to use the index Sophisticated database_systems these days are also starting to be able to look_at what materialized_views are present in a database and automatically rewrite queries to use those views without the user being aware of that the same query answer will be given it will be given faster based_on the use of an existing materialized_view So as a simple example of that let's_suppose we have a materialized_view with the student id college name and major of students_who have applied to a college in California This is similar to but not identical to the view that we showed earlier So this is going to be a stored table always up to date And this view is available to be used by the system if it can speed_up a user query So a user may come along with this query down here and what's this query doing It's finding students This time we're looking_at the ID and the GPA of students_who have applied to a California college and they want to major in C S at that college and they have a GPA over So this query has been issued over just the base_tables but we'll see how the system might decide that if it has at its disposal this materialized_view up here it could modify the query to use the materialized_view and it will get better performance because this materialized_view has already done some of the work that would be done if we executed the query down here from scratch So here's what the system can do in the rewrite it can take this college relation out altogether That reference to college is gonna be taken care of in our view and let's change this apply here to be the California apply view instead of the apply relation itself With the college table gone we don't need that first joined condition anymore and we also don't need to check that the college is in California that's taken care of in our view The remainder of the query with apply replaced by California apply will give_us exactly the same result and presumably it will do it much faster again because some of the work in executing the query and evaluating the conditions has already been done when the view was created So you can imagine actually a very complicated and interesting problem for the database system itself It has lots of materialized_views let say stored in the database V V all the way to V you know and along comes a user query Q might be a complicated query and the system wants to determine whether any of these views could be used to help Q have better performance And sometimes that performance improvement can be really really significant Again depending on the complexity of the view the size of the database The converse problem is a problem of figuring out which views we want to design to help our queries and again that's a very interesting problem as_well Unfortunately many_times that problem is left to the human doing database design although there are some tools being developed right now as we speak to help users with that design problem So in summary materialized_views provide the same advantages as virtual_views in terms of their use for authorization for modularity of applications They have the additional feature that they improve query performance as long as the workload is appropriate and doesn't impose too much of a burden when the underlying base data is modified Designing the right virtual_views for an application is a challenging process it's also challenging for systems to use the views properly but when they do there can be really dramatic performance improvements This video covers database authorization As we'll see its a relatively straight_forward topic but it is a very_important one Authorization has to do with first making sure that users only see the data that they're supposed to see And second guarding the database from being modified by malicious users Now one thing that we're not covering in this video are system or programming security issues such as Structured_Query_Language injection errors what we are focusing on is security of the data access itself So the way database authorization work is that users of the database have specific privileges and then they can only operate_on data for which they're authorized through those privileges So it's similar to file system for example privileges and authorization except that it is specific to the database constructs database contents and tends to be more fine grained access than we see with file systems Specifically for databases that privileges that are possible are on a particular relation to select the data for that relation or maybe just select the specific attributes so that's read privileges As far as write privileges or modifications we can set_up the privilege to insert a non relation or even insert specific attributesonly of a relation Most Structured_Query_Language implementations do allow you to insert data with only specific attributes specified We can have privileges to update a relation or update specific attributes and finally the privilege to delete from a relation So let's go straight to some examples We'll be using the standard college admission sample database that we've used in other videos The schema is shown here at the bottom Let's_suppose we have a user who wants to issue the update_command shown They want to find students_whose GPA is greater_than and if those students have applied_anywhere they want to update the application record and set the decision to be yes So let's look_at what privileges would be needed by the user to execute this command So clearly we're going to have to have some privileges on the Apply relation and some privileges on the Student relation In the Apply relation they're going to need to be able to update the decision attribute but there's_no other update privileges that are needed In terms of select privileges or reading the Apply relation the only attribute that's being read here is the student_ID so that's what they need For the student relation they're going to need to read the GPA as_well as the student_ID so the privilege needed there is the select privilege over the student_ID and the GPA So with this set of privileges the user would be allowed to execute this operation In our next example suppose the user wants to delete all students_who_haven't applied_anywhere So they're deleting from the student relation where the student_ID is not in the set of student IDs in the applied relation So for this one again they'll need privileges on the student relation and on the apply relation On the student relation the user would need the delete privilege and delete never has attributes associated_with it because you are always deleting entire tuples The only tuple that's actually the only attribute that's actually being read from the student relation is the Student ID So the user would also need the select privilege on Student ID And then in the applied A relation again only the student_ID is being read So the user would need the select privilege on apply of the student_ID And with these the user will be permitted to execute this operation So far so good but now let's introduce a little twist Let's_suppose that we have a user that we want to authorize to access information in the student relation but only for students_who have applied to Stanford How can we possibly do that Why don't you give that a thought Well I'll give you the answer right away The way we do that is actually by using views So we can create a view that gives_us the student information for Stanford applicants only Then we can grant users privileges on the view rather_than directly on the relations So here's our view It says we'll find all students where their student is in the IDs of the students_who have applied to Stanford and we'll call that view SS for Stanford students Now we can give a particular user the select privilege on SS And that allows them to select the data that's in the view but not see any of the data outside of the view Specifically they won't be able to see student information if the students didn't apply to Stanford Let's take a look_at a second example that involves views Let's_suppose that we want to authorize a user to delete applications but only applications that are to Berkeley So again we'll set_up a view This one's a little simpler It's just the view of the app of the records of the apply relation where the college name is Berkeley And we'll call the view BA And then what we want to grant to the user is the ability to delete the delete privilege from the B A view Now in this case we do need to have that view be updatable by the system that's supporting it So in our video about views we discuss this issue Those are the only examples I'm going to give for now but I do want to emphasize that views are quite important for customizing authorization to specific user needs And in fact authorization is one of the most_important uses of views in database_systems Now let's look how privileges are obtained When a relation is created the creator of that relation becomes the owner of the relation And the owner of the relation has all privileges and furthermore may grant_privileges to other users So there's a grant statement in the Structured_Query_Language standard and it looks_like this We grant_privileges on a particular relation to one or more users and we have the option of allowing those users to further grant_privileges to others Specifically the privileges themselves are the ones that we defined earlier And we can have a comma separated list of them So for example we could say here something like select student_ID comma delete and that would give those two privileges The users are a list of actual user names on the data base There's also the user pre defined user called public and that would grant the authorization to any user of the database And finally the grant option allows the users who are getting the privileges to grant the same or lesser privileges to other users Now what do I_mean by lesser Well it's pretty_simple If we have say select and attributes ABC then a lesser privilege would be something like select A and B Now how about the revoking of privileges This gets a little_more interesting So the command is revoke privileges Again it would be a list of privileges on a particular relation from users and again that would be a list of user names with the possibility of the special name public and then there are two option for revoking privileges called cascade and restrict And they may have to do with what_happens when privileges are being revoked from a user who was granted the ability to grant_privileges to others So let's take a look pictorially at what can happen Let's_suppose that we have a user who has a privilege to say select on a particular relation R and we'll draw that is the root of a graph And let's_suppose that's say user U And let's_suppose that user grants to user U the same privileges select on 'R' and let's_suppose that's with the grant option and this is user 'U ' So user U is allowed to further grant_privileges to other users And those may further grant_privileges to others And we may get a big sub tree here Now let's_suppose user U decides to revoke the privilege that was granted to user U So what cast it says is if there is revocation of that form then it will cascade down the tree So if you too further granted privileges then those would be revoked and so would any privileges down below so this entire sub tree is effectively removed all of those privileges however we have to be a little_binary_digit careful because it's possible that say U was granted the select privilege by a separate user I guess we'll call this one U who also granted exactly the same privilege and in that case if U does the revoke we don't want to revoke U 's privilege because U got it from another source So technically what cascade says is that when we revoke a privilege we revoke any privileges transitively when they weren't also granted by another source So what your seeing here is actually called a grant diagram and I'm not giving you all the details of grant diagrams but you can see basically what they do And their used is to properly cascade the revoking of privileges So again the cascade option and the revoke command says to also revoke any privileges that were granted from the ones being revoked transitively unless they were also granted form another source What the restrict option says is that the revoke command is not allowed to execute if cascade would revoke any other privileges So if we have any of those transitive cases so if we do have the transitive cases and we want to use restrict and we have to manually revoke those privileges effectively bottom up through that graph that we say Incidentally restrict is the default in the revoke command So if neither of these options are specified then restrict is the one that will be enforced Lastly let_me talk a little_binary_digit about where privileges actually reside in reality So we have our data that's being managed by a database system and typically we'll have application developers who are working directly with the database system often developing modules that will be invoked by the end users So those application developers have to have privileges on the database to create the modules But then we have the software that sits above the database system that is used by end users and the end users typically don't have privileges themselves They might have separate privileges privileges to access the modules but they're not going to be using the privilege system of the database system And similiarly there may be even more software layered on top of the software that the application developer builds And again that software itself wouldn't be having database privileges but might have an authorization system for the software that it's accessing To summarize database base authorization is important It makes sure that users only see the data that they're authorized to see It guards the database against being modified by malicious users There's a privileged system similar to file system privileges but specific to database constructs and users can only operate_on the data for which they're authorized via their privileges There's a grant statement and a revoke statement in the Structured_Query_Language standard for granting privileges and revoking privileges And again when a relation is created the owner of the relation starts with all privileges so that's where the granting would begin And finally for having privileges that go beyond simple table level operations views are a very_important construct and in fact authorization is one of the most_important uses of database views In this video we'll show how recursion has been added to the Structured_Query_Language language We'll show the WITH statement which is a regular part of Structured_Query_Language And then we'll show how WITH can be used to write recursive queries We'll describe a few examples that can't be written without recursion in Structured_Query_Language and then in a follow up video we'll give a demo that will show the examples in action So Structured_Query_Language is not what is known_as a turing complete language For those of you who are familiar with the idea of turing completeness it says that pretty_much any computation can be performed by a language and that's simply not true in Structured_Query_Language So Structured_Query_Language has some nice features it's simple convenient declarative meaning we don't have to say how to execute queries just what we want out of the queries We've talked_about that many_times throughout these videos We find that Structured_Query_Language is expressive enough for most all database queries we want to do except for one type And that's the type that involves unbounded computations The basic Structured_Query_Language language does not have features that allow_us to do those And I'll motivate those with a few examples I'll just say up_front that when unbounded computations need to be performed use a database Typically there's some programming in a programming language that will be accessing the database over and over to do those computations But we're also going to see that Structured_Query_Language has added a notion of recursion that allows_us to perform unbounded computations So in each of my examples I'm going to give a relational schema and then a query we'd_like to write over that schema but we will see that we can't The first is a simple ancestors computation So for example we might have that Sue is a parent of Mary and maybe Bob is also a parent of Mary And maybe Fred is a parent of Bob and Jane is also a parent of Bob and so on So we're just listing the parent child relationships in our relation called parent of And then our goal is to use that relation to compute say all of Mary's ancestors So we can certainly write a Structured_Query_Language query that finds Mary's parents I'll let you do that as an exercise It's very straight_forward We can even write a query to find Mary's grandparents A query to do that and again I'm not gonna write it here would involve two_instances of parent of so you'd be joining parent of with itself We wrote queries of that form when we were working with basic Structured_Query_Language in our videos And you know we could even find the great grandparents by using say three instances in joining them The problem is that we might not know in advance how_many steps there are to get all of Mary's ancestors And each one of those steps does involve an instance of parent of and adjoin so this query can not be executed using standard Structured_Query_Language Here's our second example A little_more complicated It involves three relations And they represent a company hierarchy We're going to have manager employee relationships We're going to have projects and we're going to have salaries So our first relation employee just gives_us the salary of each employee Our second one gives_us the manager relationship and what this is saying is say that the employee with ID is a manager I'll draw it like a tree here of two three four who himself might manage a few other employees and so on So we have a hierarchical structure of the management in the company And you'll see you can already probably recognize that this is similar to the parent relationship that we had in our previous example And finally we have a set of projects that gives the name of the project and the manager of that project And the query that we would like to run over this database is to find the total_salary cost of a given project Let's say project_X So for example if happens to be the manager of project_X then the total_salary class would be 's salary plus 's and so on down the hierarchy So I'm not even going to try to write a Structured_Query_Language query here Again you can do that as an exercise if we knew precisely how deep the tree was below the manager of project_X Then we could again use these self joins that would be on the manager relation this time to find all the employees that are in that sub tree of project_X and add up their salaries But if we don't know the depth of that hierarchy then it's impossible for us to write a Structured_Query_Language query that goes to arbitrary depth to add up the costs By the way let_me_mention that one of the reasons I'm not bothering to write the exact Structured_Query_Language right now as I motivate these examples is that we are going to see the SQLs shortly when we do the live demo And the third example and my personal favorite is finding airline flights from a starting point to an ending point at the cheapest cost So let's say we have a relation here that lists all flights the start of the flight the end of the flight the airline and the cost of the flight and we're interested in flying from point A to point B Now if I happen to be a very conservative traveler and I don't want to change planes more_than twice then I'm in good shape and I could still write this using Structured_Query_Language because I'd be only willing to take flights and I could just join three instances of the flight relation matching the destination of a flight with the origin of the next one and then adding up the costs and finding the cheapest one It's not a trivial Structured_Query_Language query to write Again I'll leave you that as an exercise And by the way I often give that very query as an exercise in my class But when we don't know the number of flights we want to take so if we are a very frugal traveler whose willing to spend arbitrary amounts of time to get the cheapest flight then we can't with regular Structured_Query_Language write a query that explores arbitrary numbers of flights to finds the cheapest_way to get from point A to point B So as you've probably surmised all three of these examples are going to be expressible once we have recursion in the Structured_Query_Language language So next I'll introduce the with construct in Structured_Query_Language The with construct is actually present even without recursion but it is the construct that was used to add recursion to the language So here is the width statement in Structured_Query_Language We give the keyword with and then we list one or more new relation names So R RN would be relations that don't exist in the database and each one of those is tied to a query So what we're effectively saying is that R is going to contain the results of query one or two of the results of query two and so on Once we've set that up then the final part of the with is a final query that can involve any tables in a database and can also involve these new tables R through_RN In some ways you can think of the with statement as setting up temporary views So it sets up a view for each one of these R's then runs a query involving the views and then the views go away Or if you want to think of them as materialized_views you could think of these as assignment statements where we have the as So we create the table R we put the data in and then we run the query In reality most systems implement the with statement like for generalized virtual_views and they'll rewrite the query down here to be expressed over the tables that are used in these queries We'll be seeing examples of the with statement when we get to our demo it will of course be the recursive with statement Just one more notational point just like as with views what normally happens is the schema of one of these Rs is inherited as the schema that's the result of the query that's associated_with the R But if the user I_mean the designer the one writing the with statement wishes to have a different schema different names for the attributes those can be specified explicitly and then those would be used in this query when R is referenced So now here's the fun part We can specify recursive as a key word after with and that let's us write recursive queries in this first portion of the with statement Very_specifically in query one here we an actually reference relation R which is the relation we're defining with Query in a recursive fashion In Query we can reference R We can actually also have a mutual_recursion which I'm going to focus_on in a separate video But that would be saying not this exactly That would be saying Query could reference R and Query would reference R but again we'll be talking_about that in a separate video For now we'll just talk_about recursion within each specification of one of these Rs By the way one thing that I wanted to mention is a sort of syntactic inconsistency The Structured_Query_Language standard actually says that this recursive modifier here goes with the relation specification So if R is recursive we would say recursive R if RN was recursive we'd say recursive RN The implementation that we're going to be using which is the postgres implementation actually says that recursive modifies the with So when you say with recursive then any of the RI's are allowed to be recursive Now let_me show what the typical form is of one of these recursive specifications in the with statement So this example I'm giving has just one R specified in the with statement and then the query at the bottom Again this is the final result of the recursive with statement involves R and possibly other tables of the database The typical way to define a recursive query is to have a base query and that would be over non over tables other than R so not R in this base query Sort of to get the recursion started and then R will be the result of that base query together with the recursive query So here we will reference R And the idea is that the result of R the R that's seen when we run the query down here is what's_known_as the fixed point of running this union over and over again until we add no additional tuples to R One other thing that I wanted to mention is that this form of the recursion this idea here that we have the base query and a union with the recursive query is not enforced in the Structured_Query_Language standard The Structured_Query_Language standard merely says that we can specify R here and then any query inside the parenthesis that reference R Although there are a number of restrictions this division into a base query and recursive query isn't required We'll be talking_about some of those restrictions late on actually in a separate video In the implementation that we're using the implementation it actually does require this form where you have a base query union and recursive query but that's not really a problem because all natural recursive queries at_least the ones I've seen and for the examples we're going to look_at do take this form And one last thing I want to mention is about the UNION operator First a reminder that in Structured_Query_Language when we say UNION as opposed to UNION ALL we're talking_about duplicate eliminating union that means that when we add a tuple to our union that's already there we don't actually add a tuple And that's really key to this notion of reaching a fixed point or with the recursion terminating because we might be running the recursive query that over and over gives_us additional tuples but if those are all tuples that we already have in R then we won't actually be adding anything new because the union eliminates duplicates and that will tell_us our recursion is done As you can imagine if I wrote UNION ALL instead then I'm continuously adding new tuples and my recursion will typically not terminate so it's not common maybe never to see UNION ALL used in recursion but rather the duplicate eliminating UNION operator So now that we've_seen a number of examples that need recursion and we've_seen how recursion has been introduced into Structured_Query_Language the next_video will give a demo of these queries in action This video gives a live demonstration of the recursive constructs in Structured_Query_Language that we introduced in the previous_video As a reminder recursion has been introduced intosequal as part of the with statement where we can set_up relations that are defined by queries that themselves refer to the relation being defined and finally we have a query that can involve the recursively_defined relations as_well as other relations or other tables in the database The typical expression within a with statement for a recursively_defined_relation would be to have a base query that doesn't depend on R and then a recursive query that does depend on R We gave three examples in the introductory_video and those are the same examples that we'll be demonstrated shortly The first one was to compute ancestors when we have only a parent relation and the family tree could be arbitrarily deep Our second example was a case where we have an arbitrarily deep company hierarchy and we want to compute the total_salary cost of a project starting at that project's manager and summing the salary of the entire sub tree and our third example was about airplane flights Where we want to find the cheapest_way to fly from point A to point B And we're willing to change planes as many kinds as we might need to in order to bring down the cost We saw that all of these examples involved basically a notion of transit of closure computed as a recursively_defined_relation The last portion of our demo after we see these three queries solved using recursion will introduce one more twist which is what_happens when we introduce cycles So in the Airline example We'll set_up a case where you can fly from one city to another one and back Which is of course true in reality and we'll see what_happens when we try to answer our query in that setting I've started by creating a table called Parent of With parent child relationships So we have Alice Carol Bob Carol Carol Dave and so on You_might actually want to write this down on a piece of paper to see what the actual tree looks_like but the query we want to run is to find all of Mary's ancestors so we're going to of course have Eve as a parent and Dave as a parent And then Dave's parent is Carol and Carol's parent is Bob and so on We'll get most of the data in our database in our query So here is the query our first example of our recursive query Let_me say right off that even more_than anything else we've done in these videos I_am going to encourage_you to download the script and take a close look_at the query and preferable actually run the queries and play with them on the Postgres system And we are using for this demo Postgres as SQLite and MySQL do not support forth the With Recursive statement at this point in time So anyway here's our query and it is the form that we described in the introduction it's a width statement with recursive that's going to set_up a recursive relation called ancestor so this is what we were calling r earlier This is our ancestor with a schema Administrative Domain for ancestor and descendant Our final query once ancestor is all set_up is very_simple it just says take the 'A' attribute from ancestor where a descendant is Mary so that will give_us Mary's defendant Of_course what's interesting is what's right here inside these parans because this our recursive query And it does take the form we described of having a base query that is the first line and then the recursive query with a union between them So we're going to start_by saying that whenever we have a parent child relationship that's also an ancestor relationship So we're going to take from our parent of table The parent and child and we have to rename them as A and D and that says that parent children are an ancestor What else in an ancestor Well if we have a tuple an ancestor an ancestor and a descendant and that descendant is the parent of a another person then the A and the ancestor together with the child from the parent of is also an ancestor relationship So this is a kind of doing the join Not just kind of it's actually joining our ancestor as its being created and extending that relationship by joining with another instance of parent So you can kind of think of going down the ancestor tree adding relationships as we go down Again I really can't encourage_you enough to download this query and play with it yourself to fully understand what's going on Let's go_ahead and run it It's going to be rather anticlimatic When we run it we do discover that these five people are Mary's ancestors and if you've drawn the little tree of the data you can verify that that's the correct answer Let's play around a little_binary_digit Let_me try a few other people's ancestors Let's try Frank We don't see Frank here because Frank actually happens to be a child of Mary's so we should get even more ancestors when we run this one especially Mary should be included and in fact she is there she is and these are Frank's ancestors Let's try George I think George was somewhere in the middle of the tree there Yes George has three ancestors and finally let's try Bob Bob is at the root so we should get an empty result and we do because again Bob has no ancestors in our database Now lets take a look_at our second example That was the one where we had a hierarchy of management chain in a company and then we were interested in computing the total_salary cost of a project so I've set_up our three tables The first one is the Employee table it just gives the IDs of the employees and the salary of each employee The second table is the Manager relationship So again you might want to draw the little tree here although it's pretty_simple this time is at the root of our little management structure as as a subordinate has two subordinates and and is another one So it's only a three level tree Of_course if we knew it was only three levels we wouldn't need recursion at all But we're going to write a query that will work for arbitrary numbers of levels So that's our management structure And finally our third table the Project table says that employee is the manager of project_X so what we want to do is find the manager of project Y in the hierarchy and then take that manager's salary along with the salary's of all the manager's subordinates recursively down to the management structure and of course that's everybody in in our little database Again I can't encourage_you to download the script and run it for yourself So here's our query to find the total_salary of project x and I'm actually going to give a couple of different_ways of running this for you The way we've done it the first time is to effectively expand the management structure into a relation called superior So that's really pretty_much doing the ancestor computation which by the way is a transitive closure I should have mentioned that earlier for those of you familiar with transitive closures It's basically that operation So we're going to compute these superiors so that we'll have every manager and employee relationship with a manager is arbitrarily above the employee And then once we have that superior relationship computed then we write a actually a fairly_complicated query so this is the final query of our with statement And this one says we've_got this recursive relation superior We're going to the salaries from the employee relation where the ID is either the manager of the project_X so that's the first half here or an employee that's managed by the manager of project_X Okay Now this down here I just want to emphasize this is not recursive it just so happens to have that same structure of union but there is nothing recursive happening down here So this is just a regular Structured_Query_Language query Once we have the superior relation that's the transitive closure of the manager relation So let's take a look_at superior Superior here this is recursive with the union says that if we have a manager and that's the MID and EID Then if somebody is managing someone else then they are their superior Notice by the way I didn't specify a schema here so the schema is implicitly going to be M I D E I D So we're going to put manager relationships in And then if we have a superior relationship so if we have an MID managing an EID in the S relationship then we can add one more level because we join with the managers saying that if S is a superior of Y and why is the manager of Z been X's superior of Z This parallels exactly what we did with the ancestor computation in the previous example Again it's going to be rather anti climactic to run the query but let's do it And we find out that four hundred is the total_salary cost of Project X when we count the manager of project_X together with all of the people underneath that manager in the hierarchical structure So when we think of recursion we often think of transitive closure or expanding hierarchies as we've done with our examples so far But if we step back for a second we can see that there is a quite a binary_digit simpler way to express the query that finds the salary burden of project_X Now not only is this actually nicer to look_at it's probably much more efficient depending on how smart the query processor is In our previous example if the query processor executes the query in a straight_forward way it would compute this superior relationship for the absolute entire company hierarchy before it figured out which of those people were involved in project_X Now a really good query processor might actually figure_out to fold in a project_X but not necessarily Here's an example and here's a new formulation of the query We're actually going to tie X specifically to our recursion What we're going to compute in our recursive With statement here So this is the temporary relation we're computing is a relation containing just a list of the IDs of the employees who are involved in project_X Once we have all the employees involved in project_X the query down here is trivial We just find those employees who are among the X employees and we sum up their salaries So let's take a look_at the recursive definition here and again it's taking the usual form of a base query union and recursive query and here's what we do Well obviously the manager of project_X is one of the IDs involved in project_X So here we find in the project the project name text and we take the manager of that project and we put that person's ID into Xemps That's the first ID that's going to go in there That's going to seed the recursion That's again the base query Then we add in our recursive step any employee who is managed by anybody who's in the X employees So we'll take our manager relationship our X employees relationship and if the employee's manager is an X then that employee is also involved in X So we seed the recursion with the manager of project_X and then we just recursively go down the tree adding all of the employees that are underneath one by one We don't have to know the depth of the tree because the recursion will continue until nobody else is added I guess I should have mentioned that earlier in my earlier examples Again the recursion sort of adds a data over and over again until there's nothing new to add and that's when it terminates So let's go_ahead and run the query Anti climatic again but we get the same answer as the salary cost of Project X Now we use the same form of query to find the total_salary cost of two projects Y and Z And that will also demonstrate having two relations that are defined in the width recursive command So I have added project Y and Z to our project table and they're both managed by employees who are already in the database so they're a little lower down the hierarchy We should expect those projects have lower total_cost That's for project_X whose manager was at the root of our hierarchy So here's our query it's a big one we're going to define YM and ZM exactly as we defined X amps in the previous example So Y amps is a table of a recursively_defined_relation temporary that's gonna contain a list of IDs of the people the employees that are involved in Project Y So we are going to put the manager of Project Y as our base query and then we're going to add to it in the recursion all of the employees who are managed by someone who's in the YMs And ZM's exactly the same We start the manager of project Z and then we add to it all of the people are managed transitively down the tree by someone who's in the ZM's relation And then our final query down here for the statement is a union of two queries The first one gets the total_salary for Y and it labels it as Y total So it takes all the Ids that are in the Y table and from the employee table get their salaries and sums them up And similarly the Z total So now we'll run this query it will be slightly less is anti climactic We do have now two tuples in our result We see that the total_salary for Y is in the total salaries for Z is And if you check cross check this result against the data you'll see that these are indeed the total salaries when we take the managers we specified for projects Y and Z And finally our last and most fun example The one to find how to fly from point A to point be when all we're concerned about is cost and we don't care how_many times we have to change planes So here's the little flights table I've set_up and I used A and B so we can literally fly from point A to point B All of our intermediate destinations are actually real airport codes and I've put in some airlines although they're not actually going to be used in our query And then I've put in the cost of the flights You_might want to draw yourself a little graph so you can see what's going on and we can fly from A to Chicago for from Chicago to B for another or we can go from A to Phoenix and then Phoenix to Las_Vegas to Las_Vegas to oh oh I don't remember what this is CMH Detroit Cincinnati somewhere in the midwest And from there to point B or we can take a non stop from A to B on good old Jet Blue for So clearly we're never going to be going through Chicago for a total of with that Jet Blue flight but I've set_up the data as you're probably not surprised so that this long route through Phoenix and Las_Vegas and somewhere in the midwest is in fact gonna be our cheapest_way to go So now let's take a look_at the recursive query that's going to find us our root from point A to point B or at_least find us the cheapest_way to get from point A to point B So the first query I'm going to show actually gives_us all the different costs of getting from A to B just so we can see those enumerated for us and then we'll modify the query to give_us the cheapest cost So here's the recursive query and we're going to use a recursively_defined_relation called root And root says that we can get from an origin to a destination for a particular total_cost OK So we again in our recursion have the base query and the recursive query This is exactly what you'd imagine We can certainly get from point X to point Y for a total_cost if we can take a direct_flight from point X to point Y for a given cost So that's our base query we start out with all of the direct flights in our route relation And then we start adding routes by doing the JOIN of a route with a additional flight So basically what this join here says If I can get from the origin in a route to the destination and then that destination is the origin of another flight then I can add that flight I can start with my original origin final destinationand the cost of that is going to be the total that I already had Plus the cost of the new flight and that's my new total So again this is another transitive closer like recursion It's very similar to the ancestor recursion Very similar to expanding the company hierarchy The only real difference here is that we're also accumulating these costs as we do the recursion So once we've done this recursion then we have a complete specification of all the roots within our flights database all of the way's we can get from A to B and the total_cost Now one thing I should say is this is not actually giving_us the ways of getting from one place to another If we wanted to accumulate the actual route that we take so the flights and the costs and the airlines and so on we have to kind of use a structured structure inside our data base to accumulate those There are ways of doing that but I'm not going to demonstrate that here I'm just going to demonstrate the is accused of recursion and computing the total_cost OK so let's go_ahead and run this query so we've computed all of the routes and then we're just gonna start_by finding the routes from A to B and what the total_cost of those are So we'll run the query and we'll find out that there are three ways of getting from A to B The first one happens to be that direct Jet Blue flight for The second was the flight through Chicago for a total_cost of You can go_back and look_at the data and verify these And the third one was that complicated routing where we stopped several times but we save a lot of money well twenty dollars over the direct_flight by going through those cities because the total sub cost is I'll leave it up to you whether it's worth twenty dollars to stop several times versus the direct_flight So now since my actual specification of what I wanted to know was the cheapest_way to go then I just say min total instead of in my final query and I run that and my answer is that is the cheapest_way to get from A to B Now here is an alternative formulation of the same query That essentially parallels the alternative that we looked at with our project cross where we built in project_X into our recursion that simplified the recursion in that case In this case it's not simpler but it could potentially be more efficient We're going to build in the fact that we're starting from origin A so instead of finding all roots from any point to any other point in our recursion and then finding the roots from A to B let's create a relation recursively that says Starting from point A I can get to a particular destination for a particular total_cost So this is going to build up roots starting from A the base query is going to of course start_by looking_at direct flights where the origin is A and is going to put the destination and the cost into our relation called from A So that starts with that first gives_us direct start from A where we can get on the direct where we can get to and how much it will cost us and then our recursion is going to add flights to that one Again it really parallels what we did with the Project X Our recursion is going to say ok we know we can get a from particular we can get to a particular place from point A for a certain cost and that's our destination If we add more flight so the origin of that flight is the destination of where we can get then that will also be a destination that we can get to from point A and we'll just add the cost of the additional flight on One more time a strong suggestion that you download and try these things for yourself Once we found all the places we can get from A then we'll use that to figure_out the cheapest_way to get to Point B But let's just start_by running the With statement where all we do is see the places we can get to from A and the total_cost of getting there So here we go And we can get to Chicago Phoenix or we can get to B a couple of different_ways three different_ways actually as we already know We can also get to Las_Vegas and this mysterious CMH I wish I remembered what it were So now if we're interested in finding the cheapest_way to get from A to B then We'll add where the destination equals B on here and we'll add the minimum total_cost and hopefully that will be our good old and indeed it is By the way we can do the same basic_idea but backwards Instead of finding all the places that we can get from city A how about if we find all the places from which we can get to city B So here's the query that does that To B is going to be our recursively define relation that's going to give_us the origin the place from which we can get to B and the total_cost of getting to B from that place So again the structure is exactly parallel We start out with our base query saying if we have a direct_flight to B then we can get from the origin of that direct_flight at the cost of the flight to B and we then recursively add flights on that you can think of if your going from left to right adding flights from the left So if we know we can get from a place to B and then we can go from take a direct_flight from somewhere else to that place And we can get from that somewhere else to be Anyway so we do that again by joining so we're going to take our origin from which we can get to B we're going to find flight that take us to that origin we're going to add the cost of that flight and that gives_us a new way to get to B And then let_me start_by just writing the query that all of the places from which we can get to B and the cost of getting there We'll run the query And we can see that we can get to B from point A in different_ways and from our other cities in our database as_well Similarly to what we did previously if we're particularly interested in getting from A to B whoops let's make that our origin then we add where origin equals a and if we want the minimum it would be our minimum total again paralleling exactly what we did before We run it and good old comes out Now we're going to have some real fun because I added another flight to our database and this flight takes us from Columbus I now know its Columbus to Phoenix and creates a loop in our flights So that means that we can fly from A to B next to Las_Vegas to Columbus back to Phoenix and then to Las_Vegas and Columbus again So we're going to have arbitrarily actually unbounded actually infinite length routes that we can take now Now obviously those routes aren't going to ever be the cheapest_way because as we take those roots it's going to get more and more expensive none of them are negative costs paying us to take flights But if we just do our naive recursion where we generate all of our roots before we take a look_at our final query then we're going to be generating an infinite number of roots So here's our original enquiry the first one we wrote where we were just finding all of the costs of getting from A to B by computing all of the roots in the entire database and then looking_at those from A to B Now with our additional flight that creates a loop we run this command and nothing happens Actually if we wait long enough we're going to get an_error Well okay we waited for a while appears that the user interface we're using isn't going to show us the error But if you try running this in post risk command line interface I assure you if you wait long enough eventually it will tell you that the recursion effectively overflowed So it is trying to compute this unbounded number of routes in the recursive part of the with statement and never even gets to the query that we want to execute OK here's my first attempt at fixing the problem We know that we're never going to want to take a arbitrarily long route We're never going to want to go around a cycle lots of times as our cheapest_way to get from point A to point B so what I've done here and I'm not going to go into this in great detail but I have added a condition in the recursion that says I'm only going to add a new route a new route into my recursively_defined route table when the total_cost of that route and that's defined as the cost plus total here when we added is less then all of the ways we can already get from that place to that origin to that destination So in other_words I'm only going to add cheaper routes than the ones that are already there And by the way if there are no routes already from the origin to the destination then this will be satisfied and we will add that first route then after that only adding cheaper ones So let's try running this query and see what_happens Well we got an_error Now this is not a runtime execution error This is actually an_error that says we're not allowed to refer to our recursively_defined_relation in a sub query within our recursion The Structured_Query_Language standard actually might allow this particular use but I don't know that any implementation actually handles it It can be fairly difficult to handle a case where you have the recursively_defined_relation in a subquery as_well as in the outer query here So that's obviously not going to solve our problem Now there's actually a feature of Basic Structured_Query_Language that can help us here with our problem There's something called Limit We actually didn't discuss this in the Structured_Query_Language videos but that says just give_us this number of results So let's say that we're going to have our recursion here for the roots but down here we're going to say I only need up to results for how I get from point A to point B And the posary system actually makes use of the limit command in the final query to restrict the recursion It's a nice feature and it was added specifically for this problem of possibly infinite recursions where we actually don't want it to be infinite because we only need a finite number of answers Okay so let's go with that here and we'll see that Ah great Everything worked well We got our roots from A to B And I do have roots I_mean so they're getting very expensive Down here I'm going to go around and around the mid west while lots and lots of times but that did the limit the recursion it did stop unlike our query where we didn't have the limit and it just went on indefinitely So that looks pretty good with one unfortunate problem which is if we still want the minimum we're going to again get a infinite execution So the old result is still sitting here but now the system is chunking on because the limit here is applied to this min to the number of tupimit the recursion we're always going to get only one tuple in our result So even if we said limit one here we'd still get the infinite behavior so we haven't quite solved our problem Okay so here's what we're going to do Aesthetically maybe it's not the absolutely best solution but I'm going to argue that it's a pretty reasonable one We tried limiting our recursion to only add new routes that were cheaper than existing routes to and from the same place We weren't allowed to do that syntactically the recursive with statement didn't allow the sub query with the recursively_defined_relation in it So we're going to do a different change here where we're not going to add new roots to our flight when the length of the root in other_words the number of flights contributing to that root is greater_than or equal to ten So how do we do that We're going to add to our recursively_defined_relation route the origin destination and total_cost of that And then we are going to add the length And so that's going to put in each root tupple how_many flights were involved in the root So let's see how we do that with our recursion We still have the base case here and the recursively_defined union In our base case we're going to be adding to our route the non stop flights so we'll have exactly what thought we had before and then we'll have the constant one to say that this non stop flight is just one flight Then when we do our recursion we're joining our route relation that we're building up by extending it with an additional flight exactly as before but there is two changes here One of them is that we're going to compute our new length by adding one to the existing length of the root for our new root because we're adding one flight And then we're only going to add tupples to the root relation when the length of the route that we're adding is less_than ten So now let's see what_happens I'm going to start again by looking_at all of the costs of getting from point A to point B and then we'll take to look_at finding the least So we'll go_ahead and execute the query and we see that we have one two three four five ways of getting from A to B where the length of the number of flights involved is less_than or equal to We see our friends here This was the nonstop flight This was the one through Boston Here's our favorite one and there's a few more so these are going to go through that cycle a couple of times But once we get to the length of ten we're not going to add any more so we've_got termination and if we want to change it to find the minimum cost flight then it's just the min total as before and we'll find good old Now what 's unaesthetic about this is that we're actually limiting the amount of recursion So the whole point of writing recursive queries is when we don't know the minimum number of computations that we need to do to get our answer So maybe it would so happen to turn out that more_than ten flights were required to get the cheapest and if that was the case then we wouldn't get our right answer Of_course we could change it to a hundred and we'd still get the one seventy five And you know honestly we could change it to and you can't see it happening here but it is actually recomputing that even when I put in I can even do a and still going to work for me So if we presume that nobody wants to take more_than a flights in order to get from Point A to Point B in the cheapest fashion then this would be a reasonable way to bound the recursion and get the answer that we want Even in the presence of cycles in our relation In this video we'll be exploring some further issues involved in recursion in the Structured_Query_Language language First a reminder of how Structured_Query_Language implements recursion There's a with statement in Structured_Query_Language that can be specified to have recursively_defined relations in it We say with recursive and then we define a set of relations where the query to find relation could involve the relation itself so that's where recursion pops in And then at the end the final result is a query that might involve those recursively_defined relations as_well as other tables in the database As we saw in the previous_video and demo it's very common for recursively_defined relations in the with statement to take the structure of having a base query that doesn't involve R the recursively define relation unioned with the recursive query and we saw many examples of that form The first thing I want to talk_about in this video is what's_called linear_recursion Linear recursion specifies that in the recursive definition of R and again let's assume it takes this form of the base query union and the recursive query In the recursive query There is only one reference to the recursively_defined_relation R So let's take a look_at an example to understand linear_recursion and nonlinear recursion the first example we used when we introduced recursion was finding ancestor relationships from a base table that just has parent child relationships so a basic transit of closure operation and the query we wanted to run was to find all of Mary's ancestors And here's the query that we wrote It does take the form of having a base query here Which says if we have a parent relationship that's also an ancestor relationship And then the recursion occurs in the second part of the union where we join the recursively_defined ancestor relationship ancestor relation with parents so that we extend the ancestors with one more generation Now this query does have linear_recursion because we only have one instance here of the recursively_defined_relation ancestor So let's take a look_at what_happens underneath when this query is executed We start with our parent table And here it is with a parent and child and let's_suppose we have say Sue and John and John and Mary for example in our parent table Then in what's effectively the first iteration the base query here is run that copies the parent table to the ancestor table So now we have Sue and John and John and Mary and anything else that we had in the parent table in the ancestor table As the iteration continues we're effectively joining the parent table and the ancestor table to get additional tuples in the ancestor table For_example we see that Sue and John the Sue and John tuple here would join with the John and Mary tuple and that would give_us Sue and Mary in the ancestor table The iteration continues until there are no new tuples to add to the ancestor table And then we're done with our recursively_defined_relation and we can go_ahead and execute the final query in the with statement And again often when I say we I really mean we the system All of this is of course being performed by the system as it executes the recursively_defined with statement Now let's take a look_at a non_linear expression of the same query And here it is What we see here is that the primary change is right in here Instead of joining the parent with the ancestor in the recursive half we're going to join two_instances of the ancestor relation And let's see what_happens during execution when this is how we express our recursion So we again start_by copying the contents of the parent table into the ancestor table as part of the base query And I've already shown that here But now instead of during iteration joining the parent table with the ancestor table We're actually going to join the ancestor table with itself to generate new tuples For_example we will join the first two tuples in ancestor with each other Sue John and John Mary in order to obtain what was the same tuple we obtained with the linear_recursion which would be the tuple with Sue and Mary Just a quick reminder I intended to say this earlier but it's the fact that we have these two references to ancestor in the recursion here that makes it non_linear OK so what's the deal with these two queries Why might we prefer one form of the query over the other And take my word for it by the way we do get equivalent results to the query in its linear and non_linear versions Well here's some pros and cons to non_linear versus linear For this particular query and actually in general when we can express a query both ways First of all there's some pluses to the non_linear so the query looks cleaner If you go_back and look_at the queries the non_linear version is sort of more symmetric a little shorter even to express than the linear version Second of all the nonlinear version actually converges faster to the fixed point to the final state than the linear version And I'm going to show that a little_binary_digit abstractly because it is actually fairly important So I'm going to create this abstract example parent child relation which is going to be completely linear just for illustrative purposes So we have this person here who's the parent of the person here who's the parent of a person here and so on We're going to make it eight levels deep So this is an abstraction of our a parent table and now let's see how ancestors are computed So in the first step we'll add one ancestor tuple for each tuple in the parent relation so the purple are the tuples that are added to ancestor Then in the second iteration we're going to join those with themselves I'm_sorry we're going to join the ancestor tuples with parent tuples so each ancestor tuple could be extended by one So that's going to give_us all pairs of tuples I'm_sorry it's already getting a binary_digit crowded here but I think that you will get the idea On the next iteration we're going to again take our ancestor tuple and extend them by one by joining them with parent So after the second we'll have all triples here so all great grandparent relationships OK and that's a big mess But you can really see what's going on Each time we iterate we get one more generation added into the ancestors And now let's think_about what_happens when we use the non_linear version Where after the first step we join ancestor with itself instead of ancestor with parent So as before on the first step and now I'm going to make these red the ancestor relation will contain exactly the same as the parents And the second step is the same as_well we're going to join ancestor with itself but since each one of ancestor is only the parent relationship we're again going to get all pairs in the second step of the iteration The difference begins in the third step Now we're joining ancestor with itself So we will be joining these two step ancestors with the single ones just like before to get all the threes But we will also be joining twos with twos In_other_words we will joining grandparent relationships with grandparent relationships And we will be getting in that same iteration the fours So as you can see the nonlinear version does converge faster Now this example is very small so it's not as blatantly obvious But the linear version is going to take a linear number of iterations in order to converge to the final recursively_defined_relation contents Whereas when we use the non_linear version it's actually logarithmic So for a large database it can be considerably faster So what about the downsides of non_linear recursion Well the major downside is that it's harder to implement or certainly harder to implement efficiently And as a result of that actually the Structured_Query_Language standard only requires linear_recursion And the postgres system that we've been using also only supports linear_recursion So back to the basic form of our with recursive statement in order to introduce a different topic which is the topic of mutual_recursion Mutual recursion as I alluded to in the previous_video is the case where one of our recursively_defined relations does not refer to itself but rather to a different recursively_defined_relation And that one refers back to the first one Or we could even have a loop of three or four or more So the idea is that we can't look_at these individually to see the recursion but together they are recursive and they have to kind of be computed in tandem So the example I'm going to use here is what's_known_as hubs and authorities Hubs and authorities was an algorithm for web searching actually For annotating web nodes for the purposes of searching Was developed around the same time as Google's page rank I guess we can see which one won out But hubs and authorities actually quite interesting just in what it does Let_me go_ahead and define the meaning of Hudson authorities and then show how mutual_recursion in Structured_Query_Language can be used to compute the Hudson authorities in a database that contains a link a structure a graph basically So here's a little graph and we're going to assume that each node has a number say associated_with it and that we have a relation called link that just tells_us the edges of the graph so the source node and the destination node So in a graph we're going to designate some of the nodes as hub nodes and some of the nodes as authority nodes And we are going to define a hub node to be a node that points to at_least some number let's say three authority nodes And similarly we're going to say an authority node is a node that's pointed to by let's say at_least three again hub notes And by the way this numbers three and three don't have to be the same And another thing I wanted to mention is in a graph say representing the web we wouldn't expect a large fraction of the notes to be hubs in authorities many would be normal notes But again this just for illustrative purposes but it also serves to teach you about the hubs in authorities concept which is kind of interesting Now you can see already how mutual_recursion is going to fit into the picture But how are we gonna get started The only way you can actually get started is to have some nodes that are predesignated as hubs and authorities For_example if we predesignated as authorities these three middle nodes here then we could compute the fact that node one is a hub So we'll also assume that we have two more relations in our database One of them gives_us a set of notes predesignated as hugs and the other a set of notes predesignated as authorities And our job is to write a query that computes all of the hub and authority nodes based_on this mutually recursive definition here So here is the query that does it By the way you have certainly noticed that I'm not doing a live demo of the recursive queries in this particular video Nonlinear recursion is not supported in the Postgres system and it's also not part of thAQL standard Mutual recursion in limited forms is part of the Structured_Query_Language standard but it's also currently not supported in Postgres so I've used the nice interface is here to get the coloring of our queries But these queries currently don't run on the systems that we're using OK so back to our actual query here So this is a query to compute hubs and authorities given that we have a starting set of hub nodes and a starting set of authority nodes And then we have the link relation that gives_us the structure of our graph So we're going to compute to relations The hub relations with the nodes that are hubs The authorities relation of the nodes that are authorities and they are going to have mutual_recursion between them So let's take a look first at the hubs and we'll see that the structure of the queries for hubs and authorities is very very similar So the base case for the hubs is that the nodes that are in the hub start relation are in the hub relation of course And then the recursive query here is a little_binary_digit complex So what we're going to find is links elements in our link relation where the destination is an authority and so we're going to find all of the sources that point to an authority We're going to group_by the source so we consider each node one at a time And then we count how_many times it appears in the link pointing to an authority So this is going to give_us a nodes that point to greater_than or equal to three authorities which was our definition of hubs Now here of course we're referring to authority which itself is a recursively_defined_relation The authority relation is very similar as I said we start with our base case of adding nodes that are in the authority start relation And then we consider destinations instead of sources in our link relation such that there are at_least three sources that are hubs and that's what we've_got down here So this is going to give_us elements that are pointed to by greater_than or equal to three hubs And here of course we are using hub which is also a recursively_defined_relation So you can think of these two as working in tandem You can think of the system as sort of iteratively adding to the hubs and the authorities until there's nothing more to add to either one Now one thing that this definition of hubs of authorities and this computation allows is for a node to be both a hub and an authority And there is nothing wrong with that if the structure of the graph yields that result But let's_suppose we don't want nodes to be allowed to be both hubs and authorities We want every node to be either one or the other That will require us to modify our query To not add nodes to become hubs if they're already authorities Or to have nodes become authorities if they're already hubs So let's go_ahead and modify the query to incorporate that additional constraint So here's the query and by the way just a reminder that you can download these queries from our website even_though you can't run them at this point in time The difference in this query from the previous one is one additional condition in the definition of hubs right here saying I'm not going to add a node a source node to the hubs if it's already in the authorities and similarly I've added one more condition here in authorities that I'm not going to add a node to authorities if it's already a hub Now let's_suppose we have the following graph structure We have a node here that hasn't been labeled as a hub or authority yet And let's_suppose that this node is pointed to by three nodes that have already been designated as hub nodes And furthermore this node points to three nodes that have already been designated as authorities So by our definition this node could be a hub because it points to three authorities and it could also be an authority because it is pointed to by three hubs But the query we've given now is not going to allow_us to label this node as both a hub and authority And just to be clear in the previous query we would have put this node in both the hub relation and the authority relation but now we're not going to be able to do that because of these conditions right down here So actually whether this node ends up as a hub or an authority depends on effectively which one of these arms of our with statement gets executed first If we first consider the possibility of the node being a hub then it will be put in the hub relation and then it won't be allowed to be put in the authority relation On the other_hand if we first make an authority then when we look for computing the hub relation it wouldn't be allowed to be a hub So you can think of this as a sort of non deterministic behavior or if you're into theory there's a non_unique fixed point of the recursion and this is considered as a not good thing Generally database people when they run queries would like to have one answer all the time They like to have deterministic answers for their queries so actually this type of mutual_recursion is not allowed in the Structured_Query_Language standard and the real crux of the problem here is that one recursively_defined_relation is depending negatively on another one So this negative dependence is what causes the problem And actually we can have a negative dependence even without mutual_recursion We could define a relation that sort of depends negatively on itself in a sub query and that wouldn't be allowed either So that completes the example of hubs authorities and again what we're trying to show first of all is mutual_recursion which can be quite_powerful And second of all the restriction that we can't have negative subqueries across recursively_defined relations The last thing that I wanted to mention in this video it's not in the title of the video since we are focusing mostly on nonlinear and mutual_recursion is recursion with aggregation And let_me just show a simple abstract example So we have a relation P that just contains one attribute we can assume that it's integers And we're going to try in our with recursive statement to computer recursively define relation called R that contains the tuples in P together with the sum of the values in the attribute of P I will just write that as sum of P So here's how we write it in Structured_Query_Language We have our base case which is that the tuples in P are also in R And then we do our UNION of the recursive part which says and also in R I want to have the sum of the tuples in R So let's say that P starts out with two tuples the values one and two So what does the query compute for R Well certainly one and two are in R based_on the first part here And then based_on the second part then in the first iteration R should also contain the sum of R which is Except as soon as we put three in the sum of R isn't three anymore the sum of R is six So shall we cross out the in and put six there But then now the sum of R has become six seven eight nine You can see the problem There's no good definition for what R should contain based_on this recursion And for that reason actually recursion with aggregation is disallowed in the Structured_Query_Language standard and isn't supported by any system So to summarize about both of our videos about recursion Structured_Query_Language has introduced recursion into the standard as part of the WITH statement Whether the keyword RECURSIVE goes with the WITH or with recursively_defined relations is a binary_digit inconsistent but in any case the basic_idea is the same When we have this statement we can write queries that refer to the relation that's being defined And we can also have mutual recursions between the queries that are defined in the with statement and finally the result is a running of the final query which might involve the recursively_defined relations Adding recursion to Structured_Query_Language does strictly extend it's expressiveness There are queries that can't be written without recursion They usually involve some type of unbounded computation for example computing any number of flights or any depths of ancestors Usually there's a transitive closure flavor to those queries Without recursion the iteration involved in computing recursively_defined_relation has to be written outside of the database has to be written in code in some fashion Now we saw that the basic functionality of Structured_Query_Language recursion is linear_recursion where we only have one instance of the recursively_defined_relation in the query defining the relation We can write a lot with linear_recursion It's very expressive and can express most of the natural queries we might want to do in recursive Structured_Query_Language But there is extended functionality there's non_linear recursion We saw that non_linear recursion can lead to nicer looking queries and can converge faster but is actually more difficult to implement efficiently And then there's mutual_recursion where R here might be defined in terms of R which itself is defined in terms of R and we saw one interesting example where we'd_like to use mutual_recursion where it was appropriate Finally in terms of what was disallowed recursive sub queries by that I_mean referencing recursively_defined_relation in sub query is actually in the Structured_Query_Language standard not supported by the postgres system that we were using When a reference in a sub query to a recursively_defined_relation is negative sort of like a not exist or not and that is disallowed by the Structured_Query_Language standard and we saw that that can lead to sort of non obvious behavior non deterministic final results And finally aggregation causes complication as_well in recursion and is disallowed too The features that are disallowed really don't come up that often naturally and once again and let_me just emphasize that the basic functionality of linear_recursion does allow one to express a lot of really nice queries and does extend the expressiveness of the Structured_Query_Language language This video introduces online_analytical_processing or Online_Analytical_Processing A subsequent video will have a demo of Online_Analytical_Processing queries in action Overall database activity can be divided into two broad classes One of them the traditional one is known_as Online_Transaction_Processing or online transaction processing The other one the subject of this video came about more recently and it's known_as Online_Analytical_Processing or online_analytical_processing Online transaction processing is typically characterized by short transactions both queries and updates Things like updating an account balance in a bank database or logging a page view in a web application Queries in Online_Transaction_Processing data bases are generally fairly simple Find an account balance or find the GPA of a student They typically touch small portions of the data And updates in this environment can be frequent We might be making airline seat reservations or updating a online shopping cart Online_Analytical_Processing is pretty_much the opposite in all respects In Online_Analytical_Processing we have long transactions often complex analysis of the data or data mining type operations The queries as I said can be complex and especially they often touch large portions of the data rather_than small portions as in Online_Transaction_Processing And updates in the Online_Analytical_Processing environment tend to be infrequent in fact sometimes in the Online_Analytical_Processing environment there are no updates to the data at all Now these two are extremes and really there is a spectrum between those two extremes We might have a sort of moderate amount of update and queries that touch a moderate portion of the data But the fact is that database_systems traditionally were designed for the first extreme And then special techniques were developed for the other extreme So the systems are tuned for the two extremes And depending on ones work load one might choose to use different options in a database system just a little_binary_digit more terminology in the Online_Analytical_Processing world There's a concept called data warehousing It's really a software architecture The idea is that often in enter prizes or other operation there are lots of operational sources So you can think of a point of sale for example might have many many Online_Transaction_Processing database pieces related to an enterprise and data warehousing is the process of bringing the data from all of those distributed Online_Transaction_Processing sources into a single gigantic warehouse where the point then is to do analyses of the data and that would fall under the Online_Analytical_Processing camp Another term you might encounter is decision support systems also known_as DSS This isn't really an exact term It's generally used to talk_about infrastructure for again large scale data analyses So if you think of a data warehouse where we're bringing in a lot of data from operational sources and that warehouse is tuned for Online_Analytical_Processing queries that would be thought of as a decision support system And of course this system is designed to support decisions that are made again based_on data analysis Now let's get into some technical details of Online_Analytical_Processing Frequently applications that are doing online_analytical_processing are designed based around a star schema so it's a certain type of relational schema In a star schema there's usually one fact table That will be a typically very_large table it will be updated frequently Often it's actually append only so there are only inserts into the fact table And then there are maybe many dimension_tables Those are updated infrequently and don't tend to be as large So examples of a fact table might be sales transactions in a sales database or in a university database maybe students enrolling in courses or in a web application logging the page views In all of these cases we can see that the fact table can be very_large and can be append only so inserts only Examples of dimension_tables might be in a sales database store's items and customers in a college enrollment database Maybe students and courses in a web application Maybe web pages his users and advertisers So you can see that these are generally smaller tables they're more stable they're not updated as frequently You can sort of think of dimension_tables as things in the real_world and then fact tables as logging things that happened It's not always divided this way but it's not a bad approximation Now you might be wondering why is it called a star schema and it's called that because we have the fact table sort of centrally referencing dimension_tables around it So I'll draw the picture Let's take a particular example and let's look_at the sales domain So we'll have our fact table here which will be the sales table and that will log sales transactions actions It will include the store where the sale was made the item that was sold the customer how_many were sold and the price that was paid And then the other three tables are the dimension_tables So those those are giving_us information_about the stores and the items and the customers So I've drawn a picture of our schema here We have our central fact table the sales table And we can see that the sales table contains these three columns I've abbreviated them in the picture the Store ID Item ID and the Customer ID The store ID values in this column will be foreign_key attributes to the primary_key of the store table if you remember our constraints video So we can think of these as pointers into the store table least specifically matching store IDs over here And we'll have similarly our item IDs will be foreign keys to the item table I won't actually point to the values here And then our costumer IDs over here will be pointing to the customer table So if you look_at this squinting you will see that it is kind of a star schema with the central fact table pointing to the dimension_tables around it and that's where the name comes from Just a little_more terminology The first three attributes here in the fact fact table These three are what are known_as dimension attributes So those are the attributes that are foreign keys into the dimension_tables Then the remaining_attributes in this case the quantity and the price are called dependent attributes So they're I guess dependent on the values for the dimension attributes and typically queries will tend to aggregate on the dependent attributes We'll see examples of that in a moment So now that we known what a star schema looks_like let's look_at the type of queries that are generally issued over this schema and they're called Online_Analytical_Processing queries Typically a query over a star schema will first join some or all of the relations And when you're joining the sale as the fact table with the dimension_tables you can almost think of it as expanding the facts in the sales table to include more information_about the sales Since we have the foreign keys we'll be adding for example to the information_about a sale More about the store The city and state of the store For a sale item will be adding the category brand and so on So that's the join process and the query will join as much as it needs in order to do the rest of it's work It might then filter the data For_example we might decide that in our query we only care about stores in California or customers in California we're only interested in shirts and so on So they can filter on the dimension attributes after joining or could filter on the price or quantity as_well After filtering there's often a group_by an aggregation So we might decide that we're interested in figuring out our total_sales divided by customer or by item or by state or all of those And then the aggregation might sum up the sales or it might determine the average price that's sold We'll be doing a number of this type of query in our demo later on So if you think_about executing queries of this type they can be quite complex and they can touch large portions of the database Sowe 're worried about performance and our data is large we do have a worry Running this type of query on a gigantic database over a standard database system can be very slow but over the past decade or so special indexing techniques have been introduced and special query processing techniques specifically to handle this type of query on star schemas on large databases And again by large just think_about the number of sales for example in a large retail chain or a number of web views or even shopping cart additions in a large online vendor So in all of those applications people are interested in doing Online_Analytical_Processing queries and they tend to use a system that supports these special techniques Another component of getting good performance in these systems is the use of materialized_views You_might remember that materialized_views are useful when we have a workload that consists of lots of queries and not so many updates And that's exactly the type of workload we have in Online_Analytical_Processing furthermore we have many queries that take roughly the same structure so material wise we use are useful in that setting as_well Now let_me switch gears and introduce a different way of looking_at the data in these Online_Analytical_Processing applications with star schemas and it's what's_known_as a data cube Sometimes this is also called multidimensional Online_Analytical_Processing and the basic_idea is that when we have data with dimensions we can think of those dimensions as forming the axis of a cube It's kind of like an N dimensional spreadsheet Now we can have any number of dimensions but for the examples I'm gonna give the best I can draw is up to three dimensions and that's why people call acute Because they know how to draw three dimensions But again any number of dimensions are possible in this view of the data So we have our dimensions forming the axis of our cube And then the cells of the cube again you can think of it sort of like cells of a spreadsheet Are the fact of data Or the dependent data It's like in the previous example that would be our quantity and price And finally we have aggregated data on the sides edges and corners of corner of the cube Again similar to how you might aggregate columns in a spreadsheet So let's go_ahead and I'll do my best to draw a picture to explain what's going on So here's my cube with these three axes that I've drawn in black And I've drawn these dash lines as_well to sort of give you a visual idea of the cube But I'm going to actually get_rid of these dash lines right now just so we don't have too much clutter So for our sales example we're sticking with the same example we have dimensions And those will label the three the three axises of are cube and in one dimension we will have the stores and another dimension we will have the customers here and in another dimension we have the items Then we can think of the points along these axes as being the different elements in each of those domains or the different tuples in each of those dimension_tables So for example in the store domain we'll have you know store store store and so on I'm not giving them any fancy names here And so each of those is a point on that dimension and similarly for the items will have item item item and so on And for the customers along the bottom we'll have customer customer number customer and so on Now here comes the tricky part especially for drawing The idea is Is that every cell in the cube so every combination of item costumer in store has a cell in the cube so this would be sort of a free floating cell here And This will have for our schema the quantity and the price for that item that customer and that store So this might be the floating thing here that's you know Item I Costumer and Store something like that And then floating in there is this cell with the quantity and the price Now we are assuming that there's just one quantity and price for the combination of those three attributes And I'll come_back to that in a moment but let's assume that for now So that's what we have in the whole central area of the cube So now on the faces edges and corner of the cube are going to have aggregated data And there does need to be with each data cube a predefined aggregate So for this one let's say that what we want as our aggregate is the sum of the quantity times the price so we're going to figure_out the total amount that we're making for different combinations of stores items and customers So now let's consider a cell on the face of the cube So again I'm not drawing this very well But let's assume this is on the bottom face of the cube So this is for a particular customer Say customer in a particular store say store and then since it's on the bottom of the cube so we didn't go up this dimension here it considers all items for customer and store So this will be the aggregate over all items for that particular store and customer And we'd have similar values on the other faces of the cube So this face over here for example would be for a particular item and customer overall stores And then on the front face of the cube if you could imagine that would be for a particular item and store over all customers Now let's talk_about what's on the edge of the cube So here we have say for store we'll have the aggregate value over all customers and items in this point for store So that will be the total_sales that we conducted at store S Over here on this edge we'd have the total for a specific costumer and over here for specific items And then finally we have at the corner of the cube the full aggregation So that's going to be in this case the sum of the quantity times price for every store customer and item So I'm not a great artist but I_hope this gives you some understanding of how the data cube works So as we saw in the cube we have one cell in the cube for each combination of store ID item ID and customer ID So if those three together form a key then it's very straight_forward If the dimension attributes together don't form a key then we might be pre aggregating already inside the data cube So we might decide to already have say the sum of quantity times price for each combination of store_item and customer Another possibility and it's done quite commonly is to add to the fact table the attribute date or even the time And that can be used to create a key Typically we won't have two transactions at exactly the same time Now if we do have an attribute here called date one might_wonder is that a dimension attribute or a dependent attribute Actually it's pretty_much a dimension attribute because we're gonna use it as another dimension in our data cube but the difference being that we would not have an actual dimension table listing the dates Now let's move on to a couple other concepts in the olap world called drill_down and roll_up The idea of drill_down is that we may be examining summary data and then we want to get more information Drill down into the details of that data And actually we can think of that very specifically in a Structured_Query_Language context as follows Let's_suppose that we have this query and Structured_Query_Language which follows by the way the description of the query I had earlier where we'll do a join and then a selection and then it grouped by and finally we have an aggregation here So this query specifically is looking_at our total_sales broken out by state and brand Maybe we'll look_at that and we'll just say that's not enough detail I need more information So to drill_down what we do is add a grouping attribute So if we added for example category when we add another grouping attribute that gets us more data in the answer more_detail in our data Rollup is exactly the opposite Rollup says we're looking_at data and we decide we have too much detail and we want to summarize And summarize is simply a matter of removing a group_by attributes So if we took out state then now we'll only see our data summarized by brand rather_than broken out into state and brand And lastly I want to add introduce some Structured_Query_Language constructs These are constructs that have been added fairly recently to the Structured_Query_Language standard in order to perform Online_Analytical_Processing queries And we'll be seeing these in our demo The constructs are called with cube and with roll_up and they're added to the group_by clause When we add with cube to a query with a group_by what_happens is that basically we're adding to the result of our query the faces edges and corner of the cube Using no values for the attributes that we're not constraining We'll see this clearly in the demo With Rollup is similar to With_Cube except it's smaller It actually is a portion of the data cube and that makes_sense when we have dimensions that are inherently hierarchical And again we'll see that in the demo as_well So we can conclude there are two broad types of data base activity online transaction processing Short simple transactions touching small portions of the data lots of updating and Online_Analytical_Processing or online_analytical_processing where we have complex queries long transactions might touch a large portion of the data and might not update the data at all For online_analytical_processing Online_Analytical_Processing we saw that star schemas are frequently used We saw how to view the data as a data cube Of_course that can be in any number of dimensions We just use three for visualization There are two new constructs in Structured_Query_Language With_Cube and With Rollup And finally this type of query can be very stressful on a database system when we have very_large databases So special techniques have been introduced into systems to help perform these queries efficiently In this video we'll be doing a live demonstration of Online_Analytical_Processing We'll create a star schema similar to the one we used in our previous examples It will be sales data with a fact table and several dimension_tables and then we'll do several different types of Online_Analytical_Processing queries We'll show a full star join over the star schema We'll show some examples of drill_down and roll_up We'll also show what's_known_as slicing and dicing which we haven't explained yet we'll just do when we get to those queries And finally we'll show the With_Cube and With Rollup clauses that have been added to the Structured_Query_Language standard for Online_Analytical_Processing queries For those we'll be using MySQL actually for the whole demo we'll be using MySQL MySQL supports with roll_up Neither of the other systems that we've been using Sequel Lite or postgres supports with roll_up yet And MySQL does not yet support With_Cube but we'll see that we can simulate the behavior With_Cube using With Rollup command of MySQL So let's move to the demo Here we are in the demo we have a fact table that contains information_about stores items and customers We don't see much here except their ID values And we have sixty tuples loaded up in our fact table Our dimension_tables are the customer table We have four customers with a gender and an age We have our item table five items a category of the item just t_shirts and jackets just in blue and red and finally we have our stores And we have six stores that are in various cities in Santa_Clara San_Mateo County in California and King County in Washington Our first query is simply the complete star join So we're joining all four tables using the foreign_key references in the fact table to the keys and the dimension_tables Now since these are foreign_key references you can think of this star join as simply extending the tuples in the sales table with further information_about the store_item and customer mentioned in each one So here we go And we should expect again sixty tuples in the results one for each tuple in the fact table and we can see that we have the information from the fact table but then we also have more information that we've joined in from the dimension_tables Now it's not typical to do the complete star join usually we would have at_least constrained the star join in some way So our next query will do selections and projections on the JOIN We're going to limit ourselves to sales where the state of the sale is California the category is t shirt the age of the customer is less_than and the last condition is actually over the fact table saying the price of the purchase was less_than And now we'll return the city color customer name and price of the item We run the query and we see now that we have just sales that meet our criteria So we've_seen the JOIN with constraints on the JOIN but what's even more common in Online_Analytical_Processing applications is to do grouping and aggregation in order to analyze the data And we're going to start_by looking_at grouping it in the aggregation involving the dimension attributes in the group_by and then the dependent attribute being aggregated This is a very common form of query So here we'll say we want to analyze our data based_on combinations of stores and customers So we'll group_by store ID and customer ID And then we'll sum up to see the total_sales for each combination of store and customer So here's our result Now maybe not that meaningful to us right now just looking_at the store IDs and customer IDs but for an analyst it might be meaningful We'll see in a moment doing group_by an aggregation on other attributes that look a little_more interesting So now I'll demonstrate the concept of drilling down So you might remember drilling down says that we're looking_at data that's aggregated in some fashion and we decide we want more_detail We get more_detail by adding more attributes to the group_by so let's say in this case I want to break out not only by store and customer but also by item So I'll add item ID to the group_by and also to the select and when I run this query I see that I get more results and I now have more_detail in the analysis of the data Now I don't have tumbles in my result even_though I'm grouping by all three dimension attributes because I do have in my fact table more_than one tuple for some combinations of store_item and customer Next I'm gonna introduce a query that shows the concept called slicing Slicing of course evokes the data cube We've talked_about the data cube in the previous_video and we specifically used this example to demonstrate a three dimensional data cube So the idea of a slicing query is a query that analyzes a slice of the cube and it does that by constraining one of the dimensions So what I_am going to do is add to this query a constraint that says let's only consider sales that are from the state of Washington And when I add that we'll continue to do the group_by an aggregation but only on that slice of the cube representing Washington stores the sales that are from Washington stores So it made a few changes to the query The first thing we see is that I added the store relation to the from clause in order to constrain the state of the sale I have to join with the store table that dimension table so that I can access the value of state which is not present in the sales table So I also had to add variables and I add the join condition for the sales table to join with the dimension table and then I add the constraint that the state is Washington The last small change is to add a variable to the store ID so that I don't get an ambiguous error So now let's go_ahead and run the query and we see that we do get a subset of what we had before And that subset is the slice of the data cube if you want to think of it that way representing the sales in Washington State Now in addition to slicing there is also a notion of dicing and again if you imagine the data cube what a dice does is slice in two dimensions and it gives you a chunk of the cube So I_am going to add a condition on the item being sold I_am going to add that the item must be red So then I'm going to be looking_at the chunk of the data cube that identifies red items purchased in the state of Washington So here's the query I've added a third relation to the from clause I added the item dimension_tables so that I can constrain the color to be red I have added a join condition joining the fact table to the item dimension table I have added the condition that the color is red And finally I had to add again the variable names to make_sure I didn't get ambiguous attribute references So we'll run this query And we see that we get an even smaller portion of the data cube Again a dice where we have constrained two dimensions So we have seen drilling down and slicing and dicing now let's take a look_at rolling up Rolling up says that we're looking_at our data analysis and we decide that it's too detailed We want to have less detail in other_words more aggregation To do that we take attributes out of the group_by clause So let's say this is way way too much detail and we just want our sales broken_down on item So we take all of the attributes out of our group_by clause except for item Here we go And then when we run the query we'll see much more summarized data And here it is broken_down again just by separate items So far our grouping and aggregation has been on the dimension attributes specifically the ID attributes identifying the tuples in the dimension table And that may be meaningful for analyses but for this demonstration it's more fun to look_at attributes that actually mean something to us So let's try a few queries that group instead based_on attributes in the dimension_tables We'll still be using the fact table We'll be joining it with dimension_tables And here we're going to start_by grouping on the state and the category of the sales Again summing up the total_sales for the different groups So here we see that we have four combinations of state and category California or Washington jackets and t_shirts and then we have the total_sales in each of those so we can see in both states the sales for jackets account for considerably more_than the sales for t_shirts but we do also notice that in California there's a lot more sales of t_shirts than there are in Washington and I guess that's not surprising given the weather Now let's demonstrate drill_down on this query So let's say we want a little_more information Maybe we want a breakdown by county in addition to state So to do that we add county to the group_by clause and the select_clause and when we run the query we see we do now have more_detail We can see for example that we had more jacket sales in Santa_Clara County than in San_Mateo County although the t_shirts were approximately equal A little actually more t_shirts in San_Mateo This is a little surprising because Santa_Clara is generally warmer than San_Mateo but it's fabricated data Now let's see we want it drilled it out even further and we want to break our data down by gender as_well as the other attributes In order to drill_down based_on gender I first needed to add the customer table to our from clause Prior to this we weren't doing any analysis that involved any attributes of the customer table And so I need to add the join condition here And then to do the drill_down I add the gender attribute to the group_by and to the select We run the query and what do we find Well we can see for example that not too many females in San_Mateo County are buying t_shirts Most of those t shirt sales are counted for by males The other thing we notice is that we don't have every single combination of county category and gender Very_specifically we look and we can see that no males in King County bought any t_shirts So we only have in our result those combinations of dimension values that actually appear in the fact table Or in the join of the fact table with the dimension_tables Now let's_suppose after I've gotten to this level of analysis I've decided that what might be most interesting is the data broken_down just by combination of state and gender So that would be a roll_up operation And remember for roll_up I take attributes out of the group_by clause So I take out the county and category and I'm just interested in state gender combinations By the way at this point if I wanted to I could remove the item table because I'm no_longer constraining based_on items or grouping based_on items But I'll just leave it in for now it's not going to make a difference in our result Of_course for efficiency I might just prefer to take it out Anyway let's run the query and now we see that rolled up data And when we look_at it I guess our main conclusion here would be that in our database the Californian males are the big spenders So far everything we've_seen has actually just used the regular Structured_Query_Language constructs that we already knew about Although we were demonstrating them over a star schema and we're showing the types of queries that one tends to run over star schema's in Online_Analytical_Processing applications Now we're going to move to the specific constructs that have been added to Structured_Query_Language for Online_Analytical_Processing As you may remember the two constructs are called With_Cube and With Rollup and they are written in the group_by clause So our first query is the absolute basic cube query We start with our sales fact table we group on all three of its dimensions and then we add With_Cube And what we're going to get is basically the data in the data cube that we saw in our previous_video that introduced the cube Let's go_ahead and run the query Disappointingly we get an_error message that this version of MySQL doesn't yet support cube Actually no version of MySQL supports cube at this point in time but we're hopeful that a future one will In fact of the three open_source systems that we have been using for our demos only MySQL supports the with rollup so we will continue with MySQL and in fact you can get the same result that you would get if with cube was supported using with roll_up although it's a little_binary_digit contorted but I'll do that now to demonstrate what you would get if you wrote with cube So here's the query I'm not going to explain it if you're particularly interested in it you can download our script and exam it yourself Again what I'm most interested in is looking_at the results and here they are So this result is the result of a With_Cube on the grouping and aggregation on all dimensions of our sales table all three dimensions So some of our tuples look very normal This first tuple second and third tuple are all giving_us the total price for combination of store_item and customer and these are exactly what we got in our original query that didn't have the With_Cube operator But now we see some tuples that have blanks and as a reminder blanks in this interface mean null So this tuple is store_item and null And what this corresponds to is an element on the face of the cube So this is the face of the cube that's not constraining the customer dimension And what the value gives_us then is the value we have on the face of the cube which is the total_sales for the combination of store one and item one and any customer and that's seventy And we can cross check that by seeing that the first three tuples add up to because those happen to be the three customers that have made purchases at Store and Item And we can similarly see for Store and Item the total_sales are If we scroll down a binary_digit we'll see cases where we have other single null_values I'll keep going until I find one here For_example here we have the null value for the store and that would be the face of the cube that's not constraining the store dimension So that would be the sum for Item and Customer at any store Item and Customer to at any store Then we also have tuples that have two null_values here's an example So this one is going to be corresponding to the edge of the cube so this is the edge of the cube that is along the store dimension and is picking out Store along that dimension and then giving the total_sales for all items and all customers in Store and we see that And again we'll see other instances where we're missing two values scroll down to get here We've got one where we're missing the store in the item so this would be on the edge along the customer dimension So this gives_us the total_sales for any store_item made by customer And then finally if we find it we'll find the one that's all three null_values and that's right here So that represents the corner of the cube That gives_us the total for all stores all items and all customers So what we've done here by adding With_Cube and again that would be the result if this query here were supported what we're getting is the contents of the data cube which are the inner parts of the cube which is what we would get without adding anything to our group_by and what we saw in one of our very first queries of the demo in addition to the data on the faces the edges and the corner of the cube Now let's do a quick cross check So this data tells_us that the corner of the cube here the one with all null_values is total price So that should be exactly what we get if we sum up our prices without doing any grouping or aggregation so lets give that a try So we merely take our sales table we sum up the total prices and we run it and indeed we get thirty three fifty So now we've_seen how Structured_Query_Language can be used to effectively create the data cube Sometimes in Online_Analytical_Processing applications it's convenient to query the cube directly So people frequently create data cube say in Structured_Query_Language using a materialized_view or just by creating a table that contains the result of the data cube which is what we are going to do There are even some systems that use the data cube as their basic native data_model So let's go_ahead and take the result of our cube query and put it in a table So I've just added create table cube as before our query and the one the other thing I did was add a name to the result of the aggregation attribute so that we can refer to it in the data cube So P then will then be containing the sum of the price for the items inside the data cube as_well as for the faces edges and corners So let's go_ahead and create that table and now let's look_at querying the cube directly So the first thing you will notice in the next few queries is that we've replaced the sales table with the cubed table so we're gonna run our queries directly on the cube and we can join the cube with the store and item tables and the customer if we needed it just as we joined the sales table because it does contain the same dimension attributes What the cube gives_us is pre aggregated data both for the store a customer and item combinations As well as the data that has null_values that is already aggregated for the faces edges and corner of the cube as we'll just see in a moment So what our query is going to do is find total_sales of blue items in the state of California And it will start_by showing the tupples that are used to get that total So we'll join our cube table with the store and the item dimension_tables in order to constrain the state and the color and then we will constrain the state and the color Notice the last thing we say is that customer id is null and that's going to give_us the data on the face of the cube that doesn't go along with the customer's dimension That_means it's going to be pre aggregated for all customers and that's what we want since we don't care about the customers and just about the color and the state So let's first run this query and we see we get six tuples and these tuples are all from the portion of the result of that cube query we ran that has a null value for customer ID and that is all combinations of stores of items and if we checked our dimension_tables we'd see that these stores are in California and these items are blue and these are our total_sales broken_down by store and item Finally to get the total total_sales we'll just sum up our column p which remember was the sum of price in the data queue So we replace c with sum of p and we run the query and we see that our total is Now let_me show you something that may seem non intuitive at first but if you think_about it or maybe run the queries yourself you'll start to understand exactly what's happening I'm going to go_back to c in the select_clause And incidentally I didn't mention it before but c gives_us in the select_clause all the attributes from the cube table and that is showing which tuples are being used for the aggregation So we had before six tuples that were being used Now I'm going to make one tiny change to the query Instead of finding the customer IDs that are null I'm going to find the customer IDs that are not null I'm going to run that query and I see that we have fourteen tuples now What we've actually done is moved away from the face of the cube and into the cube and I've said don't give me any of the summarized data just give me the original values that are the store_item and customer together So I've taken away the summaries but actually this should still give me the same answer Let's make_sure that's the case I do sum of P and I run it and I get So what_happened here is I used what's effectively the same data but broken_down by customer And I added it up that was a less efficient query but both of those queries do give the correct result they do give you the total_sales of California stores' blue items Again I encourage_you to download these and run them yourself to figure_out exactly what's going on And as a last demonstration of this very same query we can actually go_back to the sales table Let's say we don't even have our data cube So we're gonna use our sales table instead and it should give_us the same result but even less efficient than the the last query that we did So you can see here I've changed the cube to be the sales and all the C dots to F dots I took away the constraints on being null or not null since that's not relevant in the sales table Otherwise the query looks pretty_much the same So let's run this much and see how_many tuples we are operating_on now So now we're operating_on tuples So this is the original completely broken_down data Just as a reminder in the data cube even in the cells we'll be summarizing or summing for the combination of store_item and customer So if we have three tuples that are the same store_item and customer those will be broken out in the sales table and summarized in the cube even in the least summarized portion of the cube if you get what I'm saying So now we've_got our tuples and if we sum up the price of those twenty five tuples we should get that same result and we do So we saw that we had six tuples in the most efficient query over the data cube When we used just a center portion of the cube we had fourteen or something I actually don't remember the exact number and then All of them gave us the same answer Obviously these numbers are very small but if you think_about huge data that has millions or tens of millions of rows then we're talking_about dramatic differences in how much data needs to be summarized or aggregated in order to get the answer So using the summary data from the cube can be orders of magnitude faster than going to the original fact data Now a variation on the width cube is to give specific attributes with the cube operator itself And what that says is to only give summaries for these dimension attributes and not the others In_other_words we'll only see null_values in the result for the store ID and customer ID and we won't see any null_values for the Item ID So in some sense we're materializing just some of the faces edges and corner of the cube not all of them If we run this query well we'll get the same results that MySQL doesn't yet support it but this is the Structured_Query_Language standard for the cube operator with subsets of attributes As before I've cooked up an equivalent query using a MySQL's with roll_up command and I'm certainly not going to explain it but you're welcome to download it and examine it What I really like to look_at is the result and here it is So this looks a lot like our original data cube but what we will notice is that there are no null_values in the item column So it's a subset of the result of the cube query And we would use that when we know we're never going to be rolling up on items So that dimension the summaries based_on that dimension aren't going to be useful to us Now let's look_at with roll_up which is supported natively by the MySQL system With roll_up again gives_us a subset of the data cube But it's a very specific one and it's based_on the ordering of the attributes in the group_by clause So let_me just go_ahead and run it and explain what we get We again get null_values and those null_values indicate the summary For_example the Store Item and s The sum of prices for all customers for Store and Item And we see Store Item as_well We again see cases with two null_values so this is the summary for Store all of the items and customers the total_sales and we also have the triple null somewhere It's at the bottom this time with total_sales of But what we'll see is that we don't have all combinations of null_values We have a null value for customer ID or we have nulls for the combination of customer ID and item ID or all three nulls So we only have the right hand attribute or the two most right attributes or all of them and if we had a fourth dimension we'd see that we have the right most two right most three right most This doesn't really make any particular sense for this query but it's used when we have dimensions that are hierarchical And I actually introduced a hierarchical dimensions in our original schema just for this purpose So lets turn to that query So here's a query that's grouping by the state county and city These are three dimension attributes they all come from the store dimension table And they are hierarchical meaning that we don't have every combination of state county and city We have cities that are in specific counties and counties that are in specific states So when we look_at the results of the query we see of course that we when we have San_Mateo is always in the state of California King County is always in the state of Washington We don't have Palo_Alto combined with say King County with state California So we don't have all the combinations of the three We have a hierarchical structure on them And it's this structure that makes_sense when we use a roll_up So let's add with roll_up to this query and see what we get So here in our result we see our original tuples for a state county city combination And then we see for example this tuple here which is taking the state of California and the county of San_Mateo and adding up all of the sales there And we see similarly that we have all of the sales for Santa_Clara County Then we can see that we have all of the sales for each state so this is the California sales and the Washington sales And finally the triple null is the corner the cube it's all of our sales and as usual we get the total of Now what don't we see in here compared with the data cube Well we don't see for example a tuple that has California and Palo_Alto and the county as null Why is that Well Palo_Alto is always in Santa_Clara County So rolling up the county or saying I want California and Palo_Alto sales for every county is exactly the same as saying I want California and Palo_Alto sales in Santa_Clara County We also don't see for example the state and county both being null For_example if we had Seattle as a city and the state and county being null Well Seattle is always in King County and always in Washington so we're not aggregating anything there we get the exact same results as having Seattle King in Washington So if we ran WITH CUBE on this hierarchically structured data we'd actually not get anymore information we'd have a bigger result but it wouldn't be giving_us more information It would just be less efficient for getting the same data So that concludes our Online_Analytical_Processing demonstration We saw Star Schema and we saw plain Structured_Query_Language queries over that schema We saw the concept of drilling down and rolling up also slicing and dicing We introduced a WITH CUBE extension to Structured_Query_Language which is not yet implemented in MySQL but we were able to write a query that's equivalent to WITH CUBE We also saw putting a WITH CUBE query into a table and then querying that table directly and that can be much more efficient than running the equivalent query in Structured_Query_Language directly over the fact table We also saw WITH ROLLUP which is implemented We didn't demonstrate putting the result of WITH ROLLUP in a table but we could certainly do that too All of these features are useful primarily in applications that are performing analyses over very_large data sets that exhibit this dimensional type structure but this is actually quite a common structure in analysis applications This pair of videos covers NoSQL_systems The first video is going to give a history and motivation for the topic and the second video will give an overview of some specific NoSQL_solutions But let_me warn you right away that the area of NoSQL_systems is changing rapidly so I'm going to avoid giving too many details so that the videos don't get out of date too quickly Let's start_by talking_about the NoSQL name itself It's actually a little_binary_digit confusing and misleading has a binary_digit of a history to it already which we'll go through and those who invented it might be regretting it a binary_digit But let's pull it apart and let's first look_at Structured_Query_Language In the term NoSQL Structured_Query_Language is actually not talking_about the Structured_Query_Language language What it's talking_about more generally is traditional_relational data base management_systems which do have the Structured_Query_Language language but have a whole_bunch of other aspects to them as_well Over the past decade or so there have been a number of new data management and analysis problems that have cropped up and we'll talk_about some of those Where a traditional_relational database_management system might not be the best solution for those problems and again we'll talk_about that as_well So NoSQL termed when you take it apart to mean NoSQL is saying that for some problems we might not want to use a traditional_relational database system It's not talking_about this Sequel language itself not picking on that but again Talking about the whole trappings of the traditional system Now while pretty_much everyone agrees that for some problems a traditional_relational database_management system isn't the best solution there are still a whole lot of problems for which it is and so people now like to say well Problems aren't exclusively solved by traditional_relational database_systems They might be solved by traditional database_systems for some portion of them And by some other solution for other portions of the problem And for that reason NoSQL has actually come to mean and this is now the accepted definition not only Structured_Query_Language but again with Structured_Query_Language itself actually referring to traditional_relational database_management systems So what's wrong with a traditional_relational database system Well the primary issue is that it's a big package with all kinds of features And in many cases though having all those features in one place is a great thing but sometimes we don't need all of the features and it can actually be advantageous to drop some of those Now what are those features I'm going to wind all the way back to the introductory_video to these materials where we talked_about database_management systems and all the great things they provide And actually I've copied this directly from my very first set of slides That tells_us that a database_management system provides great things efficiency reliability convenience a safety multi user access to massive amounts of persistent data So let's analyze these adjectives a little_binary_digit more Let's start_by talking_about convenience and let_me give aspects of a traditional database system that lead to its convenience to use for an application So here they are A simple data_model declarative query language and transaction guarantees And these components of a database system are one of the reasons that they are really good for certain applications Because you can put your data in an understandable way you can query it using a language that's easy to write yet very_powerful And you get guarantees about what_happens if the system crashes or if multiple users are using it at the same time So the relational data_model is great because the data is organized into tables We have an understandable algebra over relations and that algebra forms the basis of the query language and everything fits neatly into a package The problem comes when our data doesn't fit so neatly into that package And if we insist on using the relational_model and our data isn't very relational then there has to be some process of organizing the data before we can say load it into the tables of our system The Structured_Query_Language language is great because it's very_powerful It includes selections projections joins aggregation all kinds of set operators useful predicates and so on but what if that query language is much more_than what we need Maybe we only need to do simple fetches of records based_on key values for example In that case using a system that supports the complicated language may be again more_than we actually need And similarly transaction guarantees are terrific when we have lots of users hitting a system at the same time And we have very strict requirements on consistency but if our requirements are a lot less even the weakest guarantees that are made by the traditional database_systems might not be appropriate for application And we're going to see examples of all of these things So the next attribute multi user ties right into our discussion of transaction guarantees Again for some applications we might not need to maintain the level of consistency when multiple users are operating_on the database at the same time that traditional systems are designed for Next attribute safety safety is both from a an authorization standpoint and from an attacker's standpoint And you know safety is not that different a concern in these NoSQL type applications than in traditional applications Although in some cases the NoSQL_solutions we're going to discuss are used more to process data offline and in a batch mode in which case safety in terms of authorization or even attack is much less of an issue than say a database system that's sitting behind a deployed website Persistence is something that's provided by database_systems and persistence is certainly something important in NoSQL type applications as_well Although for NoSQL we'll see that files are often okay as a storage mechanism rather_than specialized structures that are typically implemented in a database system Reliability again is something we're certainly going to want in any data management application but again the considerations are a little different in certain types of applications say the batch data analysis applications Or it might be just okay to completely redo the entire say processing of the data where that wouldn't be the case if you had an operational database sitting behind a website Now the last two adjectives on the other_hand are on the other end of spectrum One of the reasons for NoSQL_solutions is that the data being handled these days is much much more massive than the amount of data that the traditional_relational database_systems were designed for part of the reason is that the cost of hardware's gone down and so people are just saving much more data and then again another reason of course are websites such as Facebook and tweeter and so on that are just collecting data from enormous numbers of users at enormous rates And those same websites by the way have efficiency requirements that are much much higher than we've_seen in the past So we have these millions billions of of records and we expect a response time of under a second for fairly complex operations overloads So again these are areas where NoSQL_systems want to increase the adjectives where the earlier ones we want to sort of decrease what they're offering So in some sense you can almost think of NoSQL_systems as compromising on some of these earlier ones in order to boost some of the later ones So with that motivation now let's talk_about the NoSQL_systems So they are as I've said an alternative to a traditional_relational database system for managing and analyzing large amounts of data At the highest level here are the attributes that are provided in NoSQL_systems So first of all a more flexible schema than the rigid relational_model Second of all and I really hesitate to say this but they tend to be a binary_digit quicker and a binary_digit cheaper to set_up for a particular application Now that may change over time but that's the word on the street as of this moment Third as I already motivated they are designed for massive scalability and that means massive both in the amount of data and also with the efficiency of the operations on that data And lastly they don't necessarily have transactional guarentees In general what they do is relax the consistency that's offered by the system And in turn gain higher performance and higher availability of the system So these systems do tend to be used for applications that have strict requirements both in how fast they can do things and in being up all the time Now of course there's a downside and again this is just at the very highest level Different NoSQL_systems address things in different_ways On the downside they tend not to have a dec declarative query language So one of the benefits of a traditional system is being able to write Add hot queries in a very nice high_level language change your mind about what you want to write and make changes easily explore the database in lots of different_ways just with these simple queries so in those Structured_Query_Language systems the lack of declarative query language does mean that there's more direct programmig Involved in manipulating the data and another downside is the relaxed consistency does mean there are fewer guarantees so for applications that have very strict that need very strict guarantees about the consistency of the data or say the serialized ability of operations on the data no sequal systems are probably not a good idea So the remainder of the video is just going to go through a set of examples to motivate the cases where we might want to use a NoSQL system and the reasons that it might be difficult to use a traditional system They're all sort of simplified made up examples but I do think they give the overall idea So the first example is an application where we have a large number of logs of web activity and we want to analyze those logs in various ways Let's assume that when a web log is written there's a set of records for each access and that record is going to report that a particular user with a given user ID accessed a particular Uniform_Resource_Locator at a particular time And then we might have some additional_information associated_with that access maybe something about the user or whether it was converted into a purchase or where the user went next All kinds of possible things we might want to include in there and actually that additional info might change over time So let's_suppose that we're going to work with our web log data in a traditional database system The first test then would be to get the data loaded into the system And this is one of the first problems with relational systems is that there might be quite a binary_digit of work involved in taking data like this that might not be perfectly organized and getting it into a relational system For_example we might want to do some amount of data cleaning data cleaning refers to finding say errors or is inconsistencies in the data and resolving those For_example maybe our time stamps are in multiple formats and we need to resolve them Maybe some of our URLs are invalid So we go through a data cleaning process The next thing we might want to do is some amount of data extraction So let's take a look_at this additional_information So this might be in a structured semi_structured or free text format but if we're going to load into a database system then we're probably going to need to extract the relevant fields from that information and get that formatted so we can load it into a table We might also do some amount of verification maybe checking that all the URLs are valid And then finally we have to devise design some kind of schema or specify a schema and then get the data loaded in Now proponents of NoSQL_systems will tell you Hey you don't have to do any of that You can just do nothing and immediately start operating_on the data directly out of say the file where it's stored And that sort of comes back to the idea that you can get up and running more quickly on a NoSQL system Now of course there's_no free lunch or pay me now pay me later The reality is of course when you actually start processing the data you are going to have to embed somewhere in there these same basic operations to get the data cleaned up and usable But those would occur during the processing of the data And if there's some portions of the data that you're not operating_on you can just leave those in place without doing the clean up of that portion of the data Now let's look_at the type of operations we might want to preform over this data It might be very_simple things We might say just want to find all records that correspond to a given user Or maybe we want to find all accesses of a given Uniform_Resource_Locator or everything that happened at a particular point in time Now none of these things require Structured_Query_Language ooh NoSQL but of course that's not what NoSQL stands for But these all just require finding you know a set of records based_on a single value Or we might want to look for some special construct that appears inside the additional_information which the Structured_Query_Language language again is not particularly designed to do The other thing to notice about all of these operations is that they are highly parallelizable Each one of them in fact is just looking_at the individual records We could do everything in parallel And exploiting parallelism when you have simple operations is one of the important aspects of most NoSQL_solutions Here's an operation I came up with because it does look like it requires a relational joint Let's say that we want to find all pairs of users that have acessed the same Uniform_Resource_Locator In fact back in the Structured_Query_Language videos I gave several examples like this This is essentially a self join over two_instances of a table or two_instances of the web logs So this looks_like maybe we actually do need a SQL like solution but I'm going to argue that this is actually kind of a weird query and it's fairly unlikely that we would be doing this one on a regular basis Now let's make our data scenario slightly more_complicated In addition to the web log let's_suppose that we have separate records with information_about users So we have the user ID maybe the name age gender and some other attributes of the users And now suppose our task is to take a given Uniform_Resource_Locator and determine the average age of the users who have accessed that Uniform_Resource_Locator Okay well this is a very I would argue SQL like query So it's nice in this case to have a language somewhat like Structured_Query_Language But I'll still say that some aspects of NoSQL_solutions may be relevant to this task and in particular it's the question of consistency If we're using if we're analyzing a huge amount of data and we're just looking for some type of average some type of statistical information over that data it might not be required that we have absolute consistency In fact it might even be okay if we count some accesses to URL's that were at a certain time and we completely missed some from an earlier time just because the database might be fragmented and and inconsistent And that's that's again probably okay for this application so to summarize the weblog application it might be somewhat difficult to get web access records into a relational database system It might be easier to use them in their filed format and extract the information when we need it many of the operations that we perform maybe extremely simply just fetching a set of records based_on a value and also highly parallelizable And even for the more_complicated analyses that we might do we may not need strict consistency accessing say a very specific snapshot of the data Now let's take a look_at another application let's say the friends relationship in a social network which generates a graph when a user is represented by nodes and say the friend relation is represented by edges So each record is going to have user ID's That says that one user is friends with another and then we'll have separate records with information_about users the user ID maybe again their name age and gender and so_forth And let's imagine this is an extremely large social graph What kind of operations might we want to perform Well one of them is to find all the friends of the given user and so that's pretty straightforward Again we are just fetching the set of user 's that are associated_with a given user So not an operation that requires a complicated query language On the other_hand what if we are looking for all friends of friends of a given user Actually now we do require a joint operation in order to do that And furthermore what if we want to find all women friends of men friends of a given user In that case we're going to need a couple instances of the friend relationship and we're going to need to join that with a couple instances actually of the user information as_well So this is starting to look more a little_binary_digit more SQL like But maybe we don't need the full power of the language because we can see that there's a certain sort of pattern to the types of operations we're doing And of course the problem comes when we want friends of friends of friends of a given user In that case we're doing large number of joins and large numbers of joins tend not to be not that efficient in relational data base systems even when you use recursive Structured_Query_Language So in addition to this type of operation being not necessarily suitable for Structured_Query_Language a second attribute that we probably don't need again in this environment is consistency because we probably don't care that much whether we get an exact snapshot of the database if things are changing Typically for these types of analyses approximate solutions are acceptable So these types of operations suggest that we might want to have a special type of database system that's suitable for graph operations on very_large scale and in fact graph databases are one type of NoSQL solution that we will talk_about in the next_video And finally my last example is Wikipedia pages If you think of Wikipedia it's a large collection of documents extremely large And inside each document there's typically a combination of some structured data inside boxes that has say key valued pairs And then on the structured data which might be fairly large volumes of text a type of task we might want to do is say retrieve the first paragraph the text paragraph in all pages where we look in the structure data and find out that the page is about a United States president say before clearly this is not very suitable for loading into a relational database and querying in that fashion again because of the mix of structured and unstructured information And once more consistency is probably not critical in this environment as_well As we'll see another type of NoSQL solution is called the document database system which can be more appropriate for this type of application So to summarize NoSQL_systems provide an alternative to using a traditional database_management system for certain types of applications NoSQL_systems provide a flexible schema which can be useful when its not easy to get the data into a structured table format They can be quicker and cheaper to set_up So you might be able to get going faster on actually analyzing your data and maybe for less cost a binary_digit debatable but that's the word on the street right now They do provide massive scalability So they're generally used for very very_large applications Often applications that don't require the amount of consistency that a traditional system provides and by relaxing the amount of consistency they'll give you better performance higher availability The downsides of the this system tend to be the lack of a declarative query language That_means more programming is generally involved when using the systems and fewer guarantees are provided about consistency In the next_video we'll specific NoSQL_solutions and how they embody the adjectives that I've included here This video provides an overview of some NoSQL_systems I want to say right up_front that it's being made in November This is a field that's changing very fast so this is an overview of what's going on right now As a reminder from the previous_video NoSQL_systems have arisen because it was recognized that not every problem involving large scale management or analysis of data was best solved by using a relational database system Some problems still are but there are others that are more suitable for a different type of system that we're going to talk_about NoSQL as a term has evolved to mean not only Structured_Query_Language where Structured_Query_Language doesn't really mean the Structured_Query_Language language but it means a traditional database_management system Again as a reminder from the previous_video the NoSQL_systems are different from traditional systems in that they tend to provide a flexible schema rather_than a rigid structure They tend to be quicker or cheaper or both to set_up They're geared towards really massive scalability and they tend to use relaxed consistency models in order to give higher performance and higher availability The downside is being that there's_no declarative query language so more programming is typically involved in manipulating the data and because of the relaxed consistency models the plus is a better performance the downside is fewer guarantees about the consistency of the data So there are a number of incarnations of NoSQL_systems and I've chosen as of November to divide into four categories the MapReduce_framework key value_stores document stores and graph database_systems In terms of the first two one way you can think_about it sort of roughly is that the MapReduce_framework is typically used for applications that would have used relational Online_Analytical_Processing or online_analytical_processing They tend to be analysis applications that touch large amounts of the data to do complex analyses Whereas key value_stores tend to be more in the Online_Transaction_Processing world as a reminder that's online transaction processing and that tends to be a lot of small operations touching very small parts of the data The other two document stores and graph database_systems are self explanatory They involve documents and graphs Now you_might_wonder why I didn't mention column stores because column stores are often discussed in terms of NoSQL So column stores are in one sense just a way of organizing relational database_systems for higher performance for certain types of applications but we'll also see that key values stores do tend to have sometimes not all of them have a model that's also based_on columns being an important concept So now I'll discuss each of these in turn although I'm going to spend the most amount of time on MapReduce So we can think of MapReduce as a framework It came originally from Google They invented the term MapReduce and now there's an open_source system widely_used called Hadoop which does implement the MapReduce_framework so the first aspect of MapReduce is that there is no data_model at all The data in the MapReduce_framework is stored in files both as input and output In the Google MapReduce implementation it's the Google File System GFS In the Hadoop open_source implementation it's the Hadoop Distributed File System HDFS What the user provides to process data using the MapReduce_framework is a set of specific functions Not surprisingly one of those functions is called map and one of them is called reduce Other functions that the user needs to provide is a reader function which will read data from files and provide it as records A writer function that will take the output records and write them into files and finally there's an optional function called the combiner that we'll discuss So the user just provides this set of functions and then what the system provides is the glue that processes the data through the functions The system also provides fault tolerance of the processing so if there is a crash or a node goes down during the execution it will be guaranteed to be as if that didn't happen And finally the system also provides scalability so that the MapReduce_framework can be used for very very_large data analysis So let's talk_about the two most_important functions the map_function and the reduce_function The map_function is used to take the data analysis problem and divide it into sub problems Very_specifically the function that the user provides called map is going to take a data item as input and it's going to produce as output zero or more key value pairs Now what I_mean by a sub problem here is that we're going to separately deal with the set of records associated_with each key and that's the job of the reduce_function So the reduce_function which we'll write takes as its parameters a key and then a list of values for that key and it produces as output zero or more records Now we'll shortly see a concrete example that will hopefully make this more understandable but before we do that let 's look_at the overall architecture of how these functions are used to process data So we'll start with our map_function which let's put inside a box and then we will have input records going into the map_function As a reminder what the map_function produces from each input record is an output record that's a key value pair and we're going to have these records sort of directed in a different way for each key So let's say this is the way that the records are gonna go for key key and up to key n And of course the records will have values associated_with them as_well So we'll send each batch of records for a given key into our reduce_function so let_me just draw a few reduce boxes here there's one for each set of records for a given key And then as we mentioned before the reduce_function produces output records At the highest level that's it That's our data processing We start with a bunch of input We divide it up into sub problems based_on a key which will extract from the input record somehow we'll see an example and then each sub problem associated_with a particular key is set through the reduce_function which produces the output And that's the end of our processing Now things are of course a binary_digit more complex than that First of all there's_no reason to have one map box because the map_function takes each input record and processes it separately so we can parallelize the mapping as much as we want So let's change the picture here to have a whole set of map boxes So now each MapBox is going to take its records and it's going to produce records with given keys so we'll still send k over to the first reducer If we have k it'll go here and down here And of course this map will send things to reduce reduce reduce and so on Now you_might_wonder what_happened to those reader and writer functions that I talked_about The reality is that we don't actually start with input records we start with our data in files So here's the real original data We'll draw this picture here for files and let's erase our input records here because the job of the reader is to take the files extract the records from the files and provide them to the map functions So here is that side of thing it's a binary_digit sloppy but I think get the idea And we have a similar thing on the other end the output methods come out of the reducers but then their provided to the writer functions that which write the output to a final file So here it is our original input in files here our final output in files there Ok but let_me remind_you what the user provide what the system provides So the user creates a single map_function that takes records and emits a key value pair for each record The user provides a single reduce_function that takes a set of values for a given key and produces zero or more outputs and I should mention that the map can produce zero or more outputs from each record as_well It doesn't have to be a one to one mapping The user also provides the reader function to extract data from files and the writer function to write data to the output And there's one more optional function I_mentioned called the combiner The combiner actually is sort of attached to the mapper so we can kind of put it here And what the combiner does is it actually in sort of in the mapper will take a set of records for a given key so say for K and then we'll send a combined version of that record to the reducer In a way you can think of it as a sort of pre reduce phase and we'll see examples of this that occurs with the mapper to make things more efficient and send less data to the reducer So the user has provided these pieces these system infrastructure takes the pieces and distributes them to multiple machines because a lot of this can go on in parallel All of this can go on in parallel this too and this too Here you have to exchange data maybe from one machine to another but once you do parallelism can occur and here as_well So the system distributes them to machines and you can add more machines to make it all all run faster The system also provides fault tolerance so if something goes badly here it will redo that reducer function and here as_well and finally as I_mentioned before it provides scalability But I should add I think one of the most_important things the mass produce architecture provides is the glue that puts this all together Because again the user is only providing these functions and the system will take care of all of the execution moving the data around and calling the function over the large amounts of data that are being processed Well all of that is pretty abstract so let's look_at a concrete example and let's go_back to the domain that I introduced in the previous_video of analyzing a web log where we have in each record a user ID Uniform_Resource_Locator the time of the access and maybe some additional_information And let's start out with a fairly simple task which is that we want to count the number of accesses for each domain where the domain is inside the Uniform_Resource_Locator So for example the domain might be the stanford edu domain where we have accesses to many different URLs with that domain and we're just going to count how_many accesses there have been to Stanford So to perform this task the user has to provide a map_function and a reduce_function Let's look_at what they do The map_function is going to take a record We'll assume that the reader has already extracted the record from the file and it provides it in this format with these four fields And what the map_function is going to do is simply look inside the record and extract the domain from the Uniform_Resource_Locator and it's going to produce as output from that record the domain as the key so this is the key and then for this we can just have a null value as the value we're not going to actually need to use a value And so that's the job of the mapper pretty_simple Now what does the reduce_function do The reduce_function is going to take a domain because that's the key and that's the first argument and then it's going to take a list of values in this case it's going to be a list of null_values and what's interesting is that each one of these null_values represents one access to that domain So all the reduce_function needs to do is count up how_many nulls there are for each domain so it's going to produce as its result the domain and the count And believe it or not we've solved their problem with just a little_binary_digit of code just a code to find the domain inside the Uniform_Resource_Locator from our record and then this simple code to count up the number of NULLs The system will take care of shipping the records to the right nodes to perform the tasks in parallel and then re shipping them so all of the records for all of the outputs for a particular domain are in the same place and can be counted Now let_me give an example of how that combiner function will be used The combiner function as a reminder will operate at the same node as a mapper and do some sort of pre aggregation of the data So for example we could use a combiner we'll put that right here after the mapper and the combined function is going to take the domain and the list of NULLs actually it's going to do exactly what the reduce_function was doing and it's going to produce the domain and account And so that at each individual node we'll count up how_many accesses there were to that domain in the data that's being processed at that node but then when we get to the reduce_function we may get a bunch of those records so this list of NULL here now becomes a count that's what arrives at the reduce_function the output of the combine and then instead of doing a count here we do a sum and that will give_us the right answer as_well and that will be more efficient again because of the pre aggregation that occurs right in the same node that's processing the map_function Whoops I made one mistake there Sorry about that Actually this count here that goes to the reduce_function is a list of counts right because we're going to get one of these from each of the mappers and then we add those list of counts That's the sum that we perform here sorry about that small mistake Now let's modify the problem We'll take the same data but instead of just counting how_many accesses we have to each domain let's compute some total value of the accesses for each domain And we might do that based_on something that we see in the additional_information for example how valuable the user is whether the user went off and bought something something like that So let's modify our map and reduce functions for this slightly enhanced problem Now our map_function again is going to take a record and this time it's not going to look only at the Uniform_Resource_Locator but it's also going to look inside the additional_information and what it will produce is the domain that it extracted from the Uniform_Resource_Locator and then let's say some kind of score on how valuable that access was based_on whatever it sees inside additional_information The reduced function then is going to take a domain and it's going to take a list of scores for that domain and then similar to what we had previously the output is going to be the domain and the sum of those scores Now one of the interesting things here is how the map_function interacts with this additional_information because the map_function is going to have code that is going to look in the information and it's going to determine a score based_on what it sees If we change what's available in additional_information then we can modify the map_function but everything else can stay the same or if we say we refine how we extract the score So that is one benefit to some extent of the the MapReduce_framework because the computation of the score is just embedded in this one piece of code Now let's modify our example further similar to the modification we made in the earlier video let's_suppose that in addition to the web blog we have separate information_about the user So separately from what might be an additional info we have in a different data set the user ID the name the age the gender and so_forth And now let's say that we again want to find the total value of the accesses for each domain but now the value is computed using the user attributes that we get from the separate data set Well this frankly in map reduce is hard to do It effectively involves joining these two data sets not something that's supported natively in MapReduce So now we've kind of hit the limit of what's very convenient to do in the map reduce framework but we will momentarily see that there are solutions to that as_well So to summarize the MapReduce_framework has no built in data_model The data just starts and files and it ends in files The user just needs to provide specific functions the map_function reduce_function reader and writer and optionally a combiner And the system will provide all of the execution glue it will guarantee the tolerance to system failures and it provides scalability by doing the assignment of the processing tasks to say an increasing number of computing nodes So when the MapReduce_framework came out of Google and the Hadoop open_source implementation was released there's a lot of excitement It was pretty exciting because you could just write a couple of simple functions and then the system would provide the processing of massive amounts of data through those functions and it would be scalable it would be efficient and it would be fault tolerant But over time people realized that they don't always want that low level programming and our favorite traditional notions of database schemas and declarative queries started to be missed And so what was developed is some languages that actually sit on top of Hadoop or the MapReduce_framework One of them is called Hive and Hive offers schemas and a language that looks very much like Structured_Query_Language Another language is called Pig Pig is a little_binary_digit more imperative In_other_words it's a binary_digit more of a statement language but the fundamental constructs in Pig are still relational operators and you could almost think of a Pig script as being a little_binary_digit like those statements of relational_algebra that we saw way back when with the addition of loops and so_forth Both of these languages are what the user sees and they compile to a workflow or you think of that as a graph of Hadoop jobs Hadoop again being the open_source implementation of map and reduce any job being one instance of map and reduce like that big picture I showed before And one thing I should mention as of November which it is now a really significant portion of Hadoop jobs are actually generated by Hive and Pig or Hive or Pig So more and more users are actually choosing to use a higher_level language rather_than program the MapReduce_framework directly Now I'd be remiss if I didn't also mention one other system There's a system called Driad that allows users to specify a workflow sort of similar to the workflow that might be generated by Hive and Pig so it's more general than just one MapReduce job And there's also a language called Driadlink that sits on top of Driad and compiles to Driad sort of in the same way that Hive and Pig compile to a workflow of MapReduce jobs Now let's move on to talk_about key value_stores As a reminder the Hadoop or MapReduce_framework is designed for more OLAP type operations or analytical operations that involve scanning most of the data and I think that was very clear from what the MapReduce_framework does Where key value_stores are designed more for these Online_Transaction_Processing style applications where you're doing small operations maybe over a single record in a massive database And so the key value_stores are extremely simple The data_model for key value_stores are just pairs of keys and values not surprisingly And the basic operations are simply to insert a new record so you provide a key and value to fetch a record by it's key to update the contents the value in the record for a given key or to delete the record with the given key So that's it and with that simple set of operations as you can imagine the implementation is focusing on doing these simple operations over massive databases very very quickly So again like Hadoop efficiency scalability and fault tolerance are the most_important things because we're looking_at applications with massive amounts of data and very stringent performance requirements So the way the implementation works at a very very high_level it's actually quite complicated to make it work very well is that the records are distributed to the nodes the computing nodes based_on the key probably a hash value over the key So to find the record for a given key can be very quick You go straight to the node In fact the records may be replicated across multiple nodes and that gives you both efficiency you can go to maybe a lightly loaded node it gives you fault tolerance if a node fails The notion of the actions and key value_stores are very_simple One operation itself is a transaction so we don't have the idea of grouping a bunch of operations into transactions And furthermore they implement something called eventual consistency And that says that the replicas of a single record can actually diverge in their value for some point of time What eventual consistency specifies is that if all operations stop then the system will become consistent with all copies of each record being the same Now unfortunately as is sometimes the case these very_simple operations and this simple data_model weren't always quite enough and so some key value_stores but not all I would say have a concept called columns that occur within the value So the value here has a little_binary_digit more structure to it than just a blob of bits And the columns will typically be kind of like an embedded key value_stores One thing that's important is they don't require uniform column So none of the key value_stores are as strict in their structure as a relational database system would be The other addition that some allow is a fetch on a range of keys So this might say I want to get all keys say between two and ten and so that requires a different type of implementation as you can imagine but it does allow that operation to be performed efficiently if that is something that the application needs Just a few examples of key value_stores This is not an exhaustive list there are many more and this is only November so things will change over time But some of the more prominent key value_stores are listed here Google's Big Table Amazon Dynamo Cassandra which is an open_source Voldemort H base and again there are many others These are just a few example Now let's talk_about document stores Actually document stores are very much like key value_stores except the value itself is a document So the data_model is a key document pairs and what's interesting now is that the document in document stores is typically a known type of structure so the document might contain JavaScript_Object_Notation formatted data javascript object notation It might contain eXtensible_Markup_Language which we have learned about or other semi_structured formats The basic operations are very similar though to what we say in key value_stores You can insert a new document based_on a key We can fetch based_on a key Modify the contents associated_with key and delete the record associated_with a specific key But also very_important is that there is a fetch operation based_on the document contents and this is very system format specific what the operations would be So there is not a standardized fetched query language at this point in time Again a few example systems a not exhaustive list are the systems Couch DB Mongo Database Simple Database They all seem to have Database in their name And again this is November things that will undoubtedly change One Structured_Query_Language system I'd like to cover is graph database_systems Graph database system as the name implies are designed for storing and running queries or other operations over very_large graphs the data_model is that every object is either a node or it's an edge between nodes Nodes may have properties very often ID is a required property of a and edges may have labels so you can think of them as rolls So I think what's best to understand this is just to see an example My example is going to be a very small social network a tiny one actually A similar one to what was used for some of our Structured_Query_Language exercises So let's start with three nodes and the nodes are gonna represent people and the properties of the nodes are going to be ID name and grade And so each node is going to have a value for the ID name and grade For this one we'll make it one Amy in grade nine and we'll have two more So here are the three nodes representing three people in our social graph We also have ID which is Ben in grade nine and ID which is Carol in grade ten Depending on the system the nodes may or may not have to have uniform key value pairs within the most system won't be that stringent Then in addition to the nodes we have the edges between the nodes Typically they would be directed edges So let's make two different types of edges Let's make friend edges and let's make likes edges So let's say for example Amy likes Ben So that would be a directed edge here with the property likes and maybe Ben likes Carol let's say here And maybe then we have that Amy and Carol are both friends with each other so we'll have a different type of edge called friend Now one might_wonder how long those friendships will last with this complicated likes relationship But in any case this gives you an idea of the type of data that's stored in a graph database The data_model is very specifically about storing nodes with properties inside them like key value pairs and edges typically with labels or rolls on them of course that's not required So in graph database_systems currently the interfaces to the systems and the query languages vary a lot There's no standardization at all and the queries might just be single step queries like asking for friends They might be path_expressions like ask for the women friends of the men friends of someone We saw that example in the earlier video Or they might have full recursion where you can traverse to arbitrary depths through the graph A few example systems again as of November you know I was going to say that are a Neo J Flat Database and Prego And these systems actually differ quite a lot from each other I also wanted to mention Resource Description Framework Resource Description Framework is the resource description framework and there's something known_as the Resource Description Framework triple stores Resource Description Framework is based_on objects having relationships to other objects So you can almost think of those as two nodes with edges between them so you can imagine how Resource Description Framework can be mapped to graph databases So those were four examples of NoSQL_systems If the most prominent categories at this point in time the MapReduce_framework again with languages sitting on top of MapReduce such as Hive and Pig key value_stores for more small transactions over massive databases but just operating small bits of them at once Document stores and graph database_systems NoSQL stands for not only sql recognizing that for some applications these frameworks work better than traditional database_systems but for many applications a vast number of applications traditional databases are still used
